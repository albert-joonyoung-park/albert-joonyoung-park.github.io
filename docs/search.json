[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Albert Joon Park",
    "section": "",
    "text": "Albert Joonyoung Park is an aspiring and dedicated independent data analyst with a diverse background spanning entrepreneurship, eCommerce, manufacturing operations, and database application development. With a strong focus on delivering valuable insights, he collaborates closely with clients to extract meaningful information from diverse datasets across various industries. Additionally, Albert has recently ventured into mortgage brokering, aiming to leverage his data skills to assist individuals in identifying optimal financial solutions tailored to their specific needs. Beyond his professional pursuits, Albert finds joy in spending quality time with his spouse, continuously expanding his expertise in data science, actively participating in the church choir, and honing his piano-playing skills."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Data Cleaning and Preparation\n\n\nAnalysis techniques using pandas - Python for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\nNovember 10, 2023\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nNumerical Programming\n\n\n\n\n\n\n\n\n\nOctober 28, 2023\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nMovieLens 1M Dataset\n\n\nAnalysis techniques using pandas - Python for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\nOctober 26, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nUS Baby Names 1880-2022\n\n\nAnalysis techniques using pandas - Python for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\nOctober 26, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nBilty Data from 1.USA.gov\n\n\nAnalysis techniques using pandas - Python for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\nOctober 25, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n[5] Pandas\n\n\nA series of posts, exploring data analysis using the book - Python for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\nOctober 11, 2023\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\n[4] Arrays and Vectorized Computation\n\n\nA series of posts, exploring data analysis using the book - Python for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\nSeptember 21, 2023\n\n\n18 min\n\n\n\n\n\n\n  \n\n\n\n\n[3] Built-In Data Structures, Functions, and Files\n\n\nA series of posts, exploring data analysis using the book - Python for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\nSeptember 11, 2023\n\n\n27 min\n\n\n\n\n\n\n  \n\n\n\n\nExploring Polar Coordinate Plot in Python Using Matplotlib\n\n\nMetplotlib - Polar coordinate plot.\n\n\n\n\n\n\nAugust 22, 2023\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nPython Setup in R with Reticulate\n\n\nMetplotlib - Polar coordinate plot.\n\n\n\n\n\n\nAugust 22, 2023\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Greetings from Albert Joonyoung Park",
    "section": "",
    "text": "Welcome to my personal website, where I share my selected examples and blogs on data analysis. The primary aim is to share my enriching journey in data analysis and its multitude of benefits. When data is harnessed and interpreted effectively, it has the remarkable ability to reveal concealed paths that guide decision-making, be it in our personal lives or the business world. To fully unlock its potential, I invite you to don your\"Curiosity Hat\" and explore the stories that lie within the data. Brace yourself, for the results have the potential to transcend mere significance—they possess the inherent power to shape and transform.\n  \n  \n  \n    Albert Joonyoung Park"
  },
  {
    "objectID": "posts/my-first-blog.html",
    "href": "posts/my-first-blog.html",
    "title": "A Demo Blog Post Using Quarto",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n    geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5)\n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;%\n    ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n    geom_jitter(size = 4, alpha = 0.6) +\n    facet_wrap(vars(species)) +\n    scale_color_manual(values = c(\"grey60\", thematic::okabe_ito(3)[3])) +\n    scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n    theme_minimal(base_size = 12) +\n    theme(\n        legend.position = \"top\",\n        panel.background = element_rect(color = \"black\"),\n        panel.grid.minor = element_blank()\n    ) +\n    labs(\n        x = \"Body mass (in g)\",\n        y = \"Bill length (in mm)\"\n    )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\\int_0^1 f(x) \\ dx\n\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n    stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/my-first-blog.html#merriweather",
    "href": "posts/my-first-blog.html#merriweather",
    "title": "A Demo Blog Post Using Quarto",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n    geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5)\n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;%\n    ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n    geom_jitter(size = 4, alpha = 0.6) +\n    facet_wrap(vars(species)) +\n    scale_color_manual(values = c(\"grey60\", thematic::okabe_ito(3)[3])) +\n    scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n    theme_minimal(base_size = 12) +\n    theme(\n        legend.position = \"top\",\n        panel.background = element_rect(color = \"black\"),\n        panel.grid.minor = element_blank()\n    ) +\n    labs(\n        x = \"Body mass (in g)\",\n        y = \"Bill length (in mm)\"\n    )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\\int_0^1 f(x) \\ dx"
  },
  {
    "objectID": "posts/my-first-blog.html#columns",
    "href": "posts/my-first-blog.html#columns",
    "title": "A Demo Blog Post Using Quarto",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/my-first-blog.html#margin-captions",
    "href": "posts/my-first-blog.html#margin-captions",
    "title": "A Demo Blog Post Using Quarto",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n    stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "projects/cyclistic-bike-share-analysis.html#characters-and-teams",
    "href": "projects/cyclistic-bike-share-analysis.html#characters-and-teams",
    "title": "The Cyclistic Bike-share Analysis Case Study",
    "section": "Characters and teams",
    "text": "Characters and teams\n\nCyclistic: A bike-share program that features more than 5,800 bicycles and 600 docking stations. Cyclistic sets itself apart by also offering reclining bikes, hand tricycles, and cargo bikes, making bike-share more inclusive to people with disabilities and riders who can’t use a standard two-wheeled bike. The majority of riders opt for traditional bikes; about 8% of riders use the assistive options. Cyclistic users are more likely to ride for leisure, but about 30% use them to commute to work each day.\nLily Moreno: The director of marketing and your manager. Moreno is responsible for the development of campaigns and initiatives to promote the bike-share program. These may include email, social media, and other channels.\nCyclistic marketing analytics team: A team of data analysts who are responsible for collecting, analyzing, and reporting data that helps guide Cyclistic marketing strategy. You joined this team six months ago and have been busy learning about Cyclistic’s mission and business goals — as well as how you, as a junior data analyst, can help Cyclistic achieve them.\nCyclistic executive team: The notoriously detail-oriented executive team will decide whether to approve the recommended marketing program."
  },
  {
    "objectID": "projects/cyclistic-bike-share-analysis.html#about-the-company",
    "href": "projects/cyclistic-bike-share-analysis.html#about-the-company",
    "title": "The Cyclistic Bike-share Analysis Case Study",
    "section": "About the company",
    "text": "About the company\nIn 2016, Cyclistic launched a successful bike-share offering. Since then, the program has grown to a fleet of 5,824 bicycles that are geotracked and locked into a network of 692 stations across Chicago. The bikes can be unlocked from one station and returned to any other station in the system anytime.\nUntil now, Cyclistic’s marketing strategy relied on building general awareness and appealing to broad consumer segments. One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes, and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Customers who purchase annual memberships are Cyclistic members.\nCyclistic’s finance analysts have concluded that annual members are much more profitable than casual riders. Although the pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a very good chance to convert casual riders into members. She notes that casual riders are already aware of the Cyclistic program and have chosen Cyclistic for their mobility needs.\nMoreno has set a clear goal: Design marketing strategies aimed at converting casual riders into annual members. In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends."
  },
  {
    "objectID": "projects/cyclistic-bike-share-analysis.html#project-deliverables",
    "href": "projects/cyclistic-bike-share-analysis.html#project-deliverables",
    "title": "The Cyclistic Bike-share Analysis Case Study",
    "section": "Project deliverables",
    "text": "Project deliverables\nProduce a report with the following deliverables:\n\n\nA clear statement of the business task.\nA description of all data sources used.\nDocumentation of any cleaning or manipulation of data.\nA summary of your analysis.\nSupporting visualizations and key findings.\nYour top three recommendations based on your analysis."
  },
  {
    "objectID": "projects/cyclistic-bike-share-analysis.html#key-findings",
    "href": "projects/cyclistic-bike-share-analysis.html#key-findings",
    "title": "The Cyclistic Bike-share Analysis Case Study",
    "section": "Key findings",
    "text": "Key findings\n\nThe bikes were used by members more frequently than casual users, with a 24.12% higher usage rate in the last 12 months.\nHowever, casual riders had longer ride duration compared to members, with an average difference of 4-7 minutes.\nFrom a quarterly perspective, the third and second quarters of the year were the most popular seasons for casual riders.\nCasual riders exhibited a preference for riding bikes on Fridays, Saturdays, and Sundays, and they had longer trip duration during these days.\nAmong the rideable types, classic bikes were the most popular choice among casual riders.\nCasual riders were the only user group that utilized docked bikes.\n“Streeter Dr & Grand Ave” and “DuSable Lake Shore Dr & Monroe St” were the most common starting stations for casual users."
  },
  {
    "objectID": "projects/cyclistic-bike-share-analysis.html#three-recommendations",
    "href": "projects/cyclistic-bike-share-analysis.html#three-recommendations",
    "title": "The Cyclistic Bike-share Analysis Case Study",
    "section": "Three recommendations",
    "text": "Three recommendations\n\nCreate a marketing campaign that educates casual riders on their long-ride trips and invites them to a membership that can cater to their riding needs year-round. The campaign can be advertised both online and offline, with a focus on regions where the most popular stations for casual riders are located. This recommendation is supported by the data analysis, which highlights the long ride durations of casual riders and emphasizes the value of a membership that can provide continuous benefits.\nConsider creating a specialty membership specifically designed for docked-bike users. This targeted approach can be an effective way to engage casual riders who prefer docked bikes. However, instead of offering a separate membership, it may be more beneficial to incorporate docked-bike usage as an additional feature within the existing full-scale membership. This way, casual riders can experience the benefits of membership without creating a distinct membership category.\nDevelop a special introductory membership for the months between April and September or for the days of the week (Friday to Sunday). This limited-time membership can serve as a trial period, offering casual riders the opportunity to experience the benefits of a membership during the most popular seasons or days. Following the introductory period, you can then transition these riders to a regular yearly membership."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The Cyclistic Bike-share Analysis Case Study\n\n\n\n\n\n\n\n\n\nFebruary 5, 2023\n\n\nAlbert Joonyoung Park\n\n\n25 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/document.html",
    "href": "posts/document.html",
    "title": "Exploring Polar Coordinate Plots in Python Using Matplotlib",
    "section": "",
    "text": "A polar plot is a type of graph that presents data in a circular format, offering a fresh perspective beyond the conventional Cartesian coordinate system. In a polar plot, data points are represented using radial distance from a central point and angular positions, creating a distinct visual representation that emphasizes patterns and relationships in a unique way. This type of plot is particularly suitable for data that exhibits directional or cyclic patterns, such as time series data, circular data (e.g., wind direction), or any data with inherent periodicity. Polar plots excel at revealing cyclical trends and variations that might go unnoticed on traditional scatter plots. They allow us to easily observe changes in data values as they evolve over angles, providing insights into periodic behaviors, phase shifts, and synchronization. Whether in the realms of engineering, meteorology, or scientific research, polar plots offer a valuable tool for gaining deeper insights from complex datasets that might otherwise remain hidden in a sea of numbers.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define an array of values for the radial distance\nr = np.arange(0, 2, 0.01)\n\n# Convert the radial distance into angles\ntheta = 2 * np.pi * r\n\n# Create a figure and axis with polar projection\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n\n# Plot the data points on the polar axis\nax.plot(theta, r)\n\n# Customize the radial ticks to enhance readability\nax.set_rticks([0.5, 1, 1.5, 2])\n\n# Add grid lines for a clear reference\nax.grid(True)\n\n# Display the enchanting plot\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "projects/bellabeat-analysis.html#characters-and-products",
    "href": "projects/bellabeat-analysis.html#characters-and-products",
    "title": "Bellabeat Analysis Case Study",
    "section": "Characters and products",
    "text": "Characters and products\n\nCharacters\n\nUrška Sršen: Bellabeat’s cofounder and Chief Creative Officer\nSando Mur: Mathematician and Bellabeat’s cofounder; key member of the Bellabeat executive team\nBellabeat marketing analytics team: A team of data analysts responsible for collecting, analyzing, and reporting data that helps guide Bellabeat’s marketing strategy. You joined this team six months ago and have been busy learning about Bellabeat’’s mission and business goals — as well as how you, as a junior data analyst, can help Bellabeat achieve them.\n\nProducts\n\nBellabeat app: The Bellabeat app provides users with health data related to their activity, sleep, stress, menstrual cycle, and mindfulness habits. This data can help users better understand their current habits and make healthy decisions. The Bellabeat app connects to their line of smart wellness products.\nLeaf: Bellabeat’s classic wellness tracker can be worn as a bracelet, necklace, or clip. The Leaf tracker connects to the Bellabeat app to track activity, sleep, and stress.\nTime: This wellness watch combines the timeless look of a classic timepiece with smart technology to track user activity, sleep, and stress. The Time watch connects to the Bellabeat app to provide you with insights into your daily wellness.\nSpring: This is a water bottle that tracks daily water intake using smart technology to ensure that you are appropriately hydrated throughout the day. The Spring bottle connects to the Bellabeat app to track your hydration levels.\nBellabeat membership: Bellabeat also offers a subscription-based membership program for users. Membership gives users 24/7 access to fully personalized guidance on nutrition, activity, sleep, health and beauty, and mindfulness based on their lifestyle and goals."
  },
  {
    "objectID": "projects/bellabeat-analysis.html#about-the-company",
    "href": "projects/bellabeat-analysis.html#about-the-company",
    "title": "Bellabeat Analysis Case Study",
    "section": "About the company",
    "text": "About the company\nUrška Sršen and Sando Mur founded Bellabeat, a high-tech company that manufactures health-focused smart products. Sršen used her background as an artist to develop beautifully designed technology that informs and inspires women around the world. Collecting data on activity, sleep, stress, and reproductive health has allowed Bellabeat to empower women with knowledge about their own health and habits. Since it was founded in 2013, Bellabeat has grown rapidly and quickly positioned itself as a tech-driven wellness company for women.\nBy 2016, Bellabeat had opened offices around the world and launched multiple products. Bellabeat products became available through a growing number of online retailers in addition to their own e-commerce channel on their website. The company has invested in traditional advertising media, such as radio, out-of-home billboards, print, and television, but focuses on digital marketing extensively. Bellabeat invests year-round in Google Search, maintaining active Facebook and Instagram pages, and consistently engages consumers on Twitter. Additionally, Bellabeat runs video ads on Youtube and display ads on the Google Display Network to support campaigns around key marketing dates.\nSršen knows that an analysis of Bellabeat’s available consumer data would reveal more opportunities for growth. She has asked the marketing analytics team to focus on a Bellabeat product and analyze smart device usage data in order to gain insight into how people are already using their smart devices. Then, using this information, she would like high-level recommendations for how these trends can inform Bellabeat marketing strategy."
  },
  {
    "objectID": "projects/bellabeat-analysis.html#project-deliverables",
    "href": "projects/bellabeat-analysis.html#project-deliverables",
    "title": "Bellabeat Analysis Case Study",
    "section": "Project deliverables",
    "text": "Project deliverables\nProduce a report with the following deliverables:\n\n\nA clear statement of the business task.\nA description of all data sources used.\nDocumentation of any cleaning or manipulation of data.\nA summary of your analysis.\nSupporting visualizations and key findings.\nYour top three recommendations based on your analysis."
  },
  {
    "objectID": "projects/bellabeat-analysis.html#key-findings",
    "href": "projects/bellabeat-analysis.html#key-findings",
    "title": "Bellabeat Analysis Case Study",
    "section": "Key findings",
    "text": "Key findings\n\nThe bikes were used by members more frequently than casual users, with a 24.12% higher usage rate in the last 12 months.\nHowever, casual riders had longer ride duration compared to members, with an average difference of 4-7 minutes.\nFrom a quarterly perspective, the third and second quarters of the year were the most popular seasons for casual riders.\nCasual riders exhibited a preference for riding bikes on Fridays, Saturdays, and Sundays, and they had longer trip duration during these days.\nAmong the rideable types, classic bikes were the most popular choice among casual riders.\nCasual riders were the only user group that utilized docked bikes.\n“Streeter Dr & Grand Ave” and “DuSable Lake Shore Dr & Monroe St” were the most common starting stations for casual users."
  },
  {
    "objectID": "projects/bellabeat-analysis.html#three-recommendations",
    "href": "projects/bellabeat-analysis.html#three-recommendations",
    "title": "Bellabeat Analysis Case Study",
    "section": "Three recommendations",
    "text": "Three recommendations\n\nCreate a marketing campaign that educates casual riders on their long-ride trips and invites them to a membership that can cater to their riding needs year-round. The campaign can be advertised both online and offline, with a focus on regions where the most popular stations for casual riders are located. This recommendation is supported by the data analysis, which highlights the long ride durations of casual riders and emphasizes the value of a membership that can provide continuous benefits.\nConsider creating a specialty membership specifically designed for docked-bike users. This targeted approach can be an effective way to engage casual riders who prefer docked bikes. However, instead of offering a separate membership, it may be more beneficial to incorporate docked-bike usage as an additional feature within the existing full-scale membership. This way, casual riders can experience the benefits of membership without creating a distinct membership category.\nDevelop a special introductory membership for the months between April and September or for the days of the week (Friday to Sunday). This limited-time membership can serve as a trial period, offering casual riders the opportunity to experience the benefits of a membership during the most popular seasons or days. Following the introductory period, you can then transition these riders to a regular yearly membership."
  },
  {
    "objectID": "posts/document.html#polar-plot",
    "href": "posts/document.html#polar-plot",
    "title": "Exploring Polar Coordinate Plots in Python Using Matplotlib",
    "section": "",
    "text": "A polar plot is a type of graph that presents data in a circular format, offering a fresh perspective beyond the conventional Cartesian coordinate system. In a polar plot, data points are represented using radial distance from a central point and angular positions, creating a distinct visual representation that emphasizes patterns and relationships in a unique way. This type of plot is particularly suitable for data that exhibits directional or cyclic patterns, such as time series data, circular data (e.g., wind direction), or any data with inherent periodicity. Polar plots excel at revealing cyclical trends and variations that might go unnoticed on traditional scatter plots. They allow us to easily observe changes in data values as they evolve over angles, providing insights into periodic behaviors, phase shifts, and synchronization. Whether in the realms of engineering, meteorology, or scientific research, polar plots offer a valuable tool for gaining deeper insights from complex datasets that might otherwise remain hidden in a sea of numbers.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define an array of values for the radial distance\nr = np.arange(0, 2, 0.01)\n\n# Convert the radial distance into angles\ntheta = 2 * np.pi * r\n\n# Create a figure and axis with polar projection\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n\n# Plot the data points on the polar axis\nax.plot(theta, r)\n\n# Customize the radial ticks to enhance readability\nax.set_rticks([0.5, 1, 1.5, 2])\n\n# Add grid lines for a clear reference\nax.grid(True)\n\n# Display the enchanting plot\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/document.html#decoding-the-code",
    "href": "posts/document.html#decoding-the-code",
    "title": "Exploring Polar Coordinate Plots in Python Using Matplotlib",
    "section": "Decoding the Code",
    "text": "Decoding the Code\nImporting Libraries: We start by importing the NumPy and Matplotlib libraries. These are our trusty tools for handling numerical calculations and creating visual wonders.\nDefining Radial Values: The r array represents the radial distance from the center. We use NumPy’s arange() function to generate values from 0 to 2 in increments of 0.01. This gives us the varying distances from the center.\nConverting to Angles: Here comes the magic! We calculate the corresponding angles in radians using the formula 2 * pi * r. This establishes the circular motion around the center point.\nCreating the Plotting Canvas: With Matplotlib’s help, we create a figure and axis with a polar projection. This sets the stage for our polar coordinate plot.\nPlotting the Data: Using the plot() function, we unveil our plot to the world! The theta values represent the angles, and the r values dictate the distance from the center. As we connect the dots, a beautiful pattern unfolds.\nRadial Ticks and Grid: To make our plot more user-friendly, we customize the radial ticks to show values of 0.5, 1, 1.5, and 2. We also add grid lines to assist in interpreting the plot.\nShowcasing the Masterpiece: Finally, we display our creation using plt.show(). Behold the captivating polar coordinate plot!"
  },
  {
    "objectID": "posts/document.html#your-voyage-into-polar-coordinates",
    "href": "posts/document.html#your-voyage-into-polar-coordinates",
    "title": "Exploring Polar Coordinate Plots in Python Using Matplotlib",
    "section": "Your Voyage into Polar Coordinates",
    "text": "Your Voyage into Polar Coordinates\nCongratulations! You’ve just embarked on a visual adventure that combines mathematics and artistry. Through a few lines of Python code, you’ve created a stunning polar coordinate plot using Matplotlib. As you delve deeper into the world of programming and data visualization, remember that every line of code has the potential to bring your imagination to life.\nSo go ahead, experiment with different values, and let your creativity flourish. The world of data visualization is at your fingertips, waiting for you to explore and innovate. Happy coding and plotting!"
  },
  {
    "objectID": "posts/document.html#lets-break-it-down-decoding-the-code",
    "href": "posts/document.html#lets-break-it-down-decoding-the-code",
    "title": "Exploring Polar Coordinate Plots in Python Using Matplotlib",
    "section": "Let’s Break It Down: Decoding the Code",
    "text": "Let’s Break It Down: Decoding the Code\n1. Importing Libraries: We start by importing the NumPy and Matplotlib libraries. These are our trusty tools for handling numerical calculations and creating visual wonders.\n2. Defining Radial Values: The r array represents the radial distance from the center. We use NumPy’s arange() function to generate values from 0 to 2 in increments of 0.01. This gives us the varying distances from the center.\n3. Converting to Angles: Here comes the magic! We calculate the corresponding angles in radians using the formula 2 * pi * r. This establishes the circular motion around the center point.\n4. Creating the Plotting Canvas: With Matplotlib’s help, we create a figure and axis with a polar projection. This sets the stage for our polar coordinate plot.\n5. Plotting the Data: Using the plot() function, we unveil our plot to the world! The theta values represent the angles, and the r values dictate the distance from the center. As we connect the dots, a beautiful pattern unfolds.\n6. Radial Ticks and Grid: To make our plot more user-friendly, we customize the radial ticks to show values of 0.5, 1, 1.5, and 2. We also add grid lines to assist in interpreting the plot.\n7. Showcasing the Masterpiece: Finally, we display our creation using plt.show(). Behold the captivating polar coordinate plot!"
  },
  {
    "objectID": "posts/document.html#conclusion-your-voyage-into-polar-coordinates",
    "href": "posts/document.html#conclusion-your-voyage-into-polar-coordinates",
    "title": "Exploring Polar Coordinate Plots in Python Using Matplotlib",
    "section": "Conclusion: Your Voyage into Polar Coordinates",
    "text": "Conclusion: Your Voyage into Polar Coordinates\nCongratulations! You’ve just embarked on a visual adventure that combines mathematics and artistry. Through a few lines of Python code, you’ve created a stunning polar coordinate plot using Matplotlib. As you delve deeper into the world of programming and data visualization, remember that every line of code has the potential to bring your imagination to life.\nSo go ahead, experiment with different values, and let your creativity flourish. The world of data visualization is at your fingertips, waiting for you to explore and innovate. Happy coding and plotting!"
  },
  {
    "objectID": "posts/matplotlib.html",
    "href": "posts/matplotlib.html",
    "title": "Exploring Polar Coordinate Plot in Python Using Matplotlib",
    "section": "",
    "text": "A polar plot is a type of graph that presents data in a circular format, offering a fresh perspective beyond the conventional Cartesian coordinate system. In a polar plot, data points are represented using radial distance from a central point and angular positions, creating a distinct visual representation that emphasizes patterns and relationships in a unique way. This type of plot is particularly suitable for data that exhibits directional or cyclic patterns, such as time series data, circular data (e.g., wind direction), or any data with inherent periodicity. Polar plots excel at revealing cyclical trends and variations that might go unnoticed on traditional scatter plots. They allow us to easily observe changes in data values as they evolve over angles, providing insights into periodic behaviors, phase shifts, and synchronization. Whether in the realms of engineering, meteorology, or scientific research, polar plots offer a valuable tool for gaining deeper insights from complex datasets that might otherwise remain hidden in a sea of numbers.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define an array of values for the radial distance\nr = np.arange(0, 2, 0.01)\n\n# Convert the radial distance into angles\ntheta = 2 * np.pi * r\n\n# Create a figure and axis with polar projection\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n\n# Plot the data points on the polar axis\nax.plot(theta, r)\n\n# Customize the radial ticks to enhance readability\nax.set_rticks([0.5, 1, 1.5, 2])\n\n# Add grid lines for a clear reference\nax.grid(True)\n\n# Display the enchanting plot\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/matplotlib.html#polar-plot",
    "href": "posts/matplotlib.html#polar-plot",
    "title": "Exploring Polar Coordinate Plot in Python Using Matplotlib",
    "section": "",
    "text": "A polar plot is a type of graph that presents data in a circular format, offering a fresh perspective beyond the conventional Cartesian coordinate system. In a polar plot, data points are represented using radial distance from a central point and angular positions, creating a distinct visual representation that emphasizes patterns and relationships in a unique way. This type of plot is particularly suitable for data that exhibits directional or cyclic patterns, such as time series data, circular data (e.g., wind direction), or any data with inherent periodicity. Polar plots excel at revealing cyclical trends and variations that might go unnoticed on traditional scatter plots. They allow us to easily observe changes in data values as they evolve over angles, providing insights into periodic behaviors, phase shifts, and synchronization. Whether in the realms of engineering, meteorology, or scientific research, polar plots offer a valuable tool for gaining deeper insights from complex datasets that might otherwise remain hidden in a sea of numbers.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define an array of values for the radial distance\nr = np.arange(0, 2, 0.01)\n\n# Convert the radial distance into angles\ntheta = 2 * np.pi * r\n\n# Create a figure and axis with polar projection\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n\n# Plot the data points on the polar axis\nax.plot(theta, r)\n\n# Customize the radial ticks to enhance readability\nax.set_rticks([0.5, 1, 1.5, 2])\n\n# Add grid lines for a clear reference\nax.grid(True)\n\n# Display the enchanting plot\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/matplotlib.html#lets-break-it-down-decoding-the-code",
    "href": "posts/matplotlib.html#lets-break-it-down-decoding-the-code",
    "title": "Exploring Polar Coordinate Plot in Python Using Matplotlib",
    "section": "Let’s Break It Down: Decoding the Code",
    "text": "Let’s Break It Down: Decoding the Code\n1. Importing Libraries: We start by importing the NumPy and Matplotlib libraries. These are our trusty tools for handling numerical calculations and creating visual wonders.\n2. Defining Radial Values: The r array represents the radial distance from the center. We use NumPy’s arange() function to generate values from 0 to 2 in increments of 0.01. This gives us the varying distances from the center.\n3. Converting to Angles: Here comes the magic! We calculate the corresponding angles in radians using the formula 2 * pi * r. This establishes the circular motion around the center point.\n4. Creating the Plotting Canvas: With Matplotlib’s help, we create a figure and axis with a polar projection. This sets the stage for our polar coordinate plot.\n5. Plotting the Data: Using the plot() function, we unveil our plot to the world! The theta values represent the angles, and the r values dictate the distance from the center. As we connect the dots, a beautiful pattern unfolds.\n6. Radial Ticks and Grid: To make our plot more user-friendly, we customize the radial ticks to show values of 0.5, 1, 1.5, and 2. We also add grid lines to assist in interpreting the plot.\n7. Showcasing the Masterpiece: Finally, we display our creation using plt.show(). Behold the captivating polar coordinate plot!"
  },
  {
    "objectID": "posts/matplotlib.html#conclusion-your-voyage-into-polar-coordinates",
    "href": "posts/matplotlib.html#conclusion-your-voyage-into-polar-coordinates",
    "title": "Exploring Polar Coordinate Plot in Python Using Matplotlib",
    "section": "Conclusion: Your Voyage into Polar Coordinates",
    "text": "Conclusion: Your Voyage into Polar Coordinates\nCongratulations! You’ve just embarked on a visual adventure that combines mathematics and artistry. Through a few lines of Python code, you’ve created a stunning polar coordinate plot using Matplotlib. As you delve deeper into the world of programming and data visualization, remember that every line of code has the potential to bring your imagination to life.\nSo go ahead, experiment with different values, and let your creativity flourish. The world of data visualization is at your fingertips, waiting for you to explore and innovate. Happy coding and plotting!"
  },
  {
    "objectID": "posts/reticulate.html",
    "href": "posts/reticulate.html",
    "title": "Python Setup in R with Reticulate",
    "section": "",
    "text": "# load r packages, including reticulate\nlibrary(tidyverse)\nlibrary(reticulate)\n\n# list current conda environments\nconda_list()\n# Set conda environment to use\nuse_condaenv(\"pydata-book\", required=TRUE)\n# Confirm the selected conda and python environemtn\npy_config()\n# Run some strings of python codes.\npy_run_string(\"import os as os\")\n\n# Very simple python function and an expression\nprint(\"Hello World!\")\n1+1\n\nHello World!\n\n\n2\n\n\n\n# package import and some data import\n\nimport numpy as np\nimport pandas as pd\n\narr = np.arange(1, 10)\nprint(type(arr))\narr\n\n# data frame creation\ndf = pd.DataFrame(data = {\"sequence\":np.arange(1,20,.01)})\ndf\ndf = df.assign(value=np.sin(df[\"sequence\"]))\ndf\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\n\n\n\n\nsequence\nvalue\n\n\n\n\n0\n1.00\n0.841471\n\n\n1\n1.01\n0.846832\n\n\n2\n1.02\n0.852108\n\n\n3\n1.03\n0.857299\n\n\n4\n1.04\n0.862404\n\n\n...\n...\n...\n\n\n1895\n19.95\n0.891409\n\n\n1896\n19.96\n0.895896\n\n\n1897\n19.97\n0.900294\n\n\n1898\n19.98\n0.904602\n\n\n1899\n19.99\n0.908819\n\n\n\n\n1900 rows × 2 columns\n\n\n\nVisualize the data frame using Matplotlib\n\nimport matplotlib.pyplot as plt\n\ndf.plot(x=\"sequence\", y = \"value\", title = \"Matplotlib\")\nplt.show()\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=0)\n\nX = [[ 1,  2,  3],  # 2 samples, 3 features\n     [11, 12, 13]]\n\ny = [0, 1]  # classes of each sample\n\nclf.fit(X, y)\n\nRandomForestClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\n\n\n\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.datasets import make_blobs\n\n# #############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,\n                            random_state=0)\n\n# Compute Affinity Propagation\naf = AffinityPropagation(preference=-50).fit(X)\ncluster_centers_indices = af.cluster_centers_indices_\nlabels = af.labels_\n\nn_clusters_ = len(cluster_centers_indices)\n\n# #############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.close('all')\nplt.figure(1)\nplt.clf()\n\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X[cluster_centers_indices[k]]\n    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\n    for x in X[class_members]:\n        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()"
  },
  {
    "objectID": "posts/reticulate.html#scikit-learn---random-forest",
    "href": "posts/reticulate.html#scikit-learn---random-forest",
    "title": "Python Setup in R with Reticulate",
    "section": "",
    "text": "from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=0)\n\nX = [[ 1,  2,  3],  # 2 samples, 3 features\n     [11, 12, 13]]\n\ny = [0, 1]  # classes of each sample\n\nclf.fit(X, y)\n\nRandomForestClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=0)"
  },
  {
    "objectID": "posts/reticulate.html#run-affinity-propagation",
    "href": "posts/reticulate.html#run-affinity-propagation",
    "title": "Python Setup in R with Reticulate",
    "section": "",
    "text": "from sklearn.cluster import AffinityPropagation\nfrom sklearn.datasets import make_blobs\n\n# #############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,\n                            random_state=0)\n\n# Compute Affinity Propagation\naf = AffinityPropagation(preference=-50).fit(X)\ncluster_centers_indices = af.cluster_centers_indices_\nlabels = af.labels_\n\nn_clusters_ = len(cluster_centers_indices)\n\n# #############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.close('all')\nplt.figure(1)\nplt.clf()\n\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X[cluster_centers_indices[k]]\n    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\n    for x in X[class_members]:\n        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()"
  },
  {
    "objectID": "posts/p4da3d-data-types.html",
    "href": "posts/p4da3d-data-types.html",
    "title": "[3] Built-In Data Structures, Functions, and Files",
    "section": "",
    "text": "Python Tuples: Immutable, ordered collections that allow you to store a sequence of elements. Tuples are defined using parentheses and can hold a mix of data types. They offer efficient data storage, are iterable, and can be used as keys in dictionaries. In this blog, we’ll explore the versatility of Python tuples and how to leverage them for various data analysis tasks.\n\n# create some tuples\ntup = (4, 5, 6)\ntup2 = 7, 8, 9\nprint(tup)\nprint(tup2)\nt = (tup, tup2)    # tuple of tuples\nprint(t)\ntype(t)\n\n# convert any sequence or iterator to tuple\nprint(tuple(['a', 'b', 'c']))\nprint(tuple('Some String'))\n\n# access elements\nnested_tup = ((4,5,6), (7,8))\nprint(nested_tup[1])\n\n(4, 5, 6)\n(7, 8, 9)\n((4, 5, 6), (7, 8, 9))\n('a', 'b', 'c')\n('S', 'o', 'm', 'e', ' ', 'S', 't', 'r', 'i', 'n', 'g')\n(7, 8)\n\n\nTuple itself is immutable; however, if an object inside a tuple is mutable, you can modify it in place.\n\ntup = tuple(['The bown ','fox ', 'jumped over the ',['fence.']])\nprint(tup)\n# tup[1] = 'wolf'           # error\ntup[3][0] = 'window '     # ok\nprint(tup)\ntup[3].append('fence')    # ok\nprint(tup)\n\n('The bown ', 'fox ', 'jumped over the ', ['fence.'])\n('The bown ', 'fox ', 'jumped over the ', ['window '])\n('The bown ', 'fox ', 'jumped over the ', ['window ', 'fence'])\n\n\nConcatenation, Variable swap, Unpacking\n\n# concatenation using +\ntup = (4, None, 'foo') + (6, 0) + ('bar',)\nprint(tup)\n\n# Swap\na, b = 1, 2\nprint(a, b)\nb, a = a, b\nprint(a, b)\n\n# Unpacking by iterating over sequences of tuples, lists\nseq = [('a1', 'a2', 'a3'),('b4', 'b5', 'b6'), ('c7', 'c8', 'c9')]\nprint(seq, type(seq))\nfor a, b, c in seq:\n  print(f'a={a}, b={b}, c={c}')\n  \n# unpacking using assignment, not concerning some trailing values\nvalues = 1, 2, 3, 4, 5, 6, 7\na, b, *_ = values     # *rest works the same as *_\nprint (a, b, *_)\n\n# method .count()\na = (1, 2, 2, 2, 3, 4, 2)\nprint(a.count(2))     # number of occurences\n\n(4, None, 'foo', 6, 0, 'bar')\n1 2\n2 1\n[('a1', 'a2', 'a3'), ('b4', 'b5', 'b6'), ('c7', 'c8', 'c9')] &lt;class 'list'&gt;\na=a1, b=a2, c=a3\na=b4, b=b5, c=b6\na=c7, b=c8, c=c9\n1 2 3 4 5 6 7\n4\n\n\n\n\n\nPython Lists: Ordered, mutable collections that allow you to store and manipulate sequences of items. Lists are versatile and can hold various data types. They support methods for adding, removing, and modifying elements, making them a fundamental data structure for data analysis and manipulation in Python.\n\n# create, convert lists\na_list = [2, 3, 7, None]\ntup = (\"foo\", \"bar\", \"baz\")\nb_list = list(tup)\nprint(b_list, type(b_list))\n\n# Access, modify list element\nb_list[1] = \"peekaboo\"\nprint(b_list)\n\n# Materialize an iterator / generator\ngen = range(20) # generator, not materialized yet.\nprint(gen) \nlist(gen) # materialize the generator to list\n\n# Work with the elements\nb_list.append(\"dwarf\")     # add to the end of list\nb_list.append(\"foo\")\nprint(b_list)\nb_list.insert(1, \"RED\")     # insert at specific location, consider using collections.deque for efficiency\nprint(b_list)\nb_list.remove(\"foo\")        # remove the first occurence only\nprint(b_list)\n\n# Check if element in the list\nprint(\"dwarf\" in b_list)\nprint(\"dwarf\" not in b_list)\n\n['foo', 'bar', 'baz'] &lt;class 'list'&gt;\n['foo', 'peekaboo', 'baz']\nrange(0, 20)\n['foo', 'peekaboo', 'baz', 'dwarf', 'foo']\n['foo', 'RED', 'peekaboo', 'baz', 'dwarf', 'foo']\n['RED', 'peekaboo', 'baz', 'dwarf', 'foo']\nTrue\nFalse\n\n\n\n# Concatenate and combine lists\nnew_list = [4, None, \"foo\"] + [7, 8, (2, 3)]\nprint(new_list)\nprint(b_list)\nnew_list.extend(b_list)\nprint(new_list)\n\n# Sorting in place\na = [\"saw\", \"small\", \"He\", \"foxes\", \"six\"]\na.sort() # default\nprint(a)\na.sort(key=len) # sort by key with given function, by the length of each element\nprint(a)\n\n# Sorted copy\na = [\"saw\", \"small\", \"He\", \"foxes\", \"six\"]\nsorted_copy = sorted(a)\nprint(sorted_copy)\n\n[4, None, 'foo', 7, 8, (2, 3)]\n['RED', 'peekaboo', 'baz', 'dwarf', 'foo']\n[4, None, 'foo', 7, 8, (2, 3), 'RED', 'peekaboo', 'baz', 'dwarf', 'foo']\n['He', 'foxes', 'saw', 'six', 'small']\n['He', 'saw', 'six', 'foxes', 'small']\n['He', 'foxes', 'saw', 'six', 'small']\n\n\n\n# Slices\n\nseq = [7, 2, 3, 7, 5, 6, 0, 1]\nprint(seq)\nprint(seq[1:5])   # index:1 - index: 4\nprint(seq[:5])    # index:beginning - index: 4\nprint(seq[5:])    # index:5 - index:last\nprint(seq[-4:])   # Indexing from the last, index:-4 to the end\nprint(seq[-6:-2]) # Indexing from the last, index: -6 to index: -2\n\nprint(\"-\"*30)\n\n# Stepping the list\nprint(seq[::2])     # Extract index 0, 2, 4, 6 ....\nprint(seq[::3])     # Extract index 0, 3, 6 ....\nprint(seq[::-2])    # Extract index -1, -3, -5 .....\nprint(seq[::-1])    # Extract index -1, -2, -3, -4 ... effectively reverse the list or tuple\n\n\nprint(\"-\"*30)\n\n# Slices replacement\nseq[3:5] = ['A','B']\nprint(seq)\n\n[7, 2, 3, 7, 5, 6, 0, 1]\n[2, 3, 7, 5]\n[7, 2, 3, 7, 5]\n[6, 0, 1]\n[5, 6, 0, 1]\n[3, 7, 5, 6]\n------------------------------\n[7, 3, 5, 0]\n[7, 7, 0]\n[1, 6, 7, 2]\n[1, 0, 6, 5, 7, 3, 2, 7]\n------------------------------\n[7, 2, 3, 'A', 'B', 6, 0, 1]\n\n\n\n\n\nPython Dictionaries: Versatile and dynamic data structures that store key-value pairs, allowing for efficient data retrieval and manipulation. Dictionaries are unordered collections that provide a way to map unique keys to associated values, making them invaluable for tasks like data storage, lookup tables, and configuration settings\n\n# Create a Dictionary (aka, hash-map, associative arrays)\n\nempty_dict = {} \nprint(empty_dict)\nd1 = {\"a\": \"some value\", \"b\": [1, 2, 3, 4]}  # key-value pair\nprint(d1)\n\n# Access, insert and set\nd1[7] = \"an integer\"      # 7:\"an integer\" pair\nprint(d1)\nprint(d1[\"b\"])            # lookup with key\n#print(d1[\"address\"])     # KeyError\nprint(\"address\" in d1)    # key exists?\n\nprint(\"-\"*30)\n\n# Delete values\ndel d1[7]              # delete with key\n#del d1[\"country\"]     # KeyError\nprint(d1)\nret = d1.pop(\"b\")      # delete with key and return the value\nprint(ret)\nprint(d1)\n\nprint(\"-\"*30)\n\n# Iteration using keys() values() items()\nd1 = {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\nprint(type(d1.keys()))     # &lt;class 'dict_keys'&gt;\nprint(type(d1.values()))   # &lt;class 'dict_values'&gt;\n\nd1_keys = list(d1.keys())   \nd1_values = list(d1.values())\nd1_items = list(d1.items())  # Gives a list of tuples in (key, value)\nprint(d1_keys)\nprint(d1_values)\nprint(d1_items)  \n\n# iteration\nfor key, value in d1.items():\n  print(f'Key: \\\"{key}\\\" has value of \\\"{value}\\\".')\n\n{}\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\n[1, 2, 3, 4]\nFalse\n------------------------------\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n[1, 2, 3, 4]\n{'a': 'some value'}\n------------------------------\n&lt;class 'dict_keys'&gt;\n&lt;class 'dict_values'&gt;\n['a', 'b', 7]\n['some value', [1, 2, 3, 4], 'an integer']\n[('a', 'some value'), ('b', [1, 2, 3, 4]), (7, 'an integer')]\nKey: \"a\" has value of \"some value\".\nKey: \"b\" has value of \"[1, 2, 3, 4]\".\nKey: \"7\" has value of \"an integer\".\n\n\n\n# Update dictionary with a dictionary of new or changes of key-value pairs\nd1 = {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\nprint(d1)\nd1.update({\"b\": \"foo\", \"c\": 12})\nprint(d1)\n\n{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\n{'a': 'some value', 'b': 'foo', 7: 'an integer', 'c': 12}\n\n\n\n# Create a dictionary from two sequences\n\nlist_country = [\"USA\", \"Canada\", \"England\", \"France\"]\nlist_city = [\"Washigton\", \"Ottawa\", \"London\", \"Paris\"]\nprint(list_country)\nprint(list_city)\n\n# Using for loop - zip and iterate\nprint(zip(list_country, list_city))\n\nmapping = {}\nfor key, value in zip(list_country, list_city):\n  mapping[key] = value\nprint(mapping, type(mapping))\n\n# Using dict() method\ncountry_city_tuples = zip(list_country, list_city)\nprint(country_city_tuples, type(country_city_tuples))\nmapping = dict(country_city_tuples)\nprint(mapping, type(mapping))\n\n['USA', 'Canada', 'England', 'France']\n['Washigton', 'Ottawa', 'London', 'Paris']\n&lt;zip object at 0x000001BD9F64EB00&gt;\n{'USA': 'Washigton', 'Canada': 'Ottawa', 'England': 'London', 'France': 'Paris'} &lt;class 'dict'&gt;\n&lt;zip object at 0x000001BD9F64C2C0&gt; &lt;class 'zip'&gt;\n{'USA': 'Washigton', 'Canada': 'Ottawa', 'England': 'London', 'France': 'Paris'} &lt;class 'dict'&gt;\n\n\n\n# Build a dictionary WUTHOUT using setdefault()\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\nprint(words, type(words))\n\nby_letter = {}\n\nfor word in words:\n  letter = word[0] # Using the first letter of each word as key\n  if letter not in by_letter:\n    by_letter[letter] = [word]\n  else:\n    by_letter[letter].append(word)\n\nprint(by_letter)\n\nprint(\"-\"*55)\n\n# Simplified version using setdefault()\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\nby_letter = {}\nfor word in words:\n  letter = word[0] # Using the first letter of each word as key\n  by_letter.setdefault(letter, []).append(word) # If the letter (the first letter of the current word) is already a key in the dictionary, it returns the corresponding value (which is a list) for that key. If the key doesn't exist, it initializes an empty list. The append() method is then used to add the current word to the list associated with the letter key.\nprint(by_letter)\n\nprint(\"-\"*55)\n\n# Using defaultdict from collections module\nfrom collections import defaultdict\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\nby_letter = defaultdict(list) # if you access a key that doesn't exist, it will automatically create that key with an empty list as the associated value. This is particularly useful when you want to avoid KeyError exceptions when working with dictionaries.\nprint(by_letter, type(by_letter))\n\nfor word in words:\n  by_letter[word[0]].append(word)\n\nprint(by_letter)\n\n['apple', 'bat', 'bar', 'atom', 'book'] &lt;class 'list'&gt;\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n-------------------------------------------------------\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n-------------------------------------------------------\ndefaultdict(&lt;class 'list'&gt;, {}) &lt;class 'collections.defaultdict'&gt;\ndefaultdict(&lt;class 'list'&gt;, {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']})\n\n\nUnderstanding Dictionary Key Types\nIn Python, dictionaries are versatile data structures used for mapping keys to corresponding values. While dictionaries allow flexibility in storing values of any Python data type as their values, there are specific requirements and considerations when it comes to selecting suitable key types. This aspect of dictionary design is crucial to ensure efficient and reliable key-value pair lookups and operations.\nKey Characteristics of Dictionary Keys\n\nImmutability Requirement:\nThe primary requirement for dictionary keys is that they should generally be immutable objects. Immutability refers to the property of an object that prevents its state from being modified after it is created.\nExamples of Immutable Key Types:\n\nScalar Types: Examples of immutable scalar types suitable as dictionary keys include integers (int), floating-point numbers (float), and strings (str). These types are inherently immutable, which means once you create a key with a specific value, that key-value pair remains consistent.\nTuples as Composite Keys: Tuples can also serve as dictionary keys, but there’s an important condition: all the objects within the tuple must also be immutable. This ensures the stability of the key, as the tuple itself remains unchanged.\n\n\nThe Significance of Hashability\nThe technical term that ties these requirements together is “hashability.” Hashability refers to an object’s ability to be hashed, which involves converting the object into a fixed-size numerical value, known as a hash code. This hash code is used as an index to efficiently store and retrieve key-value pairs in a dictionary. The key feature of hashable objects is that they produce the same hash code consistently.\nThe Role of the hash() Function\nYou can determine whether an object is hashable and can be used as a dictionary key by utilizing the built-in hash() function. When you apply the hash() function to an object, it returns a hash code if the object is hashable. However, keep in mind that not all Python objects are hashable, particularly mutable objects like lists and dictionaries.\n\n# Using hash() to validate key types in dictionary\n\nhash(\"string\")           # hashable\nhash((1, 2, (2, 3)))     # hashable\n# hash((1, 2, [2, 3]))     # Not hashable due to mutable element - list of [2, 3]\n\n# When in need of list for key, convert it tp tuple, hashable\nd = {}\nd[tuple([1,2,3])] = 5\nprint(d)  # dictionary with a tuple as key\n\n{(1, 2, 3): 5}\n\n\n\n\n\nSets in Python: A Set is an essential data structure that represents an unordered collection of unique elements. Unlike lists or tuples, which allow duplicates, sets only store distinct values. This characteristic makes sets ideal for various tasks, such as removing duplicates from a list, checking for membership, and performing set operations like union, intersection, and difference. One of the most common real-world use cases for sets is in data deduplication, where they excel at efficiently eliminating duplicate records from datasets, ensuring data integrity and enhancing the performance of data processing tasks.\n\n# Create a set\nprint(set([2, 2, 2, 1, 3, 3]))\nprint\n\n# Union, Difference, Intersection and Symmetric Difference\na = {1, 2, 3, 4, 5}\nb = {3, 4, 5, 6, 7, 8}\nprint (\"Set a:\", a, \" Set b:\", b)\n\n# a.union(b) = a | b\nprint(\"a union b: \", a.union(b), a | b)\n# a.intersection(b) = a & b\nprint(\"a intersection b: \", a.intersection(b), a & b)\n# a.difference(b) = a - b\nprint(\"a difference b: \", a.difference(b), a - b)\n# a.symmetric_difference(b)\nprint(\"a.symmetric_difference(b): \", a.symmetric_difference(b), a ^ b)\n\n{1, 2, 3}\nSet a: {1, 2, 3, 4, 5}  Set b: {3, 4, 5, 6, 7, 8}\na union b:  {1, 2, 3, 4, 5, 6, 7, 8} {1, 2, 3, 4, 5, 6, 7, 8}\na intersection b:  {3, 4, 5} {3, 4, 5}\na difference b:  {1, 2} {1, 2}\na.symmetric_difference(b):  {1, 2, 6, 7, 8} {1, 2, 6, 7, 8}\n\n\n\n# Common set operations\na = {1, 2, 3, 4, 5}\n\n# a.add(x) - Add x to the set\na.add(\"Fox\")\nprint(a)\n# a.remove(x) - Remove x from the set\na.remove(\"Fox\")\nprint(a)\n# a.pop() - Remove an element arbitrarily, set must not be empty\nret = a.pop()\nprint(a, ret)\n\n\n\n# a.clear() - reset to empty\nprint(a.clear())  # None - empty\n\n{1, 2, 3, 4, 5, 'Fox'}\n{1, 2, 3, 4, 5}\n{2, 3, 4, 5} 1\nNone\n\n\nBuilt-in Sequence Functions\n\na_list = [7, 1, 2, 6, 0, 3, 2]\n\n# Very useful, use at every opportunities\n\n# enumerate() - no need to write code to track index\nfor index, value in enumerate(a_list):\n  print(index, \" : \", value)  \n\n# sorted() - a new srted list from the elements of any sequence\nprint(sorted(a_list))\nprint(sorted(\"horse race\"))  # string is a sequence\n\n# zip() - pairs up sequences element-wise to create a list\nseq1 = [\"foo\", \"bar\", \"baz\"]\nseq2 = [\"one\", \"two\", \"three\"]\nseq3 = [\"BROWN\", \"FOX\", \"FENCE\", \"JUMP\"]\nzipped = zip(seq1, seq2, seq3)\nprint(list(zipped))  # \"JUMP\" ignored for pariing!\n\n# zip() commonly works with enumerate() for iteration - zip and iterate\nfor index, (a, b) in enumerate(zip(seq1, seq2)):\n  print(f\"{index}: {a}, {b}\")\n\n# reversed()\nreversed(range(10)) # range_iterator object, it's a generator\nprint(list(reversed(range(10)))) # list() materializes the generator\n\n0  :  7\n1  :  1\n2  :  2\n3  :  6\n4  :  0\n5  :  3\n6  :  2\n[0, 1, 2, 2, 3, 6, 7]\n[' ', 'a', 'c', 'e', 'e', 'h', 'o', 'r', 'r', 's']\n[('foo', 'one', 'BROWN'), ('bar', 'two', 'FOX'), ('baz', 'three', 'FENCE')]\n0: foo, one\n1: bar, two\n2: baz, three\n[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\n\n\n\nComprehensions in Python offer concise, readable, and often more efficient ways to create lists, sets, and dictionaries. They reduce code verbosity, enhance readability, and promote a functional programming style, ultimately leading to cleaner and more maintainable code.\nList Comprehension:\nList comprehensions provide a concise way to create lists by applying an expression to each item in an iterable (e.g., a list, tuple, or range) and optionally filtering the items based on a condition.\n\n## [exp for value in collection if condition]\n\n# create a list of strings in uppercase only if the string has more than two characters.\nstrings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\nstrings_2 = [ str.upper() for str in strings if len(str) &gt; 2 ]\nprint(strings_2)\n\n# create a list of squares of even numbers from 0 to 9\neven_squares = [ x ** 2 for x in range(10) if x % 2 == 0 ]\nprint(even_squares)\n\n['BAT', 'CAR', 'DOVE', 'PYTHON']\n[0, 4, 16, 36, 64]\n\n\nSet Comprehension:\nSet comprehensions allow you to create sets in a similar manner to list comprehensions, but with the guarantee of uniqueness among elements.\n\n## set_comp = {expr for value in collection if condition}\n\n# Create a set using strings list above, containing only the lengths of strings.\nstrings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\nunique_lengths = { len(str) for str in strings } # uniqueness guaranted\nprint(unique_lengths)\n\n# map() - even more powerful\nstrings_map = set(map(len, strings))\nprint(strings_map)\n\n# Create a set of unique even squares from 0 to 9\neven_squares_set = {x ** 2 for x in range(10) if x % 2 == 0}\nprint(even_squares_set)\n\n{1, 2, 3, 4, 6}\n{1, 2, 3, 4, 6}\n{0, 64, 4, 36, 16}\n\n\nDictionary Comprehension:\nDictionary comprehensions enable the creation of dictionaries by defining key-value pairs based on expressions applied to items in an iterable.\n\n## {key_expression: value_expression for item in iterable if condition}\n\n# Create a dictionary, mapping even numbers to their squares from 0 to 9\neven_squares_dict = { x: x**2 for x in range(10) if x % 2 == 0 }\nprint(even_squares_dict)\n\n# Create a dictionary of lookup map of string for the location of each string in the list\nstrings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\nstring_lookup_dict = { str: index for index, str in enumerate(strings) }\nprint(strings)\nprint(string_lookup_dict)\n\n{0: 0, 2: 4, 4: 16, 6: 36, 8: 64}\n['a', 'as', 'bat', 'car', 'dove', 'python']\n{'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}\n\n\nNested list comprehensions\nThe order of\n\n# List of lists with some English and Spanish names - List nesting lists\nall_data = [\n  [\"John\", \"Emily\", \"Michael\", \"Mary\", \"Steven\"],\n  [\"Maria\", \"Juan\", \"Javier\", \"Natalia\", \"Pilar\"]\n]\nprint(all_data)\nprint(\"-\"*55)\n\n# Create a list of names contating two of more of 'a'\n# The non-Pythonic apporach using for loop..\nnames_of_interest = []\nfor names in all_data:\n  print(\"Processing:\", names)\n  enough_as = [ name for name in names if name.count(\"a\") &gt;=2 ]\n  print(enough_as)\n  names_of_interest.extend(enough_as)\nprint(\"Results: \", names_of_interest)\n\n# Re-writtine in one-liner using nested list comprehension\nnames_of_interest = [ name for names in all_data for name in names if name.count(\"a\") &gt;=2 ]\nprint(\"Result using Nested List Comprehension: \", names_of_interest)\n\n# Flatten a list of tuples of integers into a simple list of integers using nested list comprehension\nsome_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\nprint(some_tuples, type(some_tuples)) # list of tuples with integers\nflattened = [ int for tuple in some_tuples for int in tuple ]\nprint(\"Flattened: \", flattened, type(flattened))  # flattend list\n\n# Readability and valid forms\nflattened = []\nprint(\"Init. flattened list: \", flattened)\nprint(\"some_tuples: \", some_tuples)\nfor tup in some_tuples:\n  for x in tup:\n    flattened.append(x)\nprint(\"flattend from some_tuples: \", flattened)\n\n# list comprehenson inside list comprehension\n# Change list of tuples to a list of lists\nprint(\"Init. list of some_tuples: \", some_tuples)\nnew_list_of_lists = [[ x for x in tup ] for tup in some_tuples ]\nprint(\"List of lists from some_tuples: \", new_list_of_lists)\n\n[['John', 'Emily', 'Michael', 'Mary', 'Steven'], ['Maria', 'Juan', 'Javier', 'Natalia', 'Pilar']]\n-------------------------------------------------------\nProcessing: ['John', 'Emily', 'Michael', 'Mary', 'Steven']\n[]\nProcessing: ['Maria', 'Juan', 'Javier', 'Natalia', 'Pilar']\n['Maria', 'Natalia']\nResults:  ['Maria', 'Natalia']\nResult using Nested List Comprehension:  ['Maria', 'Natalia']\n[(1, 2, 3), (4, 5, 6), (7, 8, 9)] &lt;class 'list'&gt;\nFlattened:  [1, 2, 3, 4, 5, 6, 7, 8, 9] &lt;class 'list'&gt;\nInit. flattened list:  []\nsome_tuples:  [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\nflattend from some_tuples:  [1, 2, 3, 4, 5, 6, 7, 8, 9]\nInit. list of some_tuples:  [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\nList of lists from some_tuples:  [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\n# 6 Practical use caes of nested list comprehension\n\n# 1. Matrix operations:\n# Add 1 to each element in a 2D matrix\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nmatrix_add_1 = [[ element + 1 for element in row] for row in matrix]\nprint(\"matrix_add_1: \", matrix_add_1)\n\n# 2. Data transformation:\n# Filter rows with even some from a list of lists\ndata = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nfiltered_data = [ row for row in data if sum(row) % 2 == 0 ]\nprint(\"filtered_data: \", filtered_data)\n\n# 3. Flattening lists of lists:\nnested_lists = [[1, 2], [3, 4], [5, 6]]\nflattened_list = [ item for sublist in nested_lists for item in sublist ]\nprint(\"flattened_list: \", flattened_list)\n\n# 4. Combining data:\n# Create all possible pairs (in tuple) from two lists - Different from element-wise pairing using zip()\nlist1 = [1, 2, 3]\nlist2 = ['a', 'b', 'c']\ncombinations = [ (x, y) for x in list1 for y in list2 ]\nprint(\"combinations: \", combinations)\n\n# 5. Conditional filtering:\n# Filter rows containing 'John' from a list of lists\ndata = [['Alice', 25], ['John', 30], ['Bob', 35]]\nfiltered_data = [ list for list in data if 'John' in list ]\nprint(\"filtered_data: \", filtered_data)\n\n# 6. Transposing data:\n# Transpose a given matrix (switching rows and columns)\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\ntransposed_matrix = [[ (row[i], print(\"row[i]: \", row[i]), print(\"processing row: \", row)) for row in matrix ] for i in range(len(matrix[0]))]\n#print(\"transposed_matrix: \", transposed_matrix)\n#print(len(matrix[0]))\n#print(range(len(matrix[0])))\n\nmatrix_add_1:  [[2, 3, 4], [5, 6, 7], [8, 9, 10]]\nfiltered_data:  [[1, 2, 3], [7, 8, 9]]\nflattened_list:  [1, 2, 3, 4, 5, 6]\ncombinations:  [(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]\nfiltered_data:  [['John', 30]]\nrow[i]:  1\nprocessing row:  [1, 2, 3, 4]\nrow[i]:  5\nprocessing row:  [5, 6, 7, 8]\nrow[i]:  9\nprocessing row:  [9, 10, 11, 12]\nrow[i]:  2\nprocessing row:  [1, 2, 3, 4]\nrow[i]:  6\nprocessing row:  [5, 6, 7, 8]\nrow[i]:  10\nprocessing row:  [9, 10, 11, 12]\nrow[i]:  3\nprocessing row:  [1, 2, 3, 4]\nrow[i]:  7\nprocessing row:  [5, 6, 7, 8]\nrow[i]:  11\nprocessing row:  [9, 10, 11, 12]\nrow[i]:  4\nprocessing row:  [1, 2, 3, 4]\nrow[i]:  8\nprocessing row:  [5, 6, 7, 8]\nrow[i]:  12\nprocessing row:  [9, 10, 11, 12]\n\n\n\n\n\n\nFunctions in Python are a cornerstone of the language, offering powerful tools for structuring and modularizing code. Python functions come with several unique features that distinguish them from functions in other programming languages.\n\nVersatile and Expressive: Python functions support both positional and keyword arguments, enabling flexible and expressive function calls.\nMultiple Return Values: Python functions can return multiple values, simplifying the packaging and unpacking of results.\nDefault Argument Values: Python functions allow for default argument values, making function usage more straightforward by providing sensible defaults when arguments are omitted.\nLambda Functions: Python features lambda functions for concise and anonymous function definitions, particularly useful for simple operations.\nFirst-Class Functions: Python treats functions as first-class citizens, allowing them to be assigned to variables, passed as arguments, and returned from other functions. This flexibility enhances code modularity, maintainability, and readability.\nModular and Reusable: Python’s function-centric approach encourages the creation of clean, reusable, and modular code, making it a crucial topic for all Python programmers to master.\n\n\n# simple declaration and calling\ndef my_function(x,y):\n  return x + y\n\nret = my_function(1, 2)\nprint(ret)\n\n# Return None when no retorn statement is encountered\ndef funciton_without_return(x):\n  print(x)\nresult = funciton_without_return(\"hello!\")\nprint(result)\n\n# positional and keyword arguments\ndef my_function2(x, y, z=1.5):  # z as keyword argument with default value\n  if z &gt; 1:\n    return z * (x + y)\n  else:\n    return z / (x + y)\n\n# Naespace, scope and local functions\n\n\nprint(my_function2(5, 6, z=0.7))  # no need to put arg names, keword arg optional\nprint(my_function2(3.14, 7, 3.5))  # no need to specify name for kewoord arguments\nprint(my_function2(10, 20))   # keyword arg optional\n\n\na = []\ndef func():\n  for i in range(5):\n    a.append(i)\n\nfunc()\n\n3\nhello!\nNone\n0.06363636363636363\n35.49\n45.0\n\n\n\n# Assigining variable outside the function's scope\n\na = None\ndef bind_a_variable():\n  global a\n  a = []\n  \nbind_a_variable()\nprint(a)\n\n# multiple returns\ndef f():\n  a = 5\n  b = 6\n  c = 7\n  return a, b, c\n\na, b, c = f()\nprint(a, b)\n\n[]\n5 6\n\n\n\n\nIn Python, functions are first-class objects, which means they are treated like any other object, such as integers, strings, or lists. This concept is a fundamental part of the Python language and has several implications and practical uses.\n\nFunctions can be assigned to variables:\n\n\n# Functions can be assigned to variables:\n\ndef greet(name):\n    return f\"Hello, {name}!\"\n\ngreeting = greet  # function assigned to variable \nprint(greeting(\"Alice\"))  # calling the function using the assigned variable\n\nprint(id(greet), id(greeting)) # both variable and function references the same object\n\nHello, Alice!\n1913991838304 1913991838304\n\n\n\nFunctions can be passed as arguments: This is particularly useful for functions that require a callback or custom behavior.\n\n\n# Functions can be passed as arguments\n\ndef apply(func, value):\n  return func(value)\n\ndef double(x):\n  return x * 2\n\nresult = apply(double, 5)\nprint(result)\n\n10\n\n\n\nFunctions can be returned from other functions: This enables dynamic function generation.\n\n\n# Functinos can be returned from other functions\ndef create_multiplier(factor):\n  \"\"\"\n  Returns a FUNCTION that produce the result of given factor * x\n  \"\"\"\n  def multiplier(x):\n    return x * factor\n  return multiplier    # return the inner function\n\ndouble = create_multiplier(2)\nprint(double(200))  # Result is a function returned by 'created_multiplier'\nprint(double(400))\n\n400\n800\n\n\n\n\n\nFunctions can be stored in data structures like lists, dictionaries, or sets, making it possible to manage and manipulate functions dynamically.\n\n# Functinos can be stored in data structures\n\ndef square(x):\n  return x**2\n\ndef cube(x):\n  return x**3\n\nmath_functions = [square, cube]\nresult = math_functions[1](3)  # access and call the 'cube' function, give 3 to the parameter\nprint(result)\n\n\n# Using a list of functinos for cleaning strings from survey data\nanswer_states = [\"   alabama \", \"georgia!\", \"Georgia\",  \\\n          \"georgia\", \"flOrIda\", \"south   carolina##\",   \\\n          \"West virginia?\"]\nprint(answer_states)\n          \ndef remove_punctuation(value):\n  return re.sub(\"[!#?]\", \"\", value)\n\nclean_ops = [str.strip, remove_punctuation, str.title] # 3 functinos in list for cleaning operation\n\nimport re\n\ndef clean_strings(strings, ops):\n    result = []\n    for value in strings:\n        for func in ops:\n            value = func(value)\n        result.append(value)\n    return result\n  \nprint(\"strings_cleaned: \", clean_strings(answer_states, clean_ops))\n\nprint(\"-\"*55)\n\n\n# Using map() to apply a function as argument to a sequence\nfor x in map(remove_punctuation, answer_states):\n  print(x)\n\n27\n['   alabama ', 'georgia!', 'Georgia', 'georgia', 'flOrIda', 'south   carolina##', 'West virginia?']\nstrings_cleaned:  ['Alabama', 'Georgia', 'Georgia', 'Georgia', 'Florida', 'South   Carolina', 'West Virginia']\n-------------------------------------------------------\n   alabama \ngeorgia\nGeorgia\ngeorgia\nflOrIda\nsouth   carolina\nWest virginia\n\n\n\n\n\nAnonymous functions, often referred to as “lambda functions,” are a concise way to create a single line, inline functions without giving them a formal name. In Python, lambda functions are defined using the lambda keyword, followed by parameters and an expression. While they are limited in scope compared to regular functions, they offer several benefits:lambda arguments: expression\nStructure: lambda arguments: expression\nConciseness: Lambda functions are compact and allow you to define simple operations in a single line of code, making them particularly useful for short, one-off functions.\nReadability: When used appropriately, lambda functions can improve code readability by encapsulating a specific operation right where it’s needed, reducing the need for named functions or temporary variables.\nFunctional Programming: Lambda functions are a key component of functional programming in Python. They can be used with higher-order functions like map(), filter(), and reduce() to perform operations on collections of data.\n\n# sort a list of tuples by the second element:\ndata = [(1, 5), (3, 2), (2, 8)]\nsorted_data = sorted(data, key=lambda x: x[1])  # extracts the second element (index 1) of each tuple in the data list\nprint(sorted_data)\n\n[(3, 2), (1, 5), (2, 8)]\n\n\n\n# filter even numbers from a list\nnumbers = [1, 2, 3, 4, 5, 6]\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))  # filtering function, iterables\nprint(even_numbers)\n\n[2, 4, 6]\n\n\n\n# Mapping a Function to a List\n\nvalues = [1, 2, 3, 4, 5]\nsquared_values = list(map(lambda x: x**2, values))\nprint(squared_values)\n\n[1, 4, 9, 16, 25]\n\n\n\ndef short_function(x):\n  return x * 2\nprint(short_function(100))\n\nequiv_amon = lambda x: x * 2  # assign a function to variable\nprint(type(equiv_amon))\nprint(equiv_amon(100))  # produces the same result\nprint(\"-\"*55)\n\n# use a lambda function for the second argument\ndef apply_to_list(some_list, f):\n  \"\"\"\n  Apply the given function to the given list\n  \"\"\"\n  return [f(x) for x in some_list]\nints = [4, 0, 1, 5, 6]\n\nret = apply_to_list(ints, lambda x: x*2)\nprint(ret)\n\n# sort the list of strings based on the number of unique charaters for each string.\nstrings = [\"foo\", \"card\", \"bar\", \"aaaa\", \"abab\"]\nstrings.sort(key=lambda x: len(set(x))) # sort by number of unique characters per string element\nprint(strings)\n\n200\n&lt;class 'function'&gt;\n200\n-------------------------------------------------------\n[8, 0, 2, 10, 12]\n['aaaa', 'foo', 'abab', 'bar', 'card']\n\n\n\n\n\nGenerators are a powerful and memory-efficient feature in Python for working with sequences of data. They allow you to create iterators on the fly, enabling the processing of large data sets or infinite sequences without storing them in memory. Generators are defined using functions but use the yield keyword to produce values one at a time. Many objects in Python support iteration, such as over objects in a list or lines in a file. This is accomplished by means of the iterator protocol, a generic way to make objects iterable.\nCreating a Generator:\n\nGenerators are defined using functions with the yield keyword.\nA function with yield becomes a generator function.\n\n\n# create a simple generator function\ndef count_up_to(n):\n  i = 1\n  while i &lt;= n:\n    yield i  # yield value and pause execution\n    i += 1\n\nprint(count_up_to(7))\n\nfor num in count_up_to(7):  # iterate generator with for\n  print(num)\n\n&lt;generator object count_up_to at 0x000001BD9F67FAC0&gt;\n1\n2\n3\n4\n5\n6\n7\n\n\nGenerator Functions vs. Regular Functions:\n\nGenerator functions use yield to produce values and pause execution.\nRegular functions use return to provide a single result and terminate.\n\nIterating Over a Generator:\n\nYou can iterate over a generator using a for loop, just like any other iterable.\nThe generator produces values one at a time as you iterate.\n\nInfinite Sequences:\n\nGenerators can be used to create infinite sequences without consuming infinite memory.\nFor example, a generator can produce an infinite stream of numbers.\n\n\n# create a generator of infinite sequences\n\ndef infinite_evens():\n  num = 0\n  while True:\n    yield num\n    num +=2\n\n# Using next() with a generator    \ninfinite_evens_generator = infinite_evens()\nprint(next(infinite_evens_generator))  # the first element yielded by generator\nprint(next(infinite_evens_generator))\nprint(next(infinite_evens_generator))\nprint(\"-\"*55)\n\n# Using for with condition with a generator\nfor num in infinite_evens_generator:\n  if num &gt; 10:\n    break\n  print(\"Gen yielded: \", num)\nprint(\"-\"*55)\n  \n# Using next() with a stopping condition\ntry:\n  seq_list = []\n  while True:\n    num = next(infinite_evens_generator)\n    if num &gt; 100:\n      break\n    seq_list.append(num)\nexcept StropIteration:\n  pass\n\nprint(\"Sequence from Gen: \", seq_list)\nprint(\"-\"*55)\n\n0\n2\n4\n-------------------------------------------------------\nGen yielded:  6\nGen yielded:  8\nGen yielded:  10\n-------------------------------------------------------\nSequence from Gen:  [14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100]\n-------------------------------------------------------\n\n\n\n# Both produces the same results\n\nsome_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\nfor key in some_dict:\n  print(key)\n\ndict_iterator = iter(some_dict)  # get iterator for the given iterable object\nprint(dict_iterator)\nfor item in dict_iterator:\n  print(item)\n\na\nb\nc\n&lt;dict_keyiterator object at 0x000001BD9F6796C0&gt;\na\nb\nc\n\n\n\n# working with an iterator\n \nsome_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\ndict_iterator = iter(some_dict)\nprint(dict_iterator)\nprint(type(dict_iterator))\nprint(\"List created from an iterator: \", list(dict_iterator))\nprint(\"-----------------------------------------------------\")\n\n# create a generator producing squares from 1 to (n**2)\ndef squares(n=10):\n  print(f\"Generator producing squares from 1 to {n**2}\")\n  for i in range(1, n+1):\n    yield i**2\n  \ngen = squares()\nprint(gen)  # when called, generator itself does not execute any code\n\nfor x in gen: # actual code execution through iteration, One at a time, Not all at once\n  print(x, end=\" \")\n\n&lt;dict_keyiterator object at 0x000001BD9F67A9D0&gt;\n&lt;class 'dict_keyiterator'&gt;\nList created from an iterator:  ['a', 'b', 'c']\n-----------------------------------------------------\n&lt;generator object squares at 0x000001BDA2C6FAE0&gt;\nGenerator producing squares from 1 to 100\n1 4 9 16 25 36 49 64 81 100 \n\n\nGenerator express: Parentheses instead of brackets in comprehensions - list, set, dictionary, instead of list comprehension style.\n\n# generator expression, one-liner, less verbose than generator function definition\ngen = (x**2 for x in range(100))\nprint(gen)\n\n# generator expression as function argument\nprint(sum(x**2 for x in range(100))) # argument for sum() function\nprint( dict((i, i ** 2) for i in range(5)) )\n\n&lt;generator object &lt;genexpr&gt; at 0x000001BD9F67F2A0&gt;\n328350\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n\nitertools module (standard library), collection of generators for common data algorithms.\nitertools.groupby(iterable, key=None)\nThis is a function from the itertools module in Python. It’s used for grouping elements from an iterable into consecutive keys that share a common characteristic, as determined by a key function. The elements in the iterable are expected to be sorted based on the same key function for groupby() to work effectively.\niterable: This is the iterable (e.g., a list, tuple, or iterator) containing the elements you want to group.\nkey: This is an optional argument that specifies a function to extract a key from each element in the iterable. The elements in the iterable are grouped by the values returned by this key function.\nThe function returns an iterator that produces pairs of a key and an iterator over the elements in the group.\n\nimport itertools\n\nwords = [\"apple\", \"banana\", \"bat\", \"cat\", \"dog\", \"elephant\"]\n\n# Sort the words by their first letter\nsorted_words = sorted(words, key=lambda x: x[0])\nprint(sorted_words)\n\n# Use groupby to group words by their first letter (consecutive words with same first letter)\ngroups = itertools.groupby(sorted_words, key=lambda x: x[0])\nprint(groups)\n\n# iterate through the groups and print each group, very useful dealing with sorted data\nfor key, group in groups:\n  print(f\"Words starting with '{key}': {list(group)}\") #list() required to convert generator\n\n['apple', 'banana', 'bat', 'cat', 'dog', 'elephant']\n&lt;itertools.groupby object at 0x000001BDA2CE5DA0&gt;\nWords starting with 'a': ['apple']\nWords starting with 'b': ['banana', 'bat']\nWords starting with 'c': ['cat']\nWords starting with 'd': ['dog']\nWords starting with 'e': ['elephant']\n\n\nitertools.chain(*iterables):\nGenerates a sequence by chaining iterators together. Once elements from the first iterator are exhausted, elements form the next iterator are returned, and so on.\n\n# chain multiple iterators for continuous iteration\nfrom itertools import chain\n\n# Three lists\nlist1 = [1, 2, 3]\nlist2 = ['a', 'b', 'c']\nlist3 = [10, 20, 30]\n\n# chain them into a single iterable\ncombined = chain(list1, list2, list3)\nprint(combined, type(combined))\n\n# continuous iteration\nfor item in combined:\n  print(item, end=\" \")\n\n&lt;itertools.chain object at 0x000001BD9F676F20&gt; &lt;class 'itertools.chain'&gt;\n1 2 3 a b c 10 20 30 \n\n\nitertools.combinations(iterable, k):\nIt generates all possible combinations of ‘k’ number of elements from an iterable. Each combination is a tuple, and the function returns an iterator. It is very useful when exploring various subsets or combinations of elements from a collection. It generate only unique combination regardless of order of the elements.\n\nfrom itertools import combinations\n\n# generate all possible combinations of 2 elements from a given list\n\nelements = [1, 2, 3, 4]\ncombos = combinations(elements, 2)\n\n# iterate the generator\nfor combo in combos:\n  print(combo)\n  # or do something else, more...\n\n(1, 2)\n(1, 3)\n(1, 4)\n(2, 3)\n(2, 4)\n(3, 4)\n\n\nitertools.permutations(iterable, k):\nIt generates all possible combinations of ‘k’ number of elements from an iterable. The order of elements in a permuation matters, so different orders are considered distinct. It cna produce more output than ‘combinations()’ when ‘k’ is less than the lenght of the iterable since it considers all possible arrangements. The result is an iterator of tuples, each representing a unique permutation of the elements.\n\nfrom itertools import permutations\n\nelements = [1, 2, 3, 4]\nperms = permutations(elements, 2)\nfor perm in perms:\n  print(perm)\n\n(1, 2)\n(1, 3)\n(1, 4)\n(2, 1)\n(2, 3)\n(2, 4)\n(3, 1)\n(3, 2)\n(3, 4)\n(4, 1)\n(4, 2)\n(4, 3)\n\n\nitertools.product(*iterables, repeat=1):\nThe function is used to generate the Cartesian product of multiple input iterables . The Cartesian product is a set of all possible combinations of elements from the input iterables, where each combination consists of one element from each input iterable.\n\n# Cartesian product of iterables\nfrom itertools import product\n\niterable1 = [1, 2]\niterable2 = ['a','b']\n\n# Generate the Cartesian product of the two iterables\nresult = product(iterable1, iterable2)\n\n# Iterate through the product and print the combinations\nfor combo in result:\n  print(combo)\nprint(\"------------------------\")\n\n# set number of times to rpeat the each input iterable\nresult = product(iterable1, iterable2, repeat=2) # repeating 2 times\nfor combo in result:\n  print(combo)\n\n(1, 'a')\n(1, 'b')\n(2, 'a')\n(2, 'b')\n------------------------\n(1, 'a', 1, 'a')\n(1, 'a', 1, 'b')\n(1, 'a', 2, 'a')\n(1, 'a', 2, 'b')\n(1, 'b', 1, 'a')\n(1, 'b', 1, 'b')\n(1, 'b', 2, 'a')\n(1, 'b', 2, 'b')\n(2, 'a', 1, 'a')\n(2, 'a', 1, 'b')\n(2, 'a', 2, 'a')\n(2, 'a', 2, 'b')\n(2, 'b', 1, 'a')\n(2, 'b', 1, 'b')\n(2, 'b', 2, 'a')\n(2, 'b', 2, 'b')\n\n\n\n\n\nThis is an important part of building robust programs. In data applications, many functions work only on certain kinds of input.\n\n# function throwing error\nprint(float(\"1.2345\"))  # convert proper string input to float value\n# print(float(\"1,2345\"))  # throwing error with improper input\n\n1.2345\n\n\n\n# Failing gracefully\ndef attempt_float(x):\n  try:\n    return float(x)\n  except:\n    return x  # return input value gracefully(?) when 'try' failed, exception raised\n\nprint(attempt_float(\"1,2345\"))\n\n1,2345\n\n\n\n# suppressing a specific error\ndef attempt_float(x):\n  try:\n    return float(x)\n  except ValueError:\n    return x\n\n# attempt_float((1, 2))  # this caused an error other than ValueError\n\n\n# suppressing multiple errors\ndef attempt_float(x):\n  try:\n    return float(x)\n  except (TypeError, ValueError):  # now added TypeError to supress it too.\n    return x\n\nattempt_float((1, 2)) # failed, and gracefully return the input value as defined in try-execept, errors suppressed.\n\n(1, 2)\n\n\n\n# 'finally' lets you execute some codes regardless the success of try-block.\n\nf = open(\"./try-finally.txt\", mode=\"w\")\ntry:\n  write_to_file(f)\nexcept:\n  print(\"Failed\")\nelse:\n  print(\"Succeeded\")\nfinally:\n  f.close()\n\nFailed\n\n\n\n\n\n\nWorking with files and the operating system is a fundamental aspect of programming in Python. Python provides powerful tools and libraries to manipulate files, read and write data, and interact with the underlying file system\n\n\n\n# writing to a text file\nwith open('file_test.txt', 'w') as file:\n  file.write(\"Hello, world!\")\n  file.write(\"Brown fox jumped over the fence.\")\n  file.write(\"Handling file with text in Python is very straight forward.\")\n\n\n# opening and reading a text file\nwith open('file_test.txt', 'r') as file:\n  content = file.read()\nprint(content)\n\nHello, world!Brown fox jumped over the fence.Handling file with text in Python is very straight forward.\n\n\n\n# appending to a text file\nwith open('file_test.txt', 'a') as file:\n  file.write('\\nAppended line 1')\n  file.write('\\nAppended line 2')\n  file.write('\\nAppended line 3')\n  file.write('\\nAppended line 4')\n  \nwith open('file_test.txt', 'r') as file:\n  content = file.read()\nprint(content)\n\nHello, world!Brown fox jumped over the fence.Handling file with text in Python is very straight forward.\nAppended line 1\nAppended line 2\nAppended line 3\nAppended line 4\n\n\n\n\n\n\n# Listing files in a directory\n\nimport os\n\nfiles = os.listdir('../') # files in upper directory\nprint(files)\n\n['.git', '.gitignore', '.nojekyll', '.quarto', '.RData', '.Rhistory', '.Rproj.user', 'about.qmd', 'albert-joonyoung-park.github.io.Rproj', 'blog.qmd', 'data', 'docs', 'images', 'index.qmd', 'posts', 'profile.jpg', 'projects', 'projects.qmd', 'reports', 'styles.css', '_freeze', '_quarto.yml']\n\n\n\nimport os\n\npath = \"examples/segismundo.txt\"\n\n# open file with given encoding, \"r\" mode by default\nf = open(path, encoding=\"utf-8\") # default Unicode encoding is platform-specific\n\n# iterate the file object and print line with EOL marker intact\nfor line in f:\n  print(line)\n\n# close the file\nf.close()\n\nSueña el rico en su riqueza,\n\nque más cuidados le ofrece;\n\n\n\nsueña el pobre que padece\n\nsu miseria y su pobreza;\n\n\n\nsueña el que a medrar empieza,\n\nsueña el que afana y pretende,\n\nsueña el que agravia y ofende,\n\n\n\ny en el mundo, en conclusión,\n\ntodos sueñan lo que son,\n\naunque ninguno lo entiende.\n\n\n\n\n\n\nimport os\n\npath = \"examples/segismundo.txt\"\n\n# EOL-free list of lines in a file\nwith open(path, encoding=\"utf-8\") as f:\n  lines = [x.rstrip() for x in f]\nprint(lines)\n\n['Sueña el rico en su riqueza,', 'que más cuidados le ofrece;', '', 'sueña el pobre que padece', 'su miseria y su pobreza;', '', 'sueña el que a medrar empieza,', 'sueña el que afana y pretende,', 'sueña el que agravia y ofende,', '', 'y en el mundo, en conclusión,', 'todos sueñan lo que son,', 'aunque ninguno lo entiende.', '']\n\n\n\n# read, seek and tell\n\n# read moves the file object position by the number of bytes read after reading character or bytes\nf1 = open(path)\nprint(f1.read(10)) # read 10 characters with the encoding used for this file operation\n\nf2 = open(path, mode=\"rb\")  # open with binary mode\nprint(f2.read(10)) # read 10 bytes\n\n# tell current file object position\nprint(f1.tell())\nprint(f2.tell())\n\nimport sys\nprint(sys.getdefaultencoding())  # this is the default encoding of the platform in use.\n\n# seek changes the file position to the indicated byte in the file\n\nprint(f1.seek(3))\nprint(f1.read(1))\nprint(f1.tell())\n\nprint(f2.seek(3))\nprint(f2.read(1))\nprint(f2.tell())\n\n# close the files\nf1.close()\nf2.close()\n\nSueÃ±a el \nb'Sue\\xc3\\xb1a el '\n10\n10\nutf-8\n3\nÃ\n4\n3\nb'\\xc3'\n4\n\n\n\nimport os\n\nprint(os.getcwd())\n\npath = \"examples/segismundo.txt\"\nprint(path)\n\n# create a new file from path, remove blank lines\n\nwith open(\"examples/tmp.txt\", mode=\"w\") as handle:\n  handle.writelines(x for x in open(path) if len(x) &gt; 1)\n\nC:\\Users\\Joon\\git\\albert-joonyoung-park.github.io\\posts\nexamples/segismundo.txt\n\n\n\n# Open the new file and read\nwith open(\"examples/tmp.txt\") as f:\n  lines = f.readlines()\n\nlines\n\n['SueÃ±a el rico en su riqueza,\\n',\n 'que mÃ¡s cuidados le ofrece;\\n',\n 'sueÃ±a el pobre que padece\\n',\n 'su miseria y su pobreza;\\n',\n 'sueÃ±a el que a medrar empieza,\\n',\n 'sueÃ±a el que afana y pretende,\\n',\n 'sueÃ±a el que agravia y ofende,\\n',\n 'y en el mundo, en conclusiÃ³n,\\n',\n 'todos sueÃ±an lo que son,\\n',\n 'aunque ninguno lo entiende.\\n']\n\n\n\n\n\n\npath = \"examples/segismundo.txt\"\n\n# default file behavior - text mode\nwith open(path) as f:\n  chars = f.read(10) # read eough bytes to decode 10 characters, if utf-8 in use by the platform\n\nprint(chars)\nprint(len(chars))\n\n# binary mode\nwith open(path, mode=\"rb\") as f:  # append b to the mode\n  data = f.read(10) # read exact 10 bytes\nprint(data)\n\n# try and decode the bytes to a string object\nprint(data.decode(\"utf-8\")) # works only if each of the encoded Unicode characters is fully formed\n# print(data[:4].decode(\"utf-8\")) # error - not fully formed\nprint(data[:5].decode(\"utf-8\")) # worked, fully formed by the given range\n\nSueÃ±a el \n10\nb'Sue\\xc3\\xb1a el '\nSueña el \nSueñ\n\n\n\n# Use encoding option with open() to covert Unicode encoding to another\n\n# Create a new file converting encoding of segismundo.txt to iso-8859-1\n\npath = \"examples/segismundo.txt\"\nsink_path = \"examples/sink.txt\"\nwith open(path) as source:\n  with open(sink_path, \"w\", encoding=\"iso-8859-1\") as target:\n    target.write(source.read())\n\nwith open(sink_path, encoding=\"iso-8859-1\") as f:\n  print(f.read(10))\n\nSueÃ±a el"
  },
  {
    "objectID": "posts/p4da3d-data-types.html#built-in-data-structures-functions-and-files",
    "href": "posts/p4da3d-data-types.html#built-in-data-structures-functions-and-files",
    "title": "3. Built-In Data Structures, Functions, and Files",
    "section": "",
    "text": "Python Tuples: Immutable, ordered collections that allow you to store a sequence of elements. Tuples are defined using parentheses and can hold a mix of data types. They offer efficient data storage, are iterable, and can be used as keys in dictionaries. In this blog, we’ll explore the versatility of Python tuples and how to leverage them for various data analysis tasks.\n\n# create some tuples\ntup = (4, 5, 6)\ntup2 = 7, 8, 9\ntup\ntup2\ntup, tup2     # tuple of tuples\ntype(tup2)\n\n# convert any sequence or iterator to tuple\ntuple(['a', 'b', 'c'])\ntuple('Some String')\n\n('S', 'o', 'm', 'e', ' ', 'S', 't', 'r', 'i', 'n', 'g')"
  },
  {
    "objectID": "posts/p4da3d-data-types.html#chapter-3-built-in-data-structures-functions-and-files",
    "href": "posts/p4da3d-data-types.html#chapter-3-built-in-data-structures-functions-and-files",
    "title": "[3] Built-In Data Structures, Functions, and Files",
    "section": "",
    "text": "Python Tuples: Immutable, ordered collections that allow you to store a sequence of elements. Tuples are defined using parentheses and can hold a mix of data types. They offer efficient data storage, are iterable, and can be used as keys in dictionaries. In this blog, we’ll explore the versatility of Python tuples and how to leverage them for various data analysis tasks.\n\n# create some tuples\ntup = (4, 5, 6)\ntup2 = 7, 8, 9\nprint(tup)\nprint(tup2)\nt = (tup, tup2)    # tuple of tuples\nprint(t)\ntype(t)\n\n# convert any sequence or iterator to tuple\nprint(tuple(['a', 'b', 'c']))\nprint(tuple('Some String'))\n\n# access elements\nnested_tup = ((4,5,6), (7,8))\nprint(nested_tup)\n\n(4, 5, 6)\n(7, 8, 9)\n((4, 5, 6), (7, 8, 9))\n('a', 'b', 'c')\n('S', 'o', 'm', 'e', ' ', 'S', 't', 'r', 'i', 'n', 'g')\n((4, 5, 6), (7, 8))"
  },
  {
    "objectID": "posts/p4da3d-data-types.html#data-structures-and-sequences",
    "href": "posts/p4da3d-data-types.html#data-structures-and-sequences",
    "title": "[3] Built-In Data Structures, Functions, and Files",
    "section": "Data Structures and Sequences",
    "text": "Data Structures and Sequences\n\nTuple\nPython Tuples: Immutable, ordered collections that allow you to store a sequence of elements. Tuples are defined using parentheses and can hold a mix of data types. They offer efficient data storage, are iterable, and can be used as keys in dictionaries. In this blog, we’ll explore the versatility of Python tuples and how to leverage them for various data analysis tasks.\n\n# create some tuples\ntup = (4, 5, 6)\ntup2 = 7, 8, 9\nprint(tup)\nprint(tup2)\nt = (tup, tup2)    # tuple of tuples\nprint(t)\ntype(t)\n\n# convert any sequence or iterator to tuple\nprint(tuple(['a', 'b', 'c']))\nprint(tuple('Some String'))\n\n# access elements\nnested_tup = ((4,5,6), (7,8))\nprint(nested_tup[1])\n\n(4, 5, 6)\n(7, 8, 9)\n((4, 5, 6), (7, 8, 9))\n('a', 'b', 'c')\n('S', 'o', 'm', 'e', ' ', 'S', 't', 'r', 'i', 'n', 'g')\n(7, 8)\n\n\nTuple itself is immutable; however, if an object inside a tuple is mutable, you can modify it in place.\n\ntup = tuple(['The bown ','fox ', 'jumped over the ',['fence.']])\nprint(tup)\n# tup[1] = 'wolf'           # error\ntup[3][0] = 'window '     # ok\nprint(tup)\ntup[3].append('fence')    # ok\nprint(tup)\n\n('The bown ', 'fox ', 'jumped over the ', ['fence.'])\n('The bown ', 'fox ', 'jumped over the ', ['window '])\n('The bown ', 'fox ', 'jumped over the ', ['window ', 'fence'])\n\n\nConcatenation, Variable swap, Unpacking\n\n# concatenation using +\ntup = (4, None, 'foo') + (6, 0) + ('bar',)\nprint(tup)\n\n# Swap\na, b = 1, 2\nprint(a, b)\nb, a = a, b\nprint(a, b)\n\n# Unpacking by iterating over sequences of tuples, lists\nseq = [('a1', 'a2', 'a3'),('b4', 'b5', 'b6'), ('c7', 'c8', 'c9')]\nprint(seq, type(seq))\nfor a, b, c in seq:\n  print(f'a={a}, b={b}, c={c}')\n  \n# unpacking using assignment, not concerning some trailing values\nvalues = 1, 2, 3, 4, 5, 6, 7\na, b, *_ = values     # *rest works the same as *_\nprint (a, b, *_)\n\n# method .count()\na = (1, 2, 2, 2, 3, 4, 2)\nprint(a.count(2))     # number of occurences\n\n(4, None, 'foo', 6, 0, 'bar')\n1 2\n2 1\n[('a1', 'a2', 'a3'), ('b4', 'b5', 'b6'), ('c7', 'c8', 'c9')] &lt;class 'list'&gt;\na=a1, b=a2, c=a3\na=b4, b=b5, c=b6\na=c7, b=c8, c=c9\n1 2 3 4 5 6 7\n4\n\n\n\n\nList\nPython Lists: Ordered, mutable collections that allow you to store and manipulate sequences of items. Lists are versatile and can hold various data types. They support methods for adding, removing, and modifying elements, making them a fundamental data structure for data analysis and manipulation in Python.\n\n# create, convert lists\na_list = [2, 3, 7, None]\ntup = (\"foo\", \"bar\", \"baz\")\nb_list = list(tup)\nprint(b_list, type(b_list))\n\n# Access, modify list element\nb_list[1] = \"peekaboo\"\nprint(b_list)\n\n# Materialize an iterator / generator\ngen = range(20) # generator, not materialized yet.\nprint(gen) \nlist(gen) # materialize the generator to list\n\n# Work with the elements\nb_list.append(\"dwarf\")     # add to the end of list\nb_list.append(\"foo\")\nprint(b_list)\nb_list.insert(1, \"RED\")     # insert at specific location, consider using collections.deque for efficiency\nprint(b_list)\nb_list.remove(\"foo\")        # remove the first occurence only\nprint(b_list)\n\n# Check if element in the list\nprint(\"dwarf\" in b_list)\nprint(\"dwarf\" not in b_list)\n\n['foo', 'bar', 'baz'] &lt;class 'list'&gt;\n['foo', 'peekaboo', 'baz']\nrange(0, 20)\n['foo', 'peekaboo', 'baz', 'dwarf', 'foo']\n['foo', 'RED', 'peekaboo', 'baz', 'dwarf', 'foo']\n['RED', 'peekaboo', 'baz', 'dwarf', 'foo']\nTrue\nFalse\n\n\n\n# Concatenate and combine lists\nnew_list = [4, None, \"foo\"] + [7, 8, (2, 3)]\nprint(new_list)\nprint(b_list)\nnew_list.extend(b_list)\nprint(new_list)\n\n# Sorting in place\na = [\"saw\", \"small\", \"He\", \"foxes\", \"six\"]\na.sort() # default\nprint(a)\na.sort(key=len) # sort by key with given function, by the length of each element\nprint(a)\n\n# Sorted copy\na = [\"saw\", \"small\", \"He\", \"foxes\", \"six\"]\nsorted_copy = sorted(a)\nprint(sorted_copy)\n\n[4, None, 'foo', 7, 8, (2, 3)]\n['RED', 'peekaboo', 'baz', 'dwarf', 'foo']\n[4, None, 'foo', 7, 8, (2, 3), 'RED', 'peekaboo', 'baz', 'dwarf', 'foo']\n['He', 'foxes', 'saw', 'six', 'small']\n['He', 'saw', 'six', 'foxes', 'small']\n['He', 'foxes', 'saw', 'six', 'small']\n\n\n\n# Slices\n\nseq = [7, 2, 3, 7, 5, 6, 0, 1]\nprint(seq)\nprint(seq[1:5])   # index:1 - index: 4\nprint(seq[:5])    # index:beginning - index: 4\nprint(seq[5:])    # index:5 - index:last\nprint(seq[-4:])   # Indexing from the last, index:-4 to the end\nprint(seq[-6:-2]) # Indexing from the last, index: -6 to index: -2\n\nprint(\"-\"*30)\n\n# Stepping the list\nprint(seq[::2])     # Extract index 0, 2, 4, 6 ....\nprint(seq[::3])     # Extract index 0, 3, 6 ....\nprint(seq[::-2])    # Extract index -1, -3, -5 .....\nprint(seq[::-1])    # Extract index -1, -2, -3, -4 ... effectively reverse the list or tuple\n\n\nprint(\"-\"*30)\n\n# Slices replacement\nseq[3:5] = ['A','B']\nprint(seq)\n\n[7, 2, 3, 7, 5, 6, 0, 1]\n[2, 3, 7, 5]\n[7, 2, 3, 7, 5]\n[6, 0, 1]\n[5, 6, 0, 1]\n[3, 7, 5, 6]\n------------------------------\n[7, 3, 5, 0]\n[7, 7, 0]\n[1, 6, 7, 2]\n[1, 0, 6, 5, 7, 3, 2, 7]\n------------------------------\n[7, 2, 3, 'A', 'B', 6, 0, 1]\n\n\n\n\nDictionary\nPython Dictionaries: Versatile and dynamic data structures that store key-value pairs, allowing for efficient data retrieval and manipulation. Dictionaries are unordered collections that provide a way to map unique keys to associated values, making them invaluable for tasks like data storage, lookup tables, and configuration settings\n\n# Create a Dictionary (aka, hash-map, associative arrays)\n\nempty_dict = {} \nprint(empty_dict)\nd1 = {\"a\": \"some value\", \"b\": [1, 2, 3, 4]}  # key-value pair\nprint(d1)\n\n# Access, insert and set\nd1[7] = \"an integer\"      # 7:\"an integer\" pair\nprint(d1)\nprint(d1[\"b\"])            # lookup with key\n#print(d1[\"address\"])     # KeyError\nprint(\"address\" in d1)    # key exists?\n\nprint(\"-\"*30)\n\n# Delete values\ndel d1[7]              # delete with key\n#del d1[\"country\"]     # KeyError\nprint(d1)\nret = d1.pop(\"b\")      # delete with key and return the value\nprint(ret)\nprint(d1)\n\nprint(\"-\"*30)\n\n# Iteration using keys() values() items()\nd1 = {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\nprint(type(d1.keys()))     # &lt;class 'dict_keys'&gt;\nprint(type(d1.values()))   # &lt;class 'dict_values'&gt;\n\nd1_keys = list(d1.keys())   \nd1_values = list(d1.values())\nd1_items = list(d1.items())  # Gives a list of tuples in (key, value)\nprint(d1_keys)\nprint(d1_values)\nprint(d1_items)  \n\n# iteration\nfor key, value in d1.items():\n  print(f'Key: \\\"{key}\\\" has value of \\\"{value}\\\".')\n\n{}\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\n[1, 2, 3, 4]\nFalse\n------------------------------\n{'a': 'some value', 'b': [1, 2, 3, 4]}\n[1, 2, 3, 4]\n{'a': 'some value'}\n------------------------------\n&lt;class 'dict_keys'&gt;\n&lt;class 'dict_values'&gt;\n['a', 'b', 7]\n['some value', [1, 2, 3, 4], 'an integer']\n[('a', 'some value'), ('b', [1, 2, 3, 4]), (7, 'an integer')]\nKey: \"a\" has value of \"some value\".\nKey: \"b\" has value of \"[1, 2, 3, 4]\".\nKey: \"7\" has value of \"an integer\".\n\n\n\n# Update dictionary with a dictionary of new or changes of key-value pairs\nd1 = {'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\nprint(d1)\nd1.update({\"b\": \"foo\", \"c\": 12})\nprint(d1)\n\n{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}\n{'a': 'some value', 'b': 'foo', 7: 'an integer', 'c': 12}\n\n\n\n# Create a dictionary from two sequences\n\nlist_country = [\"USA\", \"Canada\", \"England\", \"France\"]\nlist_city = [\"Washigton\", \"Ottawa\", \"London\", \"Paris\"]\nprint(list_country)\nprint(list_city)\n\n# Using for loop - zip and iterate\nprint(zip(list_country, list_city))\n\nmapping = {}\nfor key, value in zip(list_country, list_city):\n  mapping[key] = value\nprint(mapping, type(mapping))\n\n# Using dict() method\ncountry_city_tuples = zip(list_country, list_city)\nprint(country_city_tuples, type(country_city_tuples))\nmapping = dict(country_city_tuples)\nprint(mapping, type(mapping))\n\n['USA', 'Canada', 'England', 'France']\n['Washigton', 'Ottawa', 'London', 'Paris']\n&lt;zip object at 0x000002993D9B1640&gt;\n{'USA': 'Washigton', 'Canada': 'Ottawa', 'England': 'London', 'France': 'Paris'} &lt;class 'dict'&gt;\n&lt;zip object at 0x000002993D9B2A40&gt; &lt;class 'zip'&gt;\n{'USA': 'Washigton', 'Canada': 'Ottawa', 'England': 'London', 'France': 'Paris'} &lt;class 'dict'&gt;\n\n\n\n# Build a dictionary WUTHOUT using setdefault()\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\nprint(words, type(words))\n\nby_letter = {}\n\nfor word in words:\n  letter = word[0] # Using the first letter of each word as key\n  if letter not in by_letter:\n    by_letter[letter] = [word]\n  else:\n    by_letter[letter].append(word)\n\nprint(by_letter)\n\nprint(\"-\"*55)\n\n# Simplified version using setdefault()\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\nby_letter = {}\nfor word in words:\n  letter = word[0] # Using the first letter of each word as key\n  by_letter.setdefault(letter, []).append(word) # If the letter (the first letter of the current word) is already a key in the dictionary, it returns the corresponding value (which is a list) for that key. If the key doesn't exist, it initializes an empty list. The append() method is then used to add the current word to the list associated with the letter key.\nprint(by_letter)\n\nprint(\"-\"*55)\n\n# Using defaultdict from collections module\nfrom collections import defaultdict\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\nby_letter = defaultdict(list) # if you access a key that doesn't exist, it will automatically create that key with an empty list as the associated value. This is particularly useful when you want to avoid KeyError exceptions when working with dictionaries.\nprint(by_letter, type(by_letter))\n\nfor word in words:\n  by_letter[word[0]].append(word)\n\nprint(by_letter)\n\n['apple', 'bat', 'bar', 'atom', 'book'] &lt;class 'list'&gt;\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n-------------------------------------------------------\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n-------------------------------------------------------\ndefaultdict(&lt;class 'list'&gt;, {}) &lt;class 'collections.defaultdict'&gt;\ndefaultdict(&lt;class 'list'&gt;, {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']})\n\n\nUnderstanding Dictionary Key Types\nIn Python, dictionaries are versatile data structures used for mapping keys to corresponding values. While dictionaries allow flexibility in storing values of any Python data type as their values, there are specific requirements and considerations when it comes to selecting suitable key types. This aspect of dictionary design is crucial to ensure efficient and reliable key-value pair lookups and operations.\nKey Characteristics of Dictionary Keys\n\nImmutability Requirement:\nThe primary requirement for dictionary keys is that they should generally be immutable objects. Immutability refers to the property of an object that prevents its state from being modified after it is created.\nExamples of Immutable Key Types:\n\nScalar Types: Examples of immutable scalar types suitable as dictionary keys include integers (int), floating-point numbers (float), and strings (str). These types are inherently immutable, which means once you create a key with a specific value, that key-value pair remains consistent.\nTuples as Composite Keys: Tuples can also serve as dictionary keys, but there’s an important condition: all the objects within the tuple must also be immutable. This ensures the stability of the key, as the tuple itself remains unchanged.\n\n\nThe Significance of Hashability\nThe technical term that ties these requirements together is “hashability.” Hashability refers to an object’s ability to be hashed, which involves converting the object into a fixed-size numerical value, known as a hash code. This hash code is used as an index to efficiently store and retrieve key-value pairs in a dictionary. The key feature of hashable objects is that they produce the same hash code consistently.\nThe Role of the hash() Function\nYou can determine whether an object is hashable and can be used as a dictionary key by utilizing the built-in hash() function. When you apply the hash() function to an object, it returns a hash code if the object is hashable. However, keep in mind that not all Python objects are hashable, particularly mutable objects like lists and dictionaries.\n\n# Using hash() to validate key types in dictionary\n\nhash(\"string\")           # hashable\nhash((1, 2, (2, 3)))     # hashable\n# hash((1, 2, [2, 3]))     # Not hashable due to mutable element - list of [2, 3]\n\n# When in need of list for key, convert it tp tuple, hashable\nd = {}\nd[tuple([1,2,3])] = 5\nprint(d)  # dictionary with a tuple as key\n\n{(1, 2, 3): 5}\n\n\n\n\nSet\nSets in Python: A Set is an essential data structure that represents an unordered collection of unique elements. Unlike lists or tuples, which allow duplicates, sets only store distinct values. This characteristic makes sets ideal for various tasks, such as removing duplicates from a list, checking for membership, and performing set operations like union, intersection, and difference. One of the most common real-world use cases for sets is in data deduplication, where they excel at efficiently eliminating duplicate records from datasets, ensuring data integrity and enhancing the performance of data processing tasks.\n\n# Create a set\nprint(set([2, 2, 2, 1, 3, 3]))\nprint\n\n# Union, Difference, Intersection and Symmetric Difference\na = {1, 2, 3, 4, 5}\nb = {3, 4, 5, 6, 7, 8}\nprint (\"Set a:\", a, \" Set b:\", b)\n\n# a.union(b) = a | b\nprint(\"a union b: \", a.union(b), a | b)\n# a.intersection(b) = a & b\nprint(\"a intersection b: \", a.intersection(b), a & b)\n# a.difference(b) = a - b\nprint(\"a difference b: \", a.difference(b), a - b)\n# a.symmetric_difference(b)\nprint(\"a.symmetric_difference(b): \", a.symmetric_difference(b), a ^ b)\n\n{1, 2, 3}\nSet a: {1, 2, 3, 4, 5}  Set b: {3, 4, 5, 6, 7, 8}\na union b:  {1, 2, 3, 4, 5, 6, 7, 8} {1, 2, 3, 4, 5, 6, 7, 8}\na intersection b:  {3, 4, 5} {3, 4, 5}\na difference b:  {1, 2} {1, 2}\na.symmetric_difference(b):  {1, 2, 6, 7, 8} {1, 2, 6, 7, 8}\n\n\n\n# Common set operations\na = {1, 2, 3, 4, 5}\n\n# a.add(x) - Add x to the set\na.add(\"Fox\")\nprint(a)\n# a.remove(x) - Remove x from the set\na.remove(\"Fox\")\nprint(a)\n# a.pop() - Remove an element arbitrarily, set must not be empty\nret = a.pop()\nprint(a, ret)\n\n\n\n# a.clear() - reset to empty\nprint(a.clear())  # None - empty\n\n{1, 2, 3, 4, 5, 'Fox'}\n{1, 2, 3, 4, 5}\n{2, 3, 4, 5} 1\nNone\n\n\nBuilt-in Sequence Functions\n\na_list = [7, 1, 2, 6, 0, 3, 2]\n\n# Very useful, use at every opportunities\n\n# enumerate() - no need to write code to track index\nfor index, value in enumerate(a_list):\n  print(index, \" : \", value)  \n\n# sorted() - a new srted list from the elements of any sequence\nprint(sorted(a_list))\nprint(sorted(\"horse race\"))  # string is a sequence\n\n# zip() - pairs up sequences element-wise to create a list\nseq1 = [\"foo\", \"bar\", \"baz\"]\nseq2 = [\"one\", \"two\", \"three\"]\nseq3 = [\"BROWN\", \"FOX\", \"FENCE\", \"JUMP\"]\nzipped = zip(seq1, seq2, seq3)\nprint(list(zipped))  # \"JUMP\" ignored for pariing!\n\n# zip() commonly works with enumerate() for iteration - zip and iterate\nfor index, (a, b) in enumerate(zip(seq1, seq2)):\n  print(f\"{index}: {a}, {b}\")\n\n# reversed()\nreversed(range(10)) # range_iterator object, it's a generator\nprint(list(reversed(range(10)))) # list() materializes the generator\n\n0  :  7\n1  :  1\n2  :  2\n3  :  6\n4  :  0\n5  :  3\n6  :  2\n[0, 1, 2, 2, 3, 6, 7]\n[' ', 'a', 'c', 'e', 'e', 'h', 'o', 'r', 'r', 's']\n[('foo', 'one', 'BROWN'), ('bar', 'two', 'FOX'), ('baz', 'three', 'FENCE')]\n0: foo, one\n1: bar, two\n2: baz, three\n[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\n\n\nList, Set and Dictionary Comprehensions\nComprehensions in Python offer concise, readable, and often more efficient ways to create lists, sets, and dictionaries. They reduce code verbosity, enhance readability, and promote a functional programming style, ultimately leading to cleaner and more maintainable code.\nList Comprehension:\nList comprehensions provide a concise way to create lists by applying an expression to each item in an iterable (e.g., a list, tuple, or range) and optionally filtering the items based on a condition.\n\n## [exp for value in collection if condition]\n\n# create a list of strings in uppercase only if the string has more than two characters.\nstrings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\nstrings_2 = [ str.upper() for str in strings if len(str) &gt; 2 ]\nprint(strings_2)\n\n# create a list of squares of even numbers from 0 to 9\neven_squares = [ x ** 2 for x in range(10) if x % 2 == 0 ]\nprint(even_squares)\n\n['BAT', 'CAR', 'DOVE', 'PYTHON']\n[0, 4, 16, 36, 64]\n\n\nSet Comprehension:\nSet comprehensions allow you to create sets in a similar manner to list comprehensions, but with the guarantee of uniqueness among elements.\n\n## set_comp = {expr for value in collection if condition}\n\n# Create a set using strings list above, containing only the lengths of strings.\nstrings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\nunique_lengths = { len(str) for str in strings } # uniqueness guaranted\nprint(unique_lengths)\n\n# map() - even more powerful\nstrings_map = set(map(len, strings))\nprint(strings_map)\n\n# Create a set of unique even squares from 0 to 9\neven_squares_set = {x ** 2 for x in range(10) if x % 2 == 0}\nprint(even_squares_set)\n\n{1, 2, 3, 4, 6}\n{1, 2, 3, 4, 6}\n{0, 64, 4, 36, 16}\n\n\nDictionary Comprehension:\nDictionary comprehensions enable the creation of dictionaries by defining key-value pairs based on expressions applied to items in an iterable.\n\n## {key_expression: value_expression for item in iterable if condition}\n\n# Create a dictionary, mapping even numbers to their squares from 0 to 9\neven_squares_dict = { x: x**2 for x in range(10) if x % 2 == 0 }\nprint(even_squares_dict)\n\n# Create a dictionary of lookup map of string for the location of each string in the list\nstrings = [\"a\", \"as\", \"bat\", \"car\", \"dove\", \"python\"]\nstring_lookup_dict = { str: index for index, str in enumerate(strings) }\nprint(strings)\nprint(string_lookup_dict)\n\n{0: 0, 2: 4, 4: 16, 6: 36, 8: 64}\n['a', 'as', 'bat', 'car', 'dove', 'python']\n{'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}\n\n\nNested list comprehensions\nThe order of\n\n# List of lists with some English and Spanish names - List nesting lists\nall_data = [\n  [\"John\", \"Emily\", \"Michael\", \"Mary\", \"Steven\"],\n  [\"Maria\", \"Juan\", \"Javier\", \"Natalia\", \"Pilar\"]\n]\nprint(all_data)\nprint(\"-\"*55)\n\n# Create a list of names contating two of more of 'a'\n# The non-Pythonic apporach using for loop..\nnames_of_interest = []\nfor names in all_data:\n  print(\"Processing:\", names)\n  enough_as = [ name for name in names if name.count(\"a\") &gt;=2 ]\n  print(enough_as)\n  names_of_interest.extend(enough_as)\nprint(\"Results: \", names_of_interest)\n\n# Re-writtine in one-liner using nested list comprehension\nnames_of_interest = [ name for names in all_data for name in names if name.count(\"a\") &gt;=2 ]\nprint(\"Result using Nested List Comprehension: \", names_of_interest)\n\n# Flatten a list of tuples of integers into a simple list of integers using nested list comprehension\nsome_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\nprint(some_tuples, type(some_tuples)) # list of tuples with integers\nflattened = [ int for tuple in some_tuples for int in tuple ]\nprint(\"Flattened: \", flattened, type(flattened))  # flattend list\n\n# Readability and valid forms\nflattened = []\nprint(\"Init. flattened list: \", flattened)\nprint(\"some_tuples: \", some_tuples)\nfor tup in some_tuples:\n  for x in tup:\n    flattened.append(x)\nprint(\"flattend from some_tuples: \", flattened)\n\n# list comprehenson inside list comprehension\n# Change list of tuples to a list of lists\nprint(\"Init. list of some_tuples: \", some_tuples)\nnew_list_of_lists = [[ x for x in tup ] for tup in some_tuples ]\nprint(\"List of lists from some_tuples: \", new_list_of_lists)\n\n[['John', 'Emily', 'Michael', 'Mary', 'Steven'], ['Maria', 'Juan', 'Javier', 'Natalia', 'Pilar']]\n-------------------------------------------------------\nProcessing: ['John', 'Emily', 'Michael', 'Mary', 'Steven']\n[]\nProcessing: ['Maria', 'Juan', 'Javier', 'Natalia', 'Pilar']\n['Maria', 'Natalia']\nResults:  ['Maria', 'Natalia']\nResult using Nested List Comprehension:  ['Maria', 'Natalia']\n[(1, 2, 3), (4, 5, 6), (7, 8, 9)] &lt;class 'list'&gt;\nFlattened:  [1, 2, 3, 4, 5, 6, 7, 8, 9] &lt;class 'list'&gt;\nInit. flattened list:  []\nsome_tuples:  [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\nflattend from some_tuples:  [1, 2, 3, 4, 5, 6, 7, 8, 9]\nInit. list of some_tuples:  [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\nList of lists from some_tuples:  [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\n# 6 Practical use caes of nested list comprehension\n\n# 1. Matrix operations:\n# Add 1 to each element in a 2D matrix\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nmatrix_add_1 = [[ element + 1 for element in row] for row in matrix]\nprint(\"matrix_add_1: \", matrix_add_1)\n\n# 2. Data transformation:\n# Filter rows with even some from a list of lists\ndata = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nfiltered_data = [ row for row in data if sum(row) % 2 == 0 ]\nprint(\"filtered_data: \", filtered_data)\n\n# 3. Flattening lists of lists:\nnested_lists = [[1, 2], [3, 4], [5, 6]]\nflattened_list = [ item for sublist in nested_lists for item in sublist ]\nprint(\"flattened_list: \", flattened_list)\n\n# 4. Combining data:\n# Create all possible pairs (in tuple) from two lists - Different from element-wise pairing using zip()\nlist1 = [1, 2, 3]\nlist2 = ['a', 'b', 'c']\ncombinations = [ (x, y) for x in list1 for y in list2 ]\nprint(\"combinations: \", combinations)\n\n# 5. Conditional filtering:\n# Filter rows containing 'John' from a list of lists\ndata = [['Alice', 25], ['John', 30], ['Bob', 35]]\nfiltered_data = [ list for list in data if 'John' in list ]\nprint(\"filtered_data: \", filtered_data)\n\n# 6. Transposing data:\n# Transpose a given matrix (switching rows and columns)\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\ntransposed_matrix = [[ (row[i], print(\"row[i]: \", row[i]), print(\"processing row: \", row)) for row in matrix ] for i in range(len(matrix[0]))]\n#print(\"transposed_matrix: \", transposed_matrix)\n#print(len(matrix[0]))\n#print(range(len(matrix[0])))\n\nmatrix_add_1:  [[2, 3, 4], [5, 6, 7], [8, 9, 10]]\nfiltered_data:  [[1, 2, 3], [7, 8, 9]]\nflattened_list:  [1, 2, 3, 4, 5, 6]\ncombinations:  [(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]\nfiltered_data:  [['John', 30]]\nrow[i]:  1\nprocessing row:  [1, 2, 3, 4]\nrow[i]:  5\nprocessing row:  [5, 6, 7, 8]\nrow[i]:  9\nprocessing row:  [9, 10, 11, 12]\nrow[i]:  2\nprocessing row:  [1, 2, 3, 4]\nrow[i]:  6\nprocessing row:  [5, 6, 7, 8]\nrow[i]:  10\nprocessing row:  [9, 10, 11, 12]\nrow[i]:  3\nprocessing row:  [1, 2, 3, 4]\nrow[i]:  7\nprocessing row:  [5, 6, 7, 8]\nrow[i]:  11\nprocessing row:  [9, 10, 11, 12]\nrow[i]:  4\nprocessing row:  [1, 2, 3, 4]\nrow[i]:  8\nprocessing row:  [5, 6, 7, 8]\nrow[i]:  12\nprocessing row:  [9, 10, 11, 12]"
  },
  {
    "objectID": "posts/p4da3d-data-types.html#functions",
    "href": "posts/p4da3d-data-types.html#functions",
    "title": "[3] Built-In Data Structures, Functions, and Files",
    "section": "Functions",
    "text": "Functions\nFunctions in Python are a cornerstone of the language, offering powerful tools for structuring and modularizing code. Python functions come with several unique features that distinguish them from functions in other programming languages.\n\nVersatile and Expressive: Python functions support both positional and keyword arguments, enabling flexible and expressive function calls.\nMultiple Return Values: Python functions can return multiple values, simplifying the packaging and unpacking of results.\nDefault Argument Values: Python functions allow for default argument values, making function usage more straightforward by providing sensible defaults when arguments are omitted.\nLambda Functions: Python features lambda functions for concise and anonymous function definitions, particularly useful for simple operations.\nFirst-Class Functions: Python treats functions as first-class citizens, allowing them to be assigned to variables, passed as arguments, and returned from other functions. This flexibility enhances code modularity, maintainability, and readability.\nModular and Reusable: Python’s function-centric approach encourages the creation of clean, reusable, and modular code, making it a crucial topic for all Python programmers to master.\n\n\n# simple declaration and calling\ndef my_function(x,y):\n  return x + y\n\nret = my_function(1, 2)\nprint(ret)\n\n# Return None when no retorn statement is encountered\ndef funciton_without_return(x):\n  print(x)\nresult = funciton_without_return(\"hello!\")\nprint(result)\n\n# positional and keyword arguments\ndef my_function2(x, y, z=1.5):  # z as keyword argument with default value\n  if z &gt; 1:\n    return z * (x + y)\n  else:\n    return z / (x + y)\n\n# Naespace, scope and local functions\n\n\nprint(my_function2(5, 6, z=0.7))  # no need to put arg names, keword arg optional\nprint(my_function2(3.14, 7, 3.5))  # no need to specify name for kewoord arguments\nprint(my_function2(10, 20))   # keyword arg optional\n\n\na = []\ndef func():\n  for i in range(5):\n    a.append(i)\n\nfunc()\n\n3\nhello!\nNone\n0.06363636363636363\n35.49\n45.0\n\n\n\n# Assigining variable outside the function's scope\n\na = None\ndef bind_a_variable():\n  global a\n  a = []\n  \nbind_a_variable()\nprint(a)\n\n# multiple returns\ndef f():\n  a = 5\n  b = 6\n  c = 7\n  return a, b, c\n\na, b, c = f()\nprint(a, b)\n\n[]\n5 6\n\n\n\nFunctions Are Objects\nIn Python, functions are first-class objects, which means they are treated like any other object, such as integers, strings, or lists. This concept is a fundamental part of the Python language and has several implications and practical uses.\n\nFunctions can be assigned to variables:\n\n\n# Functions can be assigned to variables:\n\ndef greet(name):\n    return f\"Hello, {name}!\"\n\ngreeting = greet  # function assigned to variable \nprint(greeting(\"Alice\"))  # calling the function using the assigned variable\n\nprint(id(greet), id(greeting)) # both variable and function references the same object\n\nHello, Alice!\n2857186802272 2857186802272\n\n\n\nFunctions can be passed as arguments: This is particularly useful for functions that require a callback or custom behavior.\n\n\n# Functions can be passed as arguments\n\ndef apply(func, value):\n  return func(value)\n\ndef double(x):\n  return x * 2\n\nresult = apply(double, 5)\nprint(result)\n\n10\n\n\n\nFunctions can be returned from other functions: This enables dynamic function generation.\n\n\n# Functinos can be returned from other functions\ndef create_multiplier(factor):\n  \"\"\"\n  Returns a FUNCTION that produce the result of given factor * x\n  \"\"\"\n  def multiplier(x):\n    return x * factor\n  return multiplier    # return the inner function\n\ndouble = create_multiplier(2)\nprint(double(200))  # Result is a function returned by 'created_multiplier'\nprint(double(400))\n\n400\n800\n\n\n\n\nFunctions can be stored in data structures\nFunctions can be stored in data structures like lists, dictionaries, or sets, making it possible to manage and manipulate functions dynamically.\n\n# Functinos can be stored in data structures\n\ndef square(x):\n  return x**2\n\ndef cube(x):\n  return x**3\n\nmath_functions = [square, cube]\nresult = math_functions[1](3)  # access and call the 'cube' function, give 3 to the parameter\nprint(result)\n\n\n# Using a list of functinos for cleaning strings from survey data\nanswer_states = [\"   alabama \", \"georgia!\", \"Georgia\",  \\\n          \"georgia\", \"flOrIda\", \"south   carolina##\",   \\\n          \"West virginia?\"]\nprint(answer_states)\n          \ndef remove_punctuation(value):\n  return re.sub(\"[!#?]\", \"\", value)\n\nclean_ops = [str.strip, remove_punctuation, str.title] # 3 functinos in list for cleaning operation\n\nimport re\n\ndef clean_strings(strings, ops):\n    result = []\n    for value in strings:\n        for func in ops:\n            value = func(value)\n        result.append(value)\n    return result\n  \nprint(\"strings_cleaned: \", clean_strings(answer_states, clean_ops))\n\nprint(\"-\"*55)\n\n\n# Using map() to apply a function as argument to a sequence\nfor x in map(remove_punctuation, answer_states):\n  print(x)\n\n27\n['   alabama ', 'georgia!', 'Georgia', 'georgia', 'flOrIda', 'south   carolina##', 'West virginia?']\nstrings_cleaned:  ['Alabama', 'Georgia', 'Georgia', 'Georgia', 'Florida', 'South   Carolina', 'West Virginia']\n-------------------------------------------------------\n   alabama \ngeorgia\nGeorgia\ngeorgia\nflOrIda\nsouth   carolina\nWest virginia\n\n\n\n\nAnonymous (Lambda) Functions:\nAnonymous functions, often referred to as “lambda functions,” are a concise way to create a single line, inline functions without giving them a formal name. In Python, lambda functions are defined using the lambda keyword, followed by parameters and an expression. While they are limited in scope compared to regular functions, they offer several benefits:lambda arguments: expression\nStructure: lambda arguments: expression\nConciseness: Lambda functions are compact and allow you to define simple operations in a single line of code, making them particularly useful for short, one-off functions.\nReadability: When used appropriately, lambda functions can improve code readability by encapsulating a specific operation right where it’s needed, reducing the need for named functions or temporary variables.\nFunctional Programming: Lambda functions are a key component of functional programming in Python. They can be used with higher-order functions like map(), filter(), and reduce() to perform operations on collections of data.\n\n# sort a list of tuples by the second element:\ndata = [(1, 5), (3, 2), (2, 8)]\nsorted_data = sorted(data, key=lambda x: x[1])  # extracts the second element (index 1) of each tuple in the data list\nprint(sorted_data)\n\n[(3, 2), (1, 5), (2, 8)]\n\n\n\n# filter even numbers from a list\nnumbers = [1, 2, 3, 4, 5, 6]\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))  # filtering function, iterables\nprint(even_numbers)\n\n[2, 4, 6]\n\n\n\n# Mapping a Function to a List\n\nvalues = [1, 2, 3, 4, 5]\nsquared_values = list(map(lambda x: x**2, values))\nprint(squared_values)\n\n[1, 4, 9, 16, 25]\n\n\n\ndef short_function(x):\n  return x * 2\nprint(short_function(100))\n\nequiv_amon = lambda x: x * 2  # assign a function to variable\nprint(type(equiv_amon))\nprint(equiv_amon(100))  # produces the same result\nprint(\"-\"*55)\n\n# use a lambda function for the second argument\ndef apply_to_list(some_list, f):\n  \"\"\"\n  Apply the given function to the given list\n  \"\"\"\n  return [f(x) for x in some_list]\nints = [4, 0, 1, 5, 6]\n\nret = apply_to_list(ints, lambda x: x*2)\nprint(ret)\n\n# sort the list of strings based on the number of unique charaters for each string.\nstrings = [\"foo\", \"card\", \"bar\", \"aaaa\", \"abab\"]\nstrings.sort(key=lambda x: len(set(x))) # sort by number of unique characters per string element\nprint(strings)\n\n200\n&lt;class 'function'&gt;\n200\n-------------------------------------------------------\n[8, 0, 2, 10, 12]\n['aaaa', 'foo', 'abab', 'bar', 'card']\n\n\n\n\nGenerators\nGenerators are a powerful and memory-efficient feature in Python for working with sequences of data. They allow you to create iterators on the fly, enabling the processing of large data sets or infinite sequences without storing them in memory. Generators are defined using functions but use the yield keyword to produce values one at a time. Many objects in Python support iteration, such as over objects in a list or lines in a file. This is accomplished by means of the iterator protocol, a generic way to make objects iterable.\nCreating a Generator:\n\nGenerators are defined using functions with the yield keyword.\nA function with yield becomes a generator function.\n\n\n# create a simple generator function\ndef count_up_to(n):\n  i = 1\n  while i &lt;= n:\n    yield i  # yield value and pause execution\n    i += 1\n\nprint(count_up_to(7))\n\nfor num in count_up_to(7):  # iterate generator with for\n  print(num)\n\n&lt;generator object count_up_to at 0x000002993A35FAC0&gt;\n1\n2\n3\n4\n5\n6\n7\n\n\nGenerator Functions vs. Regular Functions:\n\nGenerator functions use yield to produce values and pause execution.\nRegular functions use return to provide a single result and terminate.\n\nIterating Over a Generator:\n\nYou can iterate over a generator using a for loop, just like any other iterable.\nThe generator produces values one at a time as you iterate.\n\nInfinite Sequences:\n\nGenerators can be used to create infinite sequences without consuming infinite memory.\nFor example, a generator can produce an infinite stream of numbers.\n\n\n# create a generator of infinite sequences\n\ndef infinite_evens():\n  num = 0\n  while True:\n    yield num\n    num +=2\n\n# Using next() with a generator    \ninfinite_evens_generator = infinite_evens()\nprint(next(infinite_evens_generator))  # the first element yielded by generator\nprint(next(infinite_evens_generator))\nprint(next(infinite_evens_generator))\nprint(\"-\"*55)\n\n# Using for with condition with a generator\nfor num in infinite_evens_generator:\n  if num &gt; 10:\n    break\n  print(\"Gen yielded: \", num)\nprint(\"-\"*55)\n  \n# Using next() with a stopping condition\ntry:\n  seq_list = []\n  while True:\n    num = next(infinite_evens_generator)\n    if num &gt; 100:\n      break\n    seq_list.append(num)\nexcept StropIteration:\n  pass\n\nprint(\"Sequence from Gen: \", seq_list)\nprint(\"-\"*55)\n\n0\n2\n4\n-------------------------------------------------------\nGen yielded:  6\nGen yielded:  8\nGen yielded:  10\n-------------------------------------------------------\nSequence from Gen:  [14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100]\n-------------------------------------------------------\n\n\n\n# Both produces the same results\n\nsome_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\nfor key in some_dict:\n  print(key)\n\ndict_iterator = iter(some_dict)  # get iterator for the given iterable object\nprint(dict_iterator)\nfor item in dict_iterator:\n  print(item)\n\na\nb\nc\n&lt;dict_keyiterator object at 0x000002993D9BA930&gt;\na\nb\nc\n\n\n\n# working with an iterator\n \nsome_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\ndict_iterator = iter(some_dict)\nprint(dict_iterator)\nprint(type(dict_iterator))\nprint(\"List created from an iterator: \", list(dict_iterator))\nprint(\"-----------------------------------------------------\")\n\n# create a generator producing squares from 1 to (n**2)\ndef squares(n=10):\n  print(f\"Generator producing squares from 1 to {n**2}\")\n  for i in range(1, n+1):\n    yield i**2\n  \ngen = squares()\nprint(gen)  # when called, generator itself does not execute any code\n\nfor x in gen: # actual code execution through iteration, One at a time, Not all at once\n  print(x, end=\" \")\n\n&lt;dict_keyiterator object at 0x000002993D9D6110&gt;\n&lt;class 'dict_keyiterator'&gt;\nList created from an iterator:  ['a', 'b', 'c']\n-----------------------------------------------------\n&lt;generator object squares at 0x000002993D93FAE0&gt;\nGenerator producing squares from 1 to 100\n1 4 9 16 25 36 49 64 81 100 \n\n\nGenerator express: Parentheses instead of brackets in comprehensions - list, set, dictionary, instead of list comprehension style.\n\n# generator expression, one-liner, less verbose than generator function definition\ngen = (x**2 for x in range(100))\nprint(gen)\n\n# generator expression as function argument\nprint(sum(x**2 for x in range(100))) # argument for sum() function\nprint( dict((i, i ** 2) for i in range(5)) )\n\n&lt;generator object &lt;genexpr&gt; at 0x000002993A35F2A0&gt;\n328350\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n\nitertools module (standard library), collection of generators for common data algorithms.\nitertools.groupby(iterable, key=None)\nThis is a function from the itertools module in Python. It’s used for grouping elements from an iterable into consecutive keys that share a common characteristic, as determined by a key function. The elements in the iterable are expected to be sorted based on the same key function for groupby() to work effectively.\niterable: This is the iterable (e.g., a list, tuple, or iterator) containing the elements you want to group.\nkey: This is an optional argument that specifies a function to extract a key from each element in the iterable. The elements in the iterable are grouped by the values returned by this key function.\nThe function returns an iterator that produces pairs of a key and an iterator over the elements in the group.\n\nimport itertools\n\nwords = [\"apple\", \"banana\", \"bat\", \"cat\", \"dog\", \"elephant\"]\n\n# Sort the words by their first letter\nsorted_words = sorted(words, key=lambda x: x[0])\nprint(sorted_words)\n\n# Use groupby to group words by their first letter (consecutive words with same first letter)\ngroups = itertools.groupby(sorted_words, key=lambda x: x[0])\nprint(groups)\n\n# iterate through the groups and print each group, very useful dealing with sorted data\nfor key, group in groups:\n  print(f\"Words starting with '{key}': {list(group)}\") #list() required to convert generator\n\n['apple', 'banana', 'bat', 'cat', 'dog', 'elephant']\n&lt;itertools.groupby object at 0x000002993D9BB790&gt;\nWords starting with 'a': ['apple']\nWords starting with 'b': ['banana', 'bat']\nWords starting with 'c': ['cat']\nWords starting with 'd': ['dog']\nWords starting with 'e': ['elephant']\n\n\nitertools.chain(*iterables):\nGenerates a sequence by chaining iterators together. Once elements from the first iterator are exhausted, elements form the next iterator are returned, and so on.\n\n# chain multiple iterators for continuous iteration\nfrom itertools import chain\n\n# Three lists\nlist1 = [1, 2, 3]\nlist2 = ['a', 'b', 'c']\nlist3 = [10, 20, 30]\n\n# chain them into a single iterable\ncombined = chain(list1, list2, list3)\nprint(combined, type(combined))\n\n# continuous iteration\nfor item in combined:\n  print(item, end=\" \")\n\n&lt;itertools.chain object at 0x000002993A355780&gt; &lt;class 'itertools.chain'&gt;\n1 2 3 a b c 10 20 30 \n\n\nitertools.combinations(iterable, k):\nIt generates all possible combinations of ‘k’ number of elements from an iterable. Each combination is a tuple, and the function returns an iterator. It is very useful when exploring various subsets or combinations of elements from a collection. It generate only unique combination regardless of order of the elements.\n\nfrom itertools import combinations\n\n# generate all possible combinations of 2 elements from a given list\n\nelements = [1, 2, 3, 4]\ncombos = combinations(elements, 2)\n\n# iterate the generator\nfor combo in combos:\n  print(combo)\n  # or do something else, more...\n\n(1, 2)\n(1, 3)\n(1, 4)\n(2, 3)\n(2, 4)\n(3, 4)\n\n\nitertools.permutations(iterable, k):\nIt generates all possible combinations of ‘k’ number of elements from an iterable. The order of elements in a permuation matters, so different orders are considered distinct. It cna produce more output than ‘combinations()’ when ‘k’ is less than the lenght of the iterable since it considers all possible arrangements. The result is an iterator of tuples, each representing a unique permutation of the elements.\n\nfrom itertools import permutations\n\nelements = [1, 2, 3, 4]\nperms = permutations(elements, 2)\nfor perm in perms:\n  print(perm)\n\n(1, 2)\n(1, 3)\n(1, 4)\n(2, 1)\n(2, 3)\n(2, 4)\n(3, 1)\n(3, 2)\n(3, 4)\n(4, 1)\n(4, 2)\n(4, 3)\n\n\nitertools.product(*iterables, repeat=1):\nThe function is used to generate the Cartesian product of multiple input iterables . The Cartesian product is a set of all possible combinations of elements from the input iterables, where each combination consists of one element from each input iterable.\n\n# Cartesian product of iterables\nfrom itertools import product\n\niterable1 = [1, 2]\niterable2 = ['a','b']\n\n# Generate the Cartesian product of the two iterables\nresult = product(iterable1, iterable2)\n\n# Iterate through the product and print the combinations\nfor combo in result:\n  print(combo)\nprint(\"------------------------\")\n\n# set number of times to rpeat the each input iterable\nresult = product(iterable1, iterable2, repeat=2) # repeating 2 times\nfor combo in result:\n  print(combo)\n\n(1, 'a')\n(1, 'b')\n(2, 'a')\n(2, 'b')\n------------------------\n(1, 'a', 1, 'a')\n(1, 'a', 1, 'b')\n(1, 'a', 2, 'a')\n(1, 'a', 2, 'b')\n(1, 'b', 1, 'a')\n(1, 'b', 1, 'b')\n(1, 'b', 2, 'a')\n(1, 'b', 2, 'b')\n(2, 'a', 1, 'a')\n(2, 'a', 1, 'b')\n(2, 'a', 2, 'a')\n(2, 'a', 2, 'b')\n(2, 'b', 1, 'a')\n(2, 'b', 1, 'b')\n(2, 'b', 2, 'a')\n(2, 'b', 2, 'b')\n\n\n\n\nErrors and Exception Handling\nThis is an important part of building robust programs. In data applications, many functions work only on certain kinds of input.\n\n# function throwing error\nprint(float(\"1.2345\"))  # convert proper string input to float value\n# print(float(\"1,2345\"))  # throwing error with improper input\n\n1.2345\n\n\n\n# Failing gracefully\ndef attempt_float(x):\n  try:\n    return float(x)\n  except:\n    return x  # return input value gracefully(?) when 'try' failed, exception raised\n\nprint(attempt_float(\"1,2345\"))\n\n1,2345\n\n\n\n# suppressing a specific error\ndef attempt_float(x):\n  try:\n    return float(x)\n  except ValueError:\n    return x\n\n# attempt_float((1, 2))  # this caused an error other than ValueError\n\n\n# suppressing multiple errors\ndef attempt_float(x):\n  try:\n    return float(x)\n  except (TypeError, ValueError):  # now added TypeError to supress it too.\n    return x\n\nattempt_float((1, 2)) # failed, and gracefully return the input value as defined in try-execept, errors suppressed.\n\n(1, 2)\n\n\n\n# 'finally' lets you execute some codes regardless the success of try-block.\n\nf = open(\"./try-finally.txt\", mode=\"w\")\ntry:\n  write_to_file(f)\nexcept:\n  print(\"Failed\")\nelse:\n  print(\"Succeeded\")\nfinally:\n  f.close()\n\nFailed"
  },
  {
    "objectID": "posts/p4da3d-data-types.html#files-and-the-operating-system",
    "href": "posts/p4da3d-data-types.html#files-and-the-operating-system",
    "title": "[3] Built-In Data Structures, Functions, and Files",
    "section": "Files and the Operating System",
    "text": "Files and the Operating System\nWorking with files and the operating system is a fundamental aspect of programming in Python. Python provides powerful tools and libraries to manipulate files, read and write data, and interact with the underlying file system\n\nFile handling basics:\n\n# writing to a text file\nwith open('file_test.txt', 'w') as file:\n  file.write(\"Hello, world!\")\n  file.write(\"Brown fox jumped over the fence.\")\n  file.write(\"Handling file with text in Python is very straight forward.\")\n\n\n# opening and reading a text file\nwith open('file_test.txt', 'r') as file:\n  content = file.read()\nprint(content)\n\nHello, world!Brown fox jumped over the fence.Handling file with text in Python is very straight forward.\n\n\n\n# appending to a text file\nwith open('file_test.txt', 'a') as file:\n  file.write('\\nAppended line 1')\n  file.write('\\nAppended line 2')\n  file.write('\\nAppended line 3')\n  file.write('\\nAppended line 4')\n  \nwith open('file_test.txt', 'r') as file:\n  content = file.read()\nprint(content)\n\nHello, world!Brown fox jumped over the fence.Handling file with text in Python is very straight forward.\nAppended line 1\nAppended line 2\nAppended line 3\nAppended line 4\n\n\n\n\nFile Navigation and Operations:\n\n# Listing files in a directory\n\nimport os\n\nfiles = os.listdir('../') # files in upper directory\nprint(files)\n\n['.git', '.gitignore', '.nojekyll', '.quarto', '.RData', '.Rhistory', '.Rproj.user', 'about.qmd', 'albert-joonyoung-park.github.io.Rproj', 'blog.qmd', 'data', 'docs', 'images', 'index.qmd', 'posts', 'profile.jpg', 'projects', 'projects.qmd', 'reports', 'styles.css', '_freeze', '_quarto.yml']\n\n\n\nimport os\n\npath = \"examples/segismundo.txt\"\n\n# open file with given encoding, \"r\" mode by default\nf = open(path, encoding=\"utf-8\") # default Unicode encoding is platform-specific\n\n# iterate the file object and print line with EOL marker intact\nfor line in f:\n  print(line)\n\n# close the file\nf.close()\n\nSueña el rico en su riqueza,\n\nque más cuidados le ofrece;\n\n\n\nsueña el pobre que padece\n\nsu miseria y su pobreza;\n\n\n\nsueña el que a medrar empieza,\n\nsueña el que afana y pretende,\n\nsueña el que agravia y ofende,\n\n\n\ny en el mundo, en conclusión,\n\ntodos sueñan lo que son,\n\naunque ninguno lo entiende.\n\n\n\n\n\n\nimport os\n\npath = \"examples/segismundo.txt\"\n\n# EOL-free list of lines in a file\nwith open(path, encoding=\"utf-8\") as f:\n  lines = [x.rstrip() for x in f]\nprint(lines)\n\n['Sueña el rico en su riqueza,', 'que más cuidados le ofrece;', '', 'sueña el pobre que padece', 'su miseria y su pobreza;', '', 'sueña el que a medrar empieza,', 'sueña el que afana y pretende,', 'sueña el que agravia y ofende,', '', 'y en el mundo, en conclusión,', 'todos sueñan lo que son,', 'aunque ninguno lo entiende.', '']\n\n\n\n# read, seek and tell\n\n# read moves the file object position by the number of bytes read after reading character or bytes\nf1 = open(path)\nprint(f1.read(10)) # read 10 characters with the encoding used for this file operation\n\nf2 = open(path, mode=\"rb\")  # open with binary mode\nprint(f2.read(10)) # read 10 bytes\n\n# tell current file object position\nprint(f1.tell())\nprint(f2.tell())\n\nimport sys\nprint(sys.getdefaultencoding())  # this is the default encoding of the platform in use.\n\n# seek changes the file position to the indicated byte in the file\n\nprint(f1.seek(3))\nprint(f1.read(1))\nprint(f1.tell())\n\nprint(f2.seek(3))\nprint(f2.read(1))\nprint(f2.tell())\n\n# close the files\nf1.close()\nf2.close()\n\nSueÃ±a el \nb'Sue\\xc3\\xb1a el '\n10\n10\nutf-8\n3\nÃ\n4\n3\nb'\\xc3'\n4\n\n\n\nimport os\n\nprint(os.getcwd())\n\npath = \"examples/segismundo.txt\"\nprint(path)\n\n# create a new file from path, remove blank lines\n\nwith open(\"examples/tmp.txt\", mode=\"w\") as handle:\n  handle.writelines(x for x in open(path) if len(x) &gt; 1)\n\nC:\\Users\\Joon\\git\\albert-joonyoung-park.github.io\\posts\nexamples/segismundo.txt\n\n\n\n# Open the new file and read\nwith open(\"examples/tmp.txt\") as f:\n  lines = f.readlines()\n\nlines\n\n['SueÃ±a el rico en su riqueza,\\n',\n 'que mÃ¡s cuidados le ofrece;\\n',\n 'sueÃ±a el pobre que padece\\n',\n 'su miseria y su pobreza;\\n',\n 'sueÃ±a el que a medrar empieza,\\n',\n 'sueÃ±a el que afana y pretende,\\n',\n 'sueÃ±a el que agravia y ofende,\\n',\n 'y en el mundo, en conclusiÃ³n,\\n',\n 'todos sueÃ±an lo que son,\\n',\n 'aunque ninguno lo entiende.\\n']\n\n\n\n\nBytes and Unicode with Files\n\npath = \"examples/segismundo.txt\"\n\n# default file behavior - text mode\nwith open(path) as f:\n  chars = f.read(10) # read eough bytes to decode 10 characters, if utf-8 in use by the platform\n\nprint(chars)\nprint(len(chars))\n\n# binary mode\nwith open(path, mode=\"rb\") as f:  # append b to the mode\n  data = f.read(10) # read exact 10 bytes\nprint(data)\n\n# try and decode the bytes to a string object\nprint(data.decode(\"utf-8\")) # works only if each of the encoded Unicode characters is fully formed\n# print(data[:4].decode(\"utf-8\")) # error - not fully formed\nprint(data[:5].decode(\"utf-8\")) # worked, fully formed by the given range\n\nSueÃ±a el \n10\nb'Sue\\xc3\\xb1a el '\nSueña el \nSueñ\n\n\n\n# Use encoding option with open() to covert Unicode encoding to another\n\n# Create a new file converting encoding of segismundo.txt to iso-8859-1\n\npath = \"examples/segismundo.txt\"\nsink_path = \"examples/sink.txt\"\nwith open(path) as source:\n  with open(sink_path, \"w\", encoding=\"iso-8859-1\") as target:\n    target.write(source.read())\n\nwith open(sink_path, encoding=\"iso-8859-1\") as f:\n  print(f.read(10))\n\nSueÃ±a el"
  },
  {
    "objectID": "posts/p4da3d-numpy.html#pseudorandom-number-generation",
    "href": "posts/p4da3d-numpy.html#pseudorandom-number-generation",
    "title": "[4] Arrays and Vectorized Computation",
    "section": "Pseudorandom Number Generation",
    "text": "Pseudorandom Number Generation\nThe numpy.random module supplements the built-in Python random module with functions for efficiently generating whole arrays of sample value from many kinds of probability distributions.\n\n# Generates 4x4 random numbers from a standard normal distribution (mean = 0, standard deviation = 1)\n\nsamples = np.random.standard_normal(size=(4, 4))\nprint(samples)\n\n[[ 0.02438752  0.3888968  -0.25353428  0.50565954]\n [ 0.69382098 -0.36495474  0.4066921  -0.47840471]\n [-0.88659672  1.19854012 -0.34464064 -0.62669276]\n [-0.01201135 -0.49104672 -0.04240425  2.90625971]]\n\n\n\n# Set to use explicit generator using seed\nrng = np.random.default_rng(seed=12345)\ndata = rng.standard_normal((2, 3))\nprint(data)\ndata2 = rng.standard_normal((2, 3))\nprint(data2)\ndata3 = rng.standard_normal((2, 3))\nprint(data3)\nprint(type(rng))\n\n[[-1.42382504  1.26372846 -0.87066174]\n [-0.25917323 -0.07534331 -0.74088465]]\n[[-1.3677927   0.6488928   0.36105811]\n [-1.95286306  2.34740965  0.96849691]]\n[[-0.75938718  0.90219827 -0.46695317]\n [-0.06068952  0.78884434 -1.25666813]]\n&lt;class 'numpy.random._generator.Generator'&gt;\n\n\nnumpy.random._generator.Generator class\npermutation: Returns a random permutation of a sequences - list, array, shuffles the elements\n\nimport numpy as np\n\n# Some array\narr = np.array([1, 2, 3, 4, 5])\n# Generate a random permutation of the array\npermuted_arr = np.random.permutation(arr)\nprint(permuted_arr)\n\n[3 4 1 2 5]\n\n\nshuffles: Shuffles elements by modifying the original in place\n\n# some array\narr = np.array([1, 2, 3, 4, 5])\n# shuffle the array in place\nnp.random.shuffle(arr)\nprint(arr)\nnp.random.shuffle(arr)\nprint(arr)\nnp.random.shuffle(arr)\nprint(arr)\n\n[3 5 4 1 2]\n[5 3 2 4 1]\n[4 5 1 3 2]\n\n\nuniform: Equal Probability Occuring, Generate random numbers from a uniform distriubution within a specified range.\n\n# generate random numbers in uniform between 0 and 1\n\nimport numpy as np\n\nrandom_nums = np.random.uniform(0, 1, size=7)\nprint(random_nums)\n\n[0.2830613  0.79880075 0.58422251 0.01745402 0.66536846 0.21399967\n 0.19575847]\n\n\nrandint: Generate random integers within a specified range\n\n# Generate random integers within a specified range.\nimport numpy as np\nrandom_ints = np.random.randint(1, 11, size=7)\nprint(random_ints)\n\n[5 2 2 3 5 3 6]\n\n\nstandard_normal: Generate random numbers from a standard normal distribution (mean=0, standard deviation=1)\n\n# generate random numbers from a standard normal distribution\n\nimport numpy as np\n\nrandom_nums = np.random.standard_normal(size=7)\nprint(random_nums)\nprint(np.mean(random_nums))  # close to 0\nprint(np.std(random_nums))  # close to 1\n\n[ 0.41237087 -0.68778692  0.77539471  1.15493846 -1.07059758  1.2746338\n  0.10654032]\n0.2807848089726647\n0.8288167933552584\n\n\nbinomial: Generates random numbers from a binomial distribution (two possible outcomes, success or failure - Bernoulli trials)\n\nimport numpy as np\n\n# Generate random numbers from a binomial distribution\nrandom_nums = np.random.binomial(n=10, p=0.5, size=5)\nprint(random_nums)\n\n# interpretation of the result\n\"\"\"\nThe code generates 5 random numbers, each representing the number of successful outcomes in 10 independent trials where each trial has a 50% chance of success.\n\nThe possible values for each of these random numbers range from 0 to 10 because you can have 0 to 10 successful outcomes in 10 trials.\n\nThe specific numbers you obtained, [7, 5, 8, 3, 7], represent the results of 5 separate random experiments. For example, in the first experiment, you got 7 successful outcomes out of 10 trials, and in the second experiment, you got 5 successful outcomes, and so on.\n\nThe result will vary each time you run the code because it's based on random sampling, but the numbers are generated according to a binomial distribution with the specified parameters (10 trials with a 50% chance of success on each trial).\n\"\"\"\n\n[5 5 5 5 2]\n\n\n\"\\nThe code generates 5 random numbers, each representing the number of successful outcomes in 10 independent trials where each trial has a 50% chance of success.\\n\\nThe possible values for each of these random numbers range from 0 to 10 because you can have 0 to 10 successful outcomes in 10 trials.\\n\\nThe specific numbers you obtained, [7, 5, 8, 3, 7], represent the results of 5 separate random experiments. For example, in the first experiment, you got 7 successful outcomes out of 10 trials, and in the second experiment, you got 5 successful outcomes, and so on.\\n\\nThe result will vary each time you run the code because it's based on random sampling, but the numbers are generated according to a binomial distribution with the specified parameters (10 trials with a 50% chance of success on each trial).\\n\"\n\n\nnormal: Generates random numbers from a normal distribution with specified mean and standard deviation.\n\nimport numpy as np\n\n# Generate random numbers from a normal distribution\nrandom_nums = np.random.normal(loc=0, scale=1, size=100)  # loc: mean(center peak),scale: standard deviation, size: number of random numbers to generate\n\nprint(random_nums)\n\n[-0.00696454  0.0214189  -1.42455243  0.36136736 -0.61930816 -0.31244649\n  1.39291758  0.98820841 -0.51702993  0.59563007  0.24867064  1.34785811\n -0.69594826  0.09236997  0.24227799 -0.5922411   2.26783981  0.16404388\n -0.25167976  1.22072294  0.52205938 -1.69107276  0.10705041 -0.67858279\n  0.99601737  1.28758256 -0.82151763 -1.15386979  0.0211763  -0.86365435\n  0.28562624  0.27583518  0.36980757  0.23285048  0.01086876  0.60453358\n -0.7103104   0.82039425 -0.12596228 -0.53830311  0.03426621 -1.77197059\n  1.48829227  0.53860449 -1.15945694  0.62534058  0.69087205  1.27406982\n  1.58980275 -0.92830126  0.979448    0.56596988 -0.77310244 -2.01179695\n -0.22836684  0.0935332   0.87793864  0.52128948 -0.35862365  0.01452477\n  0.10494151 -0.50833713 -0.13354027  2.13108755  0.5016837   1.10169622\n  1.32947346 -0.67884005 -1.00460594 -0.33893047  0.87737814  0.32742269\n  0.12677347 -2.47772429 -0.22611494 -1.57731021  0.77216936  0.52837262\n  3.38936433 -0.34167377  0.32182606 -0.37863225  0.92298595 -2.043062\n -1.1717948  -0.09791369  0.38802381 -0.10557425 -0.78998972 -0.3727696\n -1.95957704  1.70569364  0.47251256  0.14704871  0.58025787  1.75071261\n -0.38211577  0.83111002  0.09054014  0.35010363]\n\n\n\n# draw s simple plot with the array\nimport matplotlib.pyplot as plt\n\nplt.clf() # clear any previous figure with plt.\n\n# histogram plot\nplt.hist(random_nums, bins=50, density=True, alpha=0.7, color='red', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram of Random Numbers from Standard Normal Distribution')\nplt.show()\n\n\n\n\n\n# KDE (Kernel Density Estimate)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.clf() # clear any previous figure with plt.\n\nsns.kdeplot(random_nums, color='green', fill=True)\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.title('KDE Plot of Random Numbers from Standard Normal Distribution')\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.clf() # clear any previous figure with plt.\n\n# Create a box plot\nplt.boxplot(random_nums)\n\n# Add labels and title\nplt.xlabel('Value')\nplt.ylabel('Distribution')\nplt.title('Box Plot of Random Numbers from Standard Normal Distribution')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n# ECDF Plot (Empirical Cumulative Distribution Function): how the data accumulates moving along the x-axis, representing the cumulative distribution\n\nplt.clf() # clear any previous figure with plt.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create an ECDF plot\nsorted_nums = np.sort(random_nums)\nprint(len(sorted_nums))\ny = np.arange(1, len(sorted_nums) + 1) / len(sorted_nums)\nplt.plot(sorted_nums, y, marker='.', linestyle='none')\n\n# Add labels and title\nplt.xlabel('Value')\nplt.ylabel('ECDF')\nplt.title('ECDF Plot of Random Numbers from Standard Normal Distribution')\n\n# Show the plot\nplt.grid(True)\nplt.show()\n\n100\n\n\n\n\n\nbeta: Generates random numbers from a beta distribution.\n\nimport numpy as np\n\n# Generate random numbers from a beta distribution\nrandom_nums = np.random.beta(a=2, b=5, size=5)\nprint(random_nums)\n\n[0.24908806 0.22241801 0.03200077 0.35978889 0.45460631]\n\n\nchisquare: Generates random numbers from a chi-squared distribution\n\nimport numpy as np\n\n# Generate random numbers from a chi-squared distribution\nrandom_nums = np.random.chisquare(df=3, size=5)\nprint(random_nums)\n\n[3.49405042 2.55796782 5.23348605 0.93066636 3.13663889]\n\n\ngamma: Generates random numbers from a gamma distribution\n\nimport numpy as np\n\n# Generate random numbers from a gamma distribution\nrandom_nums = np.random.gamma(shape=2, scale=2, size=5)\nprint(random_nums)\n\n[2.236189   3.12003875 0.38688417 1.59182572 1.4935085 ]"
  },
  {
    "objectID": "posts/p4da3d-numpy.html#universal-functions-fast-element-wise-array-functions",
    "href": "posts/p4da3d-numpy.html#universal-functions-fast-element-wise-array-functions",
    "title": "[4] Arrays and Vectorized Computation",
    "section": "Universal Functions: Fast Element-Wise Array Functions",
    "text": "Universal Functions: Fast Element-Wise Array Functions\nufunc, a function taht performs element-wise operatinos on data in ndarrays\n\n# unary ufunc\n\narr = np.arange(10)\nprint(arr, type(arr))\nprint(np.sqrt(arr))  # sqrt on each element\nprint(np.exp(arr))   # exp on each element\nprint(\"-\"*60)\n\n# binary ufunc\nx = rng.standard_normal(8)\ny = rng.standard_normal(8)\nprint(x)\nprint(y)\nprint(np.maximum(x, y))  # element-wise maximum from x, y\n\narr = rng.standard_normal(7) * 5\nprint(arr)\nfractional_part, integral_part = np.modf(arr)  # returning two arrays\nprint(fractional_part)\nprint(integral_part) \n\n[0 1 2 3 4 5 6 7 8 9] &lt;class 'numpy.ndarray'&gt;\n[0.         1.         1.41421356 1.73205081 2.         2.23606798\n 2.44948974 2.64575131 2.82842712 3.        ]\n[1.00000000e+00 2.71828183e+00 7.38905610e+00 2.00855369e+01\n 5.45981500e+01 1.48413159e+02 4.03428793e+02 1.09663316e+03\n 2.98095799e+03 8.10308393e+03]\n------------------------------------------------------------\n[ 0.57585751  1.39897899  1.32229806 -0.29969852  0.90291934 -1.62158273\n -0.15818926  0.44948393]\n[-1.34360107 -0.08168759  1.72473993  2.61815943  0.77736134  0.8286332\n -0.95898831 -1.20938829]\n[ 0.57585751  1.39897899  1.72473993  2.61815943  0.90291934  0.8286332\n -0.15818926  0.44948393]\n[-7.06146007  2.70773415  3.75969698 -3.2938016  -6.14337493  1.28778884\n  1.56451459]\n[-0.06146007  0.70773415  0.75969698 -0.2938016  -0.14337493  0.28778884\n  0.56451459]\n[-7.  2.  3. -3. -6.  1.  1.]\n\n\nUniversal functions(ufunc)"
  },
  {
    "objectID": "posts/p4da3d-numpy.html#array-oriented-programming-with-arrays",
    "href": "posts/p4da3d-numpy.html#array-oriented-programming-with-arrays",
    "title": "[4] Arrays and Vectorized Computation",
    "section": "Array-Oriented Programming with Arrays",
    "text": "Array-Oriented Programming with Arrays\nEvaluate a function using with an array, not for loop\nnumpy.meshgrid()\n\n# mumpy.meshgrid\n\npoints = np.arange(-5, 5, 0.01)\n#print(points)\nprint(points.size) # array size\nxs, ys = np.meshgrid(points, points) # create a comple grid for coordinates\n#print(xs)\n#print(ys)\n\n# evaluate the function\nz = np.sqrt(xs**2 + ys**2)\n#print(\"z: \", z)\n\n# create a plot - colorbar\nimport matplotlib.pyplot as plt\n\nplt.clf() # clear any previous figure\n\n# using imghow() function\nplt.imshow(z, cmap=plt.cm.gray, extent=[-5, 5, -5, 5])\nplt.colorbar()\nplt.title(\"Image plot of $\\sqrt{x^2 + y^2}$ for a grid of values\")\nplt.show()\n\n1000\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate the same grid as before\npoints = np.arange(-5, 5, 0.01)\nxs, ys = np.meshgrid(points, points)\nz = np.sqrt(xs**2 + ys**2)\n\n# Create a line chart by taking values along a horizontal line (e.g., y=0)\nline_values = z[len(points)//2, :]\n\n# Create an array of x-values for the line chart\nx_values = points\n\n# Create a line chart\nplt.clf()\nplt.plot(x_values, line_values, label='Line Chart')\nplt.xlabel('X')\nplt.ylabel('Function Value')\nplt.title('Line Chart of Function Values along y=0')\nplt.legend()\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\nExpressing conditional logicas Array Operations\nnumpy.where - vectorized version fo ternary expression x if condition else y\n\nxarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])\nyarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])\n# Boolean arrary to use as conditional value for xarr, yarr\ncond = np.array([True, False, True, True, False])\n\nprint(\"xarr: \", xarr)\nprint(\"yarr: \", yarr)\nprint(\"cond: \", cond)\n\n# Generate an array to take a value for xarr when corresponding boolean array is True, otherwise take the value form yarr.\nprint(\"Generate an array to take a value for xarr when corresponding boolean array is True, otherwise take the value form yarr.\")\n\n# Using conventional list conprehension.\nresult_list_comprehension = [ (x if c else y)for x, y, c in zip(xarr, yarr, cond) ]\nprint(\"Result using list comprehension: \", result_list_comprehension)\nprint(\"Issues: 1. Slow 2. Won't work with multi-dimensional arrays\")\n\n# Using numpy.where(cond, xarr, yarr) - no need to be arrays for the 2nd and 3rd argument\nprint(\"Using numpy.where(cond, xarr, yarr) - no need to be arrays for the 2nd and 3rd argument.\")\nresult_numpy_where = np.where(cond, xarr, yarr)\nprint(\"result_numpy_where: \", result_numpy_where)\n\nxarr:  [1.1 1.2 1.3 1.4 1.5]\nyarr:  [2.1 2.2 2.3 2.4 2.5]\ncond:  [ True False  True  True False]\nGenerate an array to take a value for xarr when corresponding boolean array is True, otherwise take the value form yarr.\nResult using list comprehension:  [1.1, 2.2, 1.3, 1.4, 2.5]\nIssues: 1. Slow 2. Won't work with multi-dimensional arrays\nUsing numpy.where(cond, xarr, yarr) - no need to be arrays for the 2nd and 3rd argument.\nresult_numpy_where:  [1.1 2.2 1.3 1.4 2.5]\n\n\nWrite code: Generate a matrix with random data, and replace all positive values with 2 and all negative values with -2\n\nimport numpy as np\n\n# create a random number generator\nrng = np.random.default_rng(seed=12345)\n\n# generate an array of random numbers in 4x4, using standard_normal here,\narr = rng.standard_normal((4, 4))\nprint(\"arr using standard_normal): \", arr)\n\n# generate a conditional boolean array by checking condition\n# generate an array according to the condition and operations\nprint(\"arr &gt; 0: \", arr &gt; 0)\nprint(\"np.where(arr &gt; 0, 2, -2): \", np.where(arr &gt; 0, 2, -2)) # using scalar values\nprint(\"original arr: \", arr)\n\n# generatate an array using boolean condition, scalars and arrays\n# replace all positive values in arr with 2, if not, don't do anything\nprint(\"mix of scalar and array: \", np.where(arr &gt; 0, 2, arr)) # set only positive values to 2, otherwise untouched\n\narr using standard_normal):  [[-1.42382504  1.26372846 -0.87066174 -0.25917323]\n [-0.07534331 -0.74088465 -1.3677927   0.6488928 ]\n [ 0.36105811 -1.95286306  2.34740965  0.96849691]\n [-0.75938718  0.90219827 -0.46695317 -0.06068952]]\narr &gt; 0:  [[False  True False False]\n [False False False  True]\n [ True False  True  True]\n [False  True False False]]\nnp.where(arr &gt; 0, 2, -2):  [[-2  2 -2 -2]\n [-2 -2 -2  2]\n [ 2 -2  2  2]\n [-2  2 -2 -2]]\noriginal arr:  [[-1.42382504  1.26372846 -0.87066174 -0.25917323]\n [-0.07534331 -0.74088465 -1.3677927   0.6488928 ]\n [ 0.36105811 -1.95286306  2.34740965  0.96849691]\n [-0.75938718  0.90219827 -0.46695317 -0.06068952]]\nmix of scalar and array:  [[-1.42382504  2.         -0.87066174 -0.25917323]\n [-0.07534331 -0.74088465 -1.3677927   2.        ]\n [ 2.         -1.95286306  2.          2.        ]\n [-0.75938718  2.         -0.46695317 -0.06068952]]\n\n\n\n\nMathematical and Statistical Methods\nNumPy provides array methods and functions for statistical computations like sum, mean, and std. These can be applied to entire arrays or along specific axes. Methods are called on array instances, while functions like numpy.sum take the array as the first argument.\nWrite code: Generate some normally distributed random data, and apply reduction (aggregation) functions to the array - average, total, standard deviation, standard variance, minimum, maximum, indices of minimum and maximum, cumulative sum, cumulative product.\n\nimport numpy as np\n\n# create a random number generator using normal distributionin 5x4 array\nrng = np.random.default_rng(seed=12345)\narr = rng.standard_normal((5, 4))\nprint(\"Random number array: \", arr)\nprint(\"Average of array - array.mean(): \", arr.mean()) # call on array\nprint(\"Average of array - numpy.mean(): \", np.mean(arr)) # pass array\n\n# compute avearge over columns\nprint(\"Average along columns: \", arr.mean(axis=1)) \n\n# compute total over rows\nprint(\"Total along the rows: \", arr.sum(axis=0)) \n\n# standard deviation and variance\nprint(\"Standard variance: \", np.var(arr))\nprint(\"Standard deviation: \", np.std(arr)) # sqrt(standard_variance)\n\n# maximum value\nprint(\"Maximum: \", np.max(arr), \"Indice of max element: \", np.argmax(arr))\n\n# minimum value\nprint(\"Minimum: \", np.min(arr),  \"Indice of min element: \", np.argmin(arr))\n\n# cumulative sum of elements - flattened\nprint(\"Cumulative sum: \", arr.cumsum())\n\n# cumulative sum of elements (axis=1, columns)\nprint(\"Cumulative sum(cols): \", arr.cumsum(axis=1))\n\n# cumulative sum of elements (axis=0, rows)\nprint(\"Cumulative sum(rows): \", arr.cumsum(axis=0))\n\n# cumulative product of elements\nprint(\"Cumulative product: \", arr.cumprod())\n\nRandom number array:  [[-1.42382504  1.26372846 -0.87066174 -0.25917323]\n [-0.07534331 -0.74088465 -1.3677927   0.6488928 ]\n [ 0.36105811 -1.95286306  2.34740965  0.96849691]\n [-0.75938718  0.90219827 -0.46695317 -0.06068952]\n [ 0.78884434 -1.25666813  0.57585751  1.39897899]]\nAverage of array - array.mean():  0.0010611661248891013\nAverage of array - numpy.mean():  0.0010611661248891013\nAverage along columns:  [-0.32248289 -0.38378196  0.4310254  -0.0962079   0.37675318]\nTotal along the rows:  [-1.10865307 -1.78448912  0.21785956  2.69650595]\nStandard variance:  1.1895475864268188\nStandard deviation:  1.0906638283297099\nMaximum:  2.347409654378852 Indice of max element:  10\nMinimum:  -1.95286306301219 Indice of min element:  9\nCumulative sum:  [-1.42382504 -0.16009658 -1.03075832 -1.28993155 -1.36527486 -2.10615951\n -3.47395221 -2.82505941 -2.4640013  -4.41686436 -2.06945471 -1.1009578\n -1.86034498 -0.95814671 -1.42509988 -1.4857894  -0.69694505 -1.95361319\n -1.37775567  0.02122332]\nCumulative sum(cols):  [[-1.42382504 -0.16009658 -1.03075832 -1.28993155]\n [-0.07534331 -0.81622796 -2.18402066 -1.53512786]\n [ 0.36105811 -1.59180495  0.7556047   1.72410161]\n [-0.75938718  0.14281109 -0.32414208 -0.3848316 ]\n [ 0.78884434 -0.46782379  0.10803373  1.50701272]]\nCumulative sum(rows):  [[-1.42382504  1.26372846 -0.87066174 -0.25917323]\n [-1.49916834  0.52284381 -2.23845444  0.38971957]\n [-1.13811023 -1.43001926  0.10895521  1.35821647]\n [-1.89749741 -0.52782098 -0.35799796  1.29752695]\n [-1.10865307 -1.78448912  0.21785956  2.69650595]]\nCumulative product:  [-1.42382504e+00 -1.79932822e+00  1.56660623e+00 -4.06022405e-01\n  3.05910707e-02 -2.26644548e-02  3.10002759e-02  2.01158559e-02\n  7.26299297e-03 -1.41836307e-02 -3.32947916e-02 -3.22459027e-02\n  2.44871251e-02  2.20922420e-02 -1.03160425e-02  6.26075655e-04\n  4.93876240e-04 -6.20638532e-04 -3.57399363e-04 -4.99994201e-04]\n\n\n\n\nMethods for Boolean Arrays\nWrite code: Generate an array of random numbers, them count the positive numbers and negative numbers.\n\nprint(\"sum() can be used as a way of counting Treu/False values in an Boolean array due to its coerced feature to 1 / 0\")\n\nimport numpy as np\n\n# Get a random number generator\nrng = np.random.default_rng(seed=12345)\narr = rng.standard_normal(50)\nprint(\"Original arr: \", arr)\n\n# Evaluate the array for positive elements\narr_bool = arr &gt; 0\nprint(\"Element eval for positiveness: \", arr_bool)\nprint(\"Count the True's: \", sum(arr_bool))\n\n# Evaluate if any or all of the array is True\nprint(\"Is any element in array True? : \", arr_bool.any())\nprint(\"Are all elemtns in array True?: \", arr_bool.all())\n\nsum() can be used as a way of counting Treu/False values in an Boolean array due to its coerced feature to 1 / 0\nOriginal arr:  [-1.42382504  1.26372846 -0.87066174 -0.25917323 -0.07534331 -0.74088465\n -1.3677927   0.6488928   0.36105811 -1.95286306  2.34740965  0.96849691\n -0.75938718  0.90219827 -0.46695317 -0.06068952  0.78884434 -1.25666813\n  0.57585751  1.39897899  1.32229806 -0.29969852  0.90291934 -1.62158273\n -0.15818926  0.44948393 -1.34360107 -0.08168759  1.72473993  2.61815943\n  0.77736134  0.8286332  -0.95898831 -1.20938829 -1.41229201  0.54154683\n  0.7519394  -0.65876032 -1.22867499  0.25755777  0.31290292 -0.13081169\n  1.26998312 -0.09296246 -0.06615089 -1.10821447  0.13595685  1.34707776\n  0.06114402  0.0709146 ]\nElement eval for positiveness:  [False  True False False False False False  True  True False  True  True\n False  True False False  True False  True  True  True False  True False\n False  True False False  True  True  True  True False False False  True\n  True False False  True  True False  True False False False  True  True\n  True  True]\nCount the True's:  25\nIs any element in array True? :  True\nAre all elemtns in array True?:  False\n\n\n\n\nSorting\nWrite code: Generate an array of random numbers and sort them in place along the columns and rows\n\n# Create a random number generator and an array of random numbers from it.\nrng = np.random.default_rng(seed=12345)\narr = rng.standard_normal(6)\nprint(\"Original array: \", arr)\n\n# sort the array in place\narr.sort()\nprint(\"Array sorted: \", arr)\n\n# Create a 3x3 array and sort them aloing columns and rows\narr = rng.standard_normal((3,3))\nprint(\"Orginal array: \", arr)\narr.sort(axis=0)\nprint(\"Array each column sorted in place: \", arr)\narr.sort(axis=1)\nprint(\"Array each row sorted in place: \", arr)\n\n# Get a copy of sorted array\narr2 = np.array([5, 10, 7, 1, 0, 3])\nsorted_arr2 = np.sort(arr2)\nprint(\"Sorted copy: \", sorted_arr2)\nprint(\"Original: \", arr2)\n\nOriginal array:  [-1.42382504  1.26372846 -0.87066174 -0.25917323 -0.07534331 -0.74088465]\nArray sorted:  [-1.42382504 -0.87066174 -0.74088465 -0.25917323 -0.07534331  1.26372846]\nOrginal array:  [[-1.3677927   0.6488928   0.36105811]\n [-1.95286306  2.34740965  0.96849691]\n [-0.75938718  0.90219827 -0.46695317]]\nArray each column sorted in place:  [[-1.95286306  0.6488928  -0.46695317]\n [-1.3677927   0.90219827  0.36105811]\n [-0.75938718  2.34740965  0.96849691]]\nArray each row sorted in place:  [[-1.95286306 -0.46695317  0.6488928 ]\n [-1.3677927   0.36105811  0.90219827]\n [-0.75938718  0.96849691  2.34740965]]\nSorted copy:  [ 0  1  3  5  7 10]\nOriginal:  [ 5 10  7  1  0  3]\n\n\n\n\nUnique and Other Set Logic\nWrite code: Create a numpy 1-d array with arbitrary names, and create an array with unique elements\n\n# create 1-d array\nnames = np.array([\"Bob\", \"Will\", \"Joe\", \"Bob\", \"Will\", \"Joe\", \"Joe\"])\nprint(\"names: \", names)\n\n# create an array of unique elements\nun = np.unique(names)\nprint(un, type(un)) # numpy.ndarray\n\n# equivalent built-in Python funtino, slower\nun = sorted(set(names))\nprint(un, type(un))  # list\n\n# test membership\nvalues = np.array([6, 0, 0, 3, 2, 5, 6])\ntest_against = [2, 3, 6]\nprint(\"Values: \", values)\nprint(\"Texting membership of each element in values: \", np.in1d(values, test_against))\n\nnames:  ['Bob' 'Will' 'Joe' 'Bob' 'Will' 'Joe' 'Joe']\n['Bob' 'Joe' 'Will'] &lt;class 'numpy.ndarray'&gt;\n['Bob', 'Joe', 'Will'] &lt;class 'list'&gt;\nValues:  [6 0 0 3 2 5 6]\nTexting membership of each element in values:  [ True False False  True  True False  True]\n\n\n\n\nMore Array set operations\n\nunique(x): sorted, unique elemtns in x\nintersect1d(x, y): sorted, common elements\nunion1d(x, y): sorted union of elements\nin1d(x, y): Boolean array of membership of each x in y\nsetdiff1d(x, y): elements that are only in x\nsetxor1d(x, y): elements that are either in x or y, but not in both\n\n\nimport numpy as np\n\n# two arrays for numpy set operations\nx = np.array([1, 2, 3, 4, 4])\ny = np.array([3, 4, 5, 6, 6])\nprint(\"x: \", x)\nprint(\"y: \", y)\nprint(\"unique(x): \", np.unique(x))\nprint(\"intersect1d(x,y):\", np.intersect1d(x,y))\nprint(\"union1d(x,y): \", np.union1d(x,y))\nprint(\"in1d(x,y): \", np.in1d(x,y))\nprint(\"setdiff1d(x,y): \", np.setdiff1d(x,y))\nprint(\"setxor1d(x,y): \", np.setxor1d(x,y))\n\nx:  [1 2 3 4 4]\ny:  [3 4 5 6 6]\nunique(x):  [1 2 3 4]\nintersect1d(x,y): [3 4]\nunion1d(x,y):  [1 2 3 4 5 6]\nin1d(x,y):  [False False  True  True  True]\nsetdiff1d(x,y):  [1 2]\nsetxor1d(x,y):  [1 2 5 6]"
  },
  {
    "objectID": "posts/p4da3d-numpy.html#file-input-and-output-with-arrays",
    "href": "posts/p4da3d-numpy.html#file-input-and-output-with-arrays",
    "title": "[4] Arrays and Vectorized Computation",
    "section": "File Input and Output with Arrays",
    "text": "File Input and Output with Arrays\nNumPy is able to save and load data to and from disk in some text or binary formats.\n\nNumPy built-in binary format\nnumpy.save numpy.load\nWrite code: Create some arrays and save to disk, and load from disk using numpy\n\nimport numpy as np\nimport os\n\narr = np.arange(10)\nprint(arr)\n\n# save\nnp.save(\"some_array\", arr) # save in binary format - some_array.npy\nprint(os.path.basename('./some_array.npy'))\n\n# load\ndata = np.load(\"some_array.npy\")\nprint(data, type(data))\n\n# work with multiple arrays, uncompressed / compressed\n# save\nnp.savez(\"array_archive.npz\", a=arr, b=arr, c=arr) # save three arrays uncompressed\n#np.savez_compressed(\"array_archive.npz\", a=arr, b=arr, c=arr) # save three arrays compressed\nprint(os.path.basename('./array_archive.npz'))\n# load\narch = np.load(\"array_archive.npz\")\nprint(arch)\nprint(arch[\"a\"])\nprint(arch[\"b\"])\nprint(arch[\"c\"])\n\n[0 1 2 3 4 5 6 7 8 9]\nsome_array.npy\n[0 1 2 3 4 5 6 7 8 9] &lt;class 'numpy.ndarray'&gt;\narray_archive.npz\nNpzFile 'array_archive.npz' with keys: a, b, c\n[0 1 2 3 4 5 6 7 8 9]\n[0 1 2 3 4 5 6 7 8 9]\n[0 1 2 3 4 5 6 7 8 9]"
  },
  {
    "objectID": "posts/p4da3d-numpy.html#linear-algebra",
    "href": "posts/p4da3d-numpy.html#linear-algebra",
    "title": "[4] Arrays and Vectorized Computation",
    "section": "Linear Algebra",
    "text": "Linear Algebra\nLinear algebra operations with NumPy - matrix multiplication, decompositions, determinants, other square matrix math\nWrite code: Create two arrays with random numbers and do matrix operations using NumPy\n\n# create arrays\nx = np.array([[1., 2., 3.], [4., 5., 6.]]) # 2x3\ny = np.array([[6., 23.], [-1, 7], [8, 9]]) # 3x2\nprint(\"x: \", x)\nprint(\"y: \", y)\n\n# array multiplication\n# print(x * y) # element-wise product error, different shapes\n\n# 2-d matrix multiplication\nprint(\"Matrix dot product on array: \", x.dot(y)) # 2x3 dot 3x2 -&gt; 2x2 matrix\nprint(\"Matirx dot product using NumPy: \", np.dot(x, y)) # equivalent to the above\n\n\n# 2-d x 1-d array\nprint(x @ np.ones(3)) # using @ symbol (Matrix Multiplication Operator) # 2x3 dot 1-d of (3,)\n\nx:  [[1. 2. 3.]\n [4. 5. 6.]]\ny:  [[ 6. 23.]\n [-1.  7.]\n [ 8.  9.]]\nMatrix dot product on array:  [[ 28.  64.]\n [ 67. 181.]]\nMatirx dot product using NumPy:  [[ 28.  64.]\n [ 67. 181.]]\n[ 6. 15.]\n\n\nWrite code: Create a random 5x5 matrix and run linear algebra functions from numpy.linalg\n\n# Create a random 5x5 matrix\nrng = np.random.default_rng(seed=12345)\narr = rng.standard_normal((5,5))\nprint(\"# Random 5x5 array\")\nprint(arr)\n\n# Get diagonal matrix in 1-d\nprint(\"# Get diagonal matrix in 1-d\")\nprint(np.diag(arr))\n\n# Convert 1-d matrix tp a square matrix with zeros on the off-diagonal\nprint(\"# Convert 1-d matrix tp a square matrix with zeros on the off-diagonal\")\nprint(np.diag(np.diag(arr)))\n\n# Matrix multiplication\nprint(\"# Matrix multiplication\")\nprint(np.dot(arr, arr))\n\n# Compute the matrix determinant\nprint(\"# Compute the matrix determinant\")\nprint(np.linalg.det(arr))\n\n# Compute the inverse of a square matrix\nprint(\"# Compute the inverse of a square matrix\")\nprint(np.linalg.inv(arr))\n\n# Random 5x5 array\n[[-1.42382504  1.26372846 -0.87066174 -0.25917323 -0.07534331]\n [-0.74088465 -1.3677927   0.6488928   0.36105811 -1.95286306]\n [ 2.34740965  0.96849691 -0.75938718  0.90219827 -0.46695317]\n [-0.06068952  0.78884434 -1.25666813  0.57585751  1.39897899]\n [ 1.32229806 -0.29969852  0.90291934 -1.62158273 -0.15818926]]\n# Get diagonal matrix in 1-d\n[-1.42382504 -1.3677927  -0.75938718  0.57585751 -0.15818926]\n# Convert 1-d matrix tp a square matrix with zeros on the off-diagonal\n[[-1.42382504  0.          0.          0.          0.        ]\n [ 0.         -1.3677927   0.          0.          0.        ]\n [ 0.          0.         -0.75938718  0.          0.        ]\n [ 0.          0.          0.          0.57585751  0.        ]\n [ 0.          0.          0.          0.         -0.15818926]]\n# Matrix multiplication\n[[-1.03669626 -4.55294704  2.97852946  0.01271579 -2.30471409]\n [ 0.98730448  2.43311934 -2.95225985  3.65824184  3.23796467]\n [-6.51464318  1.75796043 -2.39406508  0.33292097 -0.37758177]\n [-1.63302762 -2.33775979  2.05851653 -2.77016438 -0.36481815]\n [ 0.34808225  1.72365844 -0.13645271 -0.3135842  -2.17951354]]\n# Compute the matrix determinant\n-11.335264065758818\n# Compute the inverse of a square matrix\n[[-0.16403331  0.05319637  0.22318788  0.16488555  0.22079067]\n [ 0.31512456 -0.85623041  0.19880833 -1.17377785 -0.5472339 ]\n [-0.29807133 -1.17254064 -0.09117927 -1.78479938 -0.89798385]\n [-0.32695068 -0.4123963   0.11716846 -0.64698576 -0.82081818]\n [-0.31797409 -0.3983991  -0.23255394  0.0468956  -0.15061793]]"
  },
  {
    "objectID": "posts/p4da3d-numpy.html#example-random-walks",
    "href": "posts/p4da3d-numpy.html#example-random-walks",
    "title": "[4] Arrays and Vectorized Computation",
    "section": "Example Random Walks",
    "text": "Example Random Walks\nIllustrate an application of utilizing array operations - Random Walks, simple random walk starting at 0 with steps of 1 and -1 occurring with equal probability.\nPure Python implementation\n\nimport random\n\nposition = 0\nwalk = [position]\nnsteps = 1000\nfor _ in range(nsteps):\n  step = 1 if random.randint(0, 1) else -1\n  position += step\n  walk.append(position)\n\nprint(walk)\n\n# plot\nplt.clf() # clear any previous figure\nplt.plot(walk[:100]) # positions for first 100 steps\nplt.show()\n\n[0, -1, -2, -3, -2, -1, -2, -3, -2, -1, 0, 1, 2, 3, 4, 3, 4, 5, 6, 5, 4, 3, 4, 3, 2, 1, 0, 1, 2, 3, 2, 3, 2, 3, 4, 3, 2, 3, 4, 3, 2, 3, 4, 3, 2, 3, 2, 3, 2, 3, 4, 5, 6, 7, 8, 7, 8, 9, 8, 9, 10, 11, 10, 9, 10, 11, 10, 11, 10, 9, 8, 9, 8, 9, 10, 11, 10, 11, 10, 9, 8, 7, 6, 5, 4, 5, 4, 3, 4, 3, 2, 1, 2, 3, 2, 3, 2, 1, 0, -1, -2, -3, -4, -3, -4, -5, -6, -5, -6, -7, -8, -7, -8, -9, -10, -11, -12, -13, -14, -13, -12, -13, -12, -13, -14, -15, -14, -13, -12, -11, -10, -9, -10, -9, -8, -9, -8, -7, -8, -7, -8, -7, -6, -7, -8, -9, -10, -11, -12, -13, -14, -13, -14, -13, -12, -13, -12, -13, -14, -13, -14, -13, -12, -13, -14, -13, -14, -13, -12, -13, -14, -15, -14, -15, -16, -17, -16, -15, -14, -15, -14, -15, -14, -13, -14, -15, -16, -17, -18, -19, -20, -19, -18, -19, -20, -21, -20, -21, -20, -19, -18, -17, -18, -17, -16, -15, -16, -15, -14, -13, -14, -15, -14, -15, -16, -17, -16, -17, -16, -17, -16, -15, -14, -13, -14, -13, -14, -13, -12, -11, -12, -13, -14, -15, -14, -13, -14, -13, -12, -11, -10, -11, -12, -11, -12, -11, -12, -11, -12, -11, -10, -11, -10, -11, -12, -11, -12, -11, -12, -11, -12, -13, -14, -15, -14, -13, -12, -13, -14, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23, -24, -23, -22, -21, -20, -21, -22, -23, -24, -25, -24, -23, -24, -25, -26, -27, -28, -27, -28, -27, -28, -27, -26, -25, -24, -25, -24, -23, -22, -21, -22, -23, -24, -23, -24, -23, -24, -25, -24, -23, -22, -23, -24, -23, -24, -25, -24, -23, -24, -23, -24, -25, -26, -27, -28, -27, -26, -27, -26, -25, -26, -25, -24, -23, -24, -23, -22, -23, -24, -25, -26, -25, -24, -23, -24, -25, -24, -23, -22, -21, -20, -21, -20, -19, -18, -17, -16, -17, -18, -19, -18, -17, -18, -19, -20, -21, -20, -21, -22, -21, -22, -23, -24, -23, -24, -23, -24, -25, -24, -25, -24, -23, -22, -21, -20, -21, -20, -19, -18, -19, -20, -19, -18, -17, -16, -15, -16, -15, -14, -15, -16, -17, -16, -15, -16, -17, -16, -17, -16, -17, -18, -19, -20, -19, -20, -19, -18, -19, -18, -19, -18, -17, -16, -15, -16, -15, -16, -15, -16, -17, -18, -19, -18, -19, -18, -19, -20, -19, -18, -17, -16, -17, -16, -17, -16, -17, -16, -17, -18, -17, -16, -17, -16, -15, -14, -15, -16, -15, -14, -13, -14, -15, -16, -17, -18, -19, -20, -19, -18, -17, -16, -17, -16, -15, -14, -13, -12, -13, -12, -11, -10, -11, -12, -11, -10, -9, -10, -11, -12, -13, -12, -11, -10, -11, -10, -9, -8, -7, -6, -7, -6, -5, -4, -3, -4, -5, -4, -3, -4, -5, -6, -7, -8, -7, -6, -5, -4, -3, -4, -3, -4, -3, -2, -3, -2, -1, 0, -1, 0, 1, 0, -1, 0, -1, -2, -1, -2, -3, -2, -3, -2, -1, 0, 1, 2, 1, 2, 1, 2, 3, 4, 5, 6, 5, 6, 7, 8, 7, 8, 7, 8, 7, 8, 9, 10, 11, 10, 11, 12, 11, 12, 11, 10, 11, 10, 9, 8, 7, 8, 9, 10, 11, 12, 11, 12, 13, 12, 13, 12, 11, 12, 13, 14, 13, 14, 13, 14, 13, 14, 15, 16, 17, 16, 15, 16, 15, 14, 13, 12, 11, 12, 11, 10, 9, 8, 9, 10, 11, 10, 11, 10, 9, 8, 7, 6, 7, 6, 7, 8, 9, 8, 9, 8, 9, 10, 9, 10, 11, 10, 9, 8, 7, 8, 9, 8, 9, 8, 9, 10, 11, 12, 13, 12, 11, 10, 11, 10, 11, 10, 9, 8, 9, 8, 7, 6, 7, 8, 7, 8, 7, 8, 9, 8, 7, 8, 9, 10, 9, 8, 9, 10, 9, 10, 11, 12, 13, 12, 11, 12, 13, 12, 13, 14, 15, 14, 13, 14, 15, 16, 17, 16, 15, 14, 15, 14, 13, 14, 13, 12, 11, 10, 9, 8, 9, 8, 7, 6, 5, 6, 7, 6, 7, 6, 7, 6, 5, 6, 5, 6, 5, 4, 5, 4, 5, 6, 7, 8, 9, 8, 7, 6, 5, 6, 5, 4, 3, 2, 1, 0, 1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 3, 4, 3, 2, 1, 0, -1, -2, -1, -2, -1, -2, -3, -2, -3, -2, -3, -2, -3, -2, -3, -2, -3, -4, -3, -2, -3, -2, -3, -2, -3, -2, -3, -2, -3, -4, -3, -4, -3, -4, -3, -4, -5, -4, -5, -4, -5, -6, -5, -6, -5, -6, -7, -6, -7, -8, -7, -8, -7, -6, -5, -6, -7, -6, -5, -6, -5, -4, -5, -4, -5, -4, -3, -4, -3, -4, -5, -4, -3, -2, -1, -2, -1, 0, 1, 0, -1, -2, -3, -4, -3, -4, -5, -4, -5, -4, -5, -6, -7, -6, -7, -6, -5, -6, -7, -8, -7, -6, -5, -4, -3, -4, -5, -6, -7, -6, -5, -4, -5, -4, -3, -2, -3, -4, -3, -4, -5, -4, -5, -6, -7, -8, -7, -6, -5, -6, -5, -6, -5, -4, -5, -4, -3, -4, -5, -4, -5, -4, -5, -4, -5, -6, -7, -6, -7, -8, -7, -8, -9, -10, -11, -10, -9, -10, -11, -10, -11, -12, -13, -12, -13, -12, -11, -12, -13, -12, -13, -12, -11, -12, -13, -14, -15, -16, -15, -16, -17, -18, -19, -18, -19, -20, -21, -22, -21, -22, -23, -22, -21, -22, -21, -22, -23, -22, -21, -20, -21, -22, -21, -22, -23, -24, -23, -24, -23, -22, -23, -24, -23, -22, -23, -22, -21, -20, -21, -20, -19, -18, -19, -20, -19, -18]\n\n\n\n\n\n\nnsteps = 1000\nrng = np.random.default_rng(seed=12345)\ndraws = rng.integers(0, 2, size=nsteps) # sequence of 0 or 1, 1000 numbers\nprint(draws)\nsteps = np.where(draws == 0, 1, -1) # numpy ternary expression to generate sequence of step direction\nprint(steps)\nwalk = steps.cumsum() # walk is the result of cumulative sum of steps\nprint(walk)\n\n# plot the walk\nplt.clf()\nplt.plot(walk[:]) # all steps of walk\nplt.show()\n\n[1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0\n 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1\n 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1\n 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1\n 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0\n 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0\n 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0\n 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1\n 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0\n 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0\n 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0\n 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1\n 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1\n 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1\n 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0\n 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1\n 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1\n 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0\n 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1\n 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0\n 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0\n 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0\n 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1\n 1]\n[-1  1 -1  1  1 -1 -1 -1 -1  1 -1  1 -1 -1  1  1  1 -1 -1 -1 -1  1 -1 -1\n -1 -1  1  1  1  1  1 -1  1 -1  1  1  1 -1 -1  1 -1  1  1  1 -1  1  1  1\n  1  1 -1 -1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1\n -1 -1  1 -1 -1  1  1  1 -1  1 -1 -1 -1  1 -1 -1  1  1  1  1 -1  1 -1  1\n  1  1  1 -1  1  1 -1  1 -1 -1  1  1  1  1 -1  1 -1  1 -1  1 -1  1  1  1\n -1  1  1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1  1 -1 -1  1  1  1 -1 -1  1\n  1 -1  1 -1 -1  1 -1 -1  1  1  1  1  1 -1 -1 -1  1 -1  1  1 -1  1 -1 -1\n -1  1 -1 -1 -1  1  1 -1 -1  1 -1  1  1  1  1  1 -1 -1 -1 -1 -1  1 -1  1\n  1  1 -1 -1  1  1  1  1  1  1  1  1 -1 -1  1  1  1 -1 -1  1  1 -1  1  1\n  1  1  1  1  1  1  1  1  1 -1 -1 -1  1  1 -1 -1 -1  1 -1 -1  1 -1 -1  1\n -1  1 -1  1 -1  1 -1 -1  1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1 -1\n -1 -1  1  1  1 -1  1  1  1 -1  1 -1  1 -1  1 -1 -1  1 -1  1 -1 -1  1 -1\n  1  1 -1  1 -1 -1 -1  1 -1  1 -1 -1 -1 -1  1 -1 -1  1 -1  1  1 -1 -1  1\n -1  1  1  1  1  1 -1 -1 -1 -1  1 -1  1 -1  1 -1  1 -1  1  1  1 -1 -1 -1\n -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1 -1 -1  1  1  1 -1  1 -1 -1  1  1 -1 -1\n -1  1  1  1  1  1 -1 -1  1  1 -1  1  1 -1 -1 -1  1  1  1  1 -1 -1 -1  1\n  1 -1 -1 -1  1  1  1  1 -1  1 -1  1  1 -1 -1  1  1 -1 -1  1  1  1 -1  1\n -1 -1 -1  1  1  1 -1  1 -1 -1  1 -1  1  1 -1  1 -1  1 -1  1 -1  1 -1  1\n -1  1 -1 -1 -1 -1  1  1  1  1 -1  1 -1 -1  1 -1 -1 -1 -1 -1  1 -1 -1 -1\n  1  1 -1  1  1  1 -1 -1  1 -1  1  1  1 -1  1 -1  1  1  1  1 -1 -1 -1 -1\n  1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1  1  1  1  1 -1  1  1 -1 -1 -1  1  1  1\n  1  1  1 -1  1  1 -1  1  1  1 -1 -1  1 -1  1 -1 -1  1 -1 -1 -1  1  1 -1\n  1 -1  1  1 -1 -1  1  1 -1  1 -1 -1 -1  1  1 -1 -1  1 -1 -1  1  1 -1  1\n -1  1  1  1  1  1  1  1  1  1  1 -1  1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1\n  1  1 -1  1  1 -1 -1  1  1  1  1 -1  1 -1  1 -1  1  1 -1  1  1  1 -1 -1\n  1  1 -1 -1 -1  1  1 -1 -1  1  1  1 -1  1  1 -1  1 -1  1  1  1  1 -1 -1\n  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1 -1  1 -1  1 -1  1 -1  1\n -1  1 -1  1  1 -1 -1  1  1 -1 -1  1 -1 -1  1  1 -1 -1  1  1  1  1 -1  1\n -1  1  1 -1  1  1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1  1  1  1 -1  1  1 -1\n  1 -1  1  1  1 -1  1  1  1  1 -1  1  1  1  1 -1  1  1 -1 -1  1  1 -1 -1\n  1  1  1 -1  1 -1  1  1  1  1  1  1  1 -1  1  1 -1 -1  1 -1 -1  1  1 -1\n -1  1 -1  1 -1 -1  1 -1 -1 -1  1  1 -1  1 -1  1  1  1 -1  1  1  1  1 -1\n  1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1 -1 -1  1 -1  1  1 -1  1 -1  1  1\n  1  1 -1 -1 -1  1  1  1 -1 -1  1 -1 -1  1 -1 -1 -1  1 -1 -1  1  1 -1 -1\n  1 -1  1 -1 -1  1 -1 -1 -1  1 -1  1  1  1  1 -1 -1  1  1  1  1  1  1  1\n  1 -1 -1  1  1  1 -1  1  1  1 -1 -1 -1 -1 -1 -1  1 -1  1  1 -1 -1 -1  1\n  1 -1  1  1 -1 -1 -1 -1  1  1  1  1  1  1 -1  1 -1 -1 -1  1  1 -1  1  1\n -1  1  1 -1 -1 -1  1 -1  1  1  1 -1  1 -1 -1  1  1  1 -1  1 -1 -1 -1  1\n  1 -1 -1 -1  1  1 -1 -1  1 -1 -1 -1  1  1 -1  1  1 -1 -1  1 -1  1 -1  1\n  1  1  1 -1 -1 -1 -1 -1  1  1  1 -1  1  1 -1 -1 -1 -1 -1  1  1  1  1  1\n -1  1 -1 -1 -1 -1  1  1  1 -1  1  1  1  1  1 -1 -1 -1  1  1 -1 -1  1  1\n  1  1 -1  1  1 -1  1  1 -1 -1 -1  1 -1  1 -1 -1]\n[-1  0 -1  0  1  0 -1 -2 -3 -2 -3 -2 -3 -4 -3 -2 -1 -2 -3 -4 -5 -4 -5 -6\n -7 -8 -7 -6 -5 -4 -3 -4 -3 -4 -3 -2 -1 -2 -3 -2 -3 -2 -1  0 -1  0  1  2\n  3  4  3  2  3  4  5  6  7  8  9  8  7  6  5  4  5  4  3  2  1  0 -1 -2\n -3 -4 -3 -4 -5 -4 -3 -2 -3 -2 -3 -4 -5 -4 -5 -6 -5 -4 -3 -2 -3 -2 -3 -2\n -1  0  1  0  1  2  1  2  1  0  1  2  3  4  3  4  3  4  3  4  3  4  5  6\n  5  6  7  8  7  8  7  8  7  8  7  8  7  8  7  8  7  6  7  8  9  8  7  8\n  9  8  9  8  7  8  7  6  7  8  9 10 11 10  9  8  9  8  9 10  9 10  9  8\n  7  8  7  6  5  6  7  6  5  6  5  6  7  8  9 10  9  8  7  6  5  6  5  6\n  7  8  7  6  7  8  9 10 11 12 13 14 13 12 13 14 15 14 13 14 15 14 15 16\n 17 18 19 20 21 22 23 24 25 24 23 22 23 24 23 22 21 22 21 20 21 20 19 20\n 19 20 19 20 19 20 19 18 19 20 19 20 21 20 21 22 23 24 23 24 23 22 23 22\n 21 20 21 22 23 22 23 24 25 24 25 24 25 24 25 24 23 24 23 24 23 22 23 22\n 23 24 23 24 23 22 21 22 21 22 21 20 19 18 19 18 17 18 17 18 19 18 17 18\n 17 18 19 20 21 22 21 20 19 18 19 18 19 18 19 18 19 18 19 20 21 20 19 18\n 17 16 15 14 15 14 13 12 11 10 11 10  9 10 11 12 11 12 11 10 11 12 11 10\n  9 10 11 12 13 14 13 12 13 14 13 14 15 14 13 12 13 14 15 16 15 14 13 14\n 15 14 13 12 13 14 15 16 15 16 15 16 17 16 15 16 17 16 15 16 17 18 17 18\n 17 16 15 16 17 18 17 18 17 16 17 16 17 18 17 18 17 18 17 18 17 18 17 18\n 17 18 17 16 15 14 15 16 17 18 17 18 17 16 17 16 15 14 13 12 13 12 11 10\n 11 12 11 12 13 14 13 12 13 12 13 14 15 14 15 14 15 16 17 18 17 16 15 14\n 15 14 13 12 11 10 11 10  9  8  7  8  9 10 11 10 11 12 11 10  9 10 11 12\n 13 14 15 14 15 16 15 16 17 18 17 16 17 16 17 16 15 16 15 14 13 14 15 14\n 15 14 15 16 15 14 15 16 15 16 15 14 13 14 15 14 13 14 13 12 13 14 13 14\n 13 14 15 16 17 18 19 20 21 22 23 22 23 22 21 20 19 18 17 16 17 16 15 16\n 17 18 17 18 19 18 17 18 19 20 21 20 21 20 21 20 21 22 21 22 23 24 23 22\n 23 24 23 22 21 22 23 22 21 22 23 24 23 24 25 24 25 24 25 26 27 28 27 26\n 27 26 25 24 23 22 21 20 19 18 19 20 21 22 23 24 23 24 23 24 23 24 23 24\n 23 24 23 24 25 24 23 24 25 24 23 24 23 22 23 24 23 22 23 24 25 26 25 26\n 25 26 27 26 27 28 27 28 27 26 25 26 25 24 23 22 21 22 23 24 23 24 25 24\n 25 24 25 26 27 26 27 28 29 30 29 30 31 32 33 32 33 34 33 32 33 34 33 32\n 33 34 35 34 35 34 35 36 37 38 39 40 41 40 41 42 41 40 41 40 39 40 41 40\n 39 40 39 40 39 38 39 38 37 36 37 38 37 38 37 38 39 40 39 40 41 42 43 42\n 43 44 43 44 43 42 43 42 41 40 39 40 39 38 37 38 37 38 39 38 39 38 39 40\n 41 42 41 40 39 40 41 42 41 40 41 40 39 40 39 38 37 38 37 36 37 38 37 36\n 37 36 37 36 35 36 35 34 33 34 33 34 35 36 37 36 35 36 37 38 39 40 41 42\n 43 42 41 42 43 44 43 44 45 46 45 44 43 42 41 40 41 40 41 42 41 40 39 40\n 41 40 41 42 41 40 39 38 39 40 41 42 43 44 43 44 43 42 41 42 43 42 43 44\n 43 44 45 44 43 42 43 42 43 44 45 44 45 44 43 44 45 46 45 46 45 44 43 44\n 45 44 43 42 43 44 43 42 43 42 41 40 41 42 41 42 43 42 41 42 41 42 41 42\n 43 44 45 44 43 42 41 40 41 42 43 42 43 44 43 42 41 40 39 40 41 42 43 44\n 43 44 43 42 41 40 41 42 43 42 43 44 45 46 47 46 45 44 45 46 45 44 45 46\n 47 48 47 48 49 48 49 50 49 48 47 48 47 48 47 46]"
  },
  {
    "objectID": "posts/p4da3d-numpy.html",
    "href": "posts/p4da3d-numpy.html",
    "title": "[4] Arrays and Vectorized Computation",
    "section": "",
    "text": "NumPy is a fundamental Python library for numerical and scientific computing. It provides support for working with large, multi-dimensional arrays and matrices, along with a vast collection of mathematical functions to operate on these arrays. Here’s a must-know summary of NumPy:\nNumPy is the foundation of many scientific and data science libraries in Python and is an essential tool for tasks such as data manipulation, statistical analysis, machine learning, and more. Its ease of use and performance make it a go-to choice for researchers, scientists, engineers, and data analysts working with numerical data in Python.\nIdea of performance difference\n# NumPy array of one-million integers vs Python list\n\nimport numpy as np\nimport timeit\n\nmy_arr = np.arange(1_000_000)\nmy_list = list(range(1_000_000))\n# time it\nexecution_time = timeit.timeit(lambda: my_arr * 2, number=10)\nprint(f\"np array multiplication: {execution_time}\")\n\nexecution_time = timeit.timeit(lambda: [ x*2 for x in my_list * 2 ], number=10)\nprint(f\"Python list element multiplication: {execution_time}\")\n\nnp array multiplication: 0.008902800007490441\nPython list element multiplication: 1.2835534999903757"
  },
  {
    "objectID": "posts/p4da3d-numpy.html#the-numpy-ndarray-a-multidimensional-array-object",
    "href": "posts/p4da3d-numpy.html#the-numpy-ndarray-a-multidimensional-array-object",
    "title": "[4] Arrays and Vectorized Computation",
    "section": "The NumPy ndarray: A Multidimensional Array Object",
    "text": "The NumPy ndarray: A Multidimensional Array Object\nBatch computation\n\nimport numpy as np\n\ndata = np.array([\n  [1.5, -0.1, 3], \n  [0, -3, 6.5],\n  [2, 7, 9.9],\n  [23, 9, 21]\n])\nprint(\"data: \", data)\n\n# math operations with the ndarray\nprint(\"data * 10: \", data * 10)\nprint(\"data + data: \", data + data)\n\ndata:  [[ 1.5 -0.1  3. ]\n [ 0.  -3.   6.5]\n [ 2.   7.   9.9]\n [23.   9.  21. ]]\ndata * 10:  [[ 15.  -1.  30.]\n [  0. -30.  65.]\n [ 20.  70.  99.]\n [230.  90. 210.]]\ndata + data:  [[ 3.  -0.2  6. ]\n [ 0.  -6.  13. ]\n [ 4.  14.  19.8]\n [46.  18.  42. ]]\n\n\n\n# shape and type\nprint(data.shape)  # tuple 4R x 3C\nprint(data.dtype)  # ndarray is for homogeneous data\n\n(4, 3)\nfloat64\n\n\n\nCreating ndarrays\n\n# ndarray creation\n\n# np.array()\ndata1 = [6, 7.5, 8, 0, 1] # any sequence-like object\narr1 = np.array(data1)\nprint(arr1)\n\ndata2 = [[1, 2, 3, 4], [5, 6, 7, 8]] # list of equal length array\narr2 = np.array(data2)\nprint(arr2)\nprint(arr2.shape)\nprint(arr2.dtype)\n\n# np.zeros() / np.empty()\nprint(np.zeros(10))\nprint(np.zeros((3, 6)))\nprint(np.zeros((2, 3, 5)))\nprint(np.empty(((2, 3, 2)))) # uninitialized meory with garbage data\n\n# np.arange()\nprint(np.arange(15))\n\n[6.  7.5 8.  0.  1. ]\n[[1 2 3 4]\n [5 6 7 8]]\n(2, 4)\nint32\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\n[[[0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0.]]\n\n [[0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0.]]]\n[[[ 3.  -0.2]\n  [ 6.   0. ]\n  [-6.  13. ]]\n\n [[ 4.  14. ]\n  [19.8 46. ]\n  [18.  42. ]]]\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n\n\nSome important NumPy array creation functions from Python for Data Analysis 3E by Wes Mckinney\n\n\nData types for ndarrays - dtype\n\n# dtype\narr1 = np.array([1, 2, 3], dtype=np.float64)\narr2 = np.array([1, 2, 3], dtype=np.int32)\nprint(arr1, arr1.dtype)\nprint(arr2, arr2.dtype)\n\n[1. 2. 3.] float64\n[1 2 3] int32\n\n\n\n# type casting - astype()\narr_float = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])\nprint(arr_float)\narr_int = arr_float.astype(np.int32)  # decimal truncated\nprint(arr_int)\nnumeric_strings = np.array([\"1.25\", \"-9.6\", \"42\"], dtype=np.string_)\nprint(numeric_strings)\nnumeric_from_strings = numeric_strings.astype(float) # ValueError when fails\nprint(numeric_from_strings)\n# conver to the other array's type\nint_array = np.arange(10)\ncalibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)\nprint(int_array.astype(calibers.dtype))\n\n[ 3.7 -1.2 -2.6  0.5 12.9 10.1]\n[ 3 -1 -2  0 12 10]\n[b'1.25' b'-9.6' b'42']\n[ 1.25 -9.6  42.  ]\n[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n\n\n\n\nArithmetic with NumPy Arrays\n\n# arrays in shame shape\nprint(\"\\nArrays in shame shape\\n\")\narr = np.array([[1., 2., 3.], [4., 5., 6.]])\nprint(arr)\nprint(arr * arr) # element-wise multiplication\n# print(arr * [10, 200]) # broadcast error, not in same shape\nprint(arr - arr) # element-wies subraction\n\n# array with salar\nprint(\"\\nArray with salar\\n\")\nprint(1 / arr) # applied with each element\nprint(arr ** 2)  \n\n# array comparison\nprint(\"\\nArray comparison\\n\")\narr2 = np.array([[0., 4., 1.], [7., 2., 12.]])\nprint(arr2)\narr_bool = arr &lt; arr2  # element-wise boolean evaluation\nprint(arr_bool)\nif arr_bool.all():\n  print(\"Two arrays are same.\") \nelse:\n  print(\"Two arrays are different.\")\n\n\nArrays in shame shape\n\n[[1. 2. 3.]\n [4. 5. 6.]]\n[[ 1.  4.  9.]\n [16. 25. 36.]]\n[[0. 0. 0.]\n [0. 0. 0.]]\n\nArray with salar\n\n[[1.         0.5        0.33333333]\n [0.25       0.2        0.16666667]]\n[[ 1.  4.  9.]\n [16. 25. 36.]]\n\nArray comparison\n\n[[ 0.  4.  1.]\n [ 7.  2. 12.]]\n[[False  True False]\n [ True False  True]]\nTwo arrays are different.\n\n\n\n\nBasic Indexing and Slicing\n\n# Similar to Python lists\nprint(\"\\nSimilar to Python lists...\\n\")\narr = np.arange(10)\nprint(arr)\nprint(arr[5]) # element accessing\nprint(arr[5:8]) # slicing\narr[5:8] = 99 # assignment changes broadcasts the new valjue, relfected to the original array - value propagation / broadcast\nprint(arr)\narr_slice = arr[5:8] # check broadcasted value\nprint(arr_slice)\narr_slice[1] = 12345 # value propagation to the original\nprint(arr)\n\n# Assignment to all values - bare slice [:]\nprint(\"\\nAssignment to all values - bare slice [:]...\\n\")\narr_slice[:] = 64\nprint(arr)  # value propagation to the original \n\n# 2d array, each index of 1-d array, not scalar\nprint(\"\\n2d array, each index of 1-d array, not scalar...\\n\")\narr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(arr2d)\nprint(arr2d[2], type(arr2d[2]))\nprint(arr2d[0][2]) # accessing individual element\n\n# Axis in 2-d array\nprint(\"\\narr(r, c) - Axis0 (row), Axis1 (col) in 2-d array...\\n\")\nprint(arr2d[0, 2]) # same as above\n\n\nSimilar to Python lists...\n\n[0 1 2 3 4 5 6 7 8 9]\n5\n[5 6 7]\n[ 0  1  2  3  4 99 99 99  8  9]\n[99 99 99]\n[    0     1     2     3     4    99 12345    99     8     9]\n\nAssignment to all values - bare slice [:]...\n\n[ 0  1  2  3  4 64 64 64  8  9]\n\n2d array, each index of 1-d array, not scalar...\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n[7 8 9] &lt;class 'numpy.ndarray'&gt;\n3\n\narr(r, c) - Axis0 (row), Axis1 (col) in 2-d array...\n\n3\n\n\n\n# 3-d -&gt; 2-d -&gt; 1-d \narr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nprint(\"arr3d: \", arr3d)\nprint(arr3d.shape)\nprint(arr3d[0]) # arr3d[0] 2x3 array, first row\nold_values = arr3d[0].copy() # save to variable\nprint(old_values)\narr3d[0] = 99 # change with scalar, broadcasting to the whole 2x3\nprint(arr3d)\narr3d[0] = old_values # change with array\nprint(arr3d)\nprint(\"arr3d[1, 0]: \", arr3d[1, 0])\n\narr3d:  [[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\n(2, 2, 3)\n[[1 2 3]\n [4 5 6]]\n[[1 2 3]\n [4 5 6]]\n[[[99 99 99]\n  [99 99 99]]\n\n [[ 7  8  9]\n  [10 11 12]]]\n[[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\narr3d[1, 0]:  [7 8 9]\n\n\n\n# Indexing with slices\n\nprint(\"arr: \", arr)\nprint(\"arr[1:6]: \", arr[1:6]) # 1-d array\nprint(\"arr2d: \", arr2d)\nprint(\"arr2d[:2]: \", arr2d[:2]) # 2-d array, slicing rows\nprint(\"arr2d[:2, 1:]: \",arr2d[:2, 1:] ) # 2-d array, slicing rows and columns\n\n# select the second row, and first two cols.\nlower_dim_slice = arr2d[1, :2]  # slicing 1-d array\nprint(lower_dim_slice)\nprint(lower_dim_slice.shape)\n\n# select the third col, and first two rows\nprint(arr2d[:2, 2])\nprint(arr)\n\n# select all rows and the first col.\nprint(arr2d[:, :1])\n\narr:  [ 0  1  2  3  4 64 64 64  8  9]\narr[1:6]:  [ 1  2  3  4 64]\narr2d:  [[1 2 3]\n [4 5 6]\n [7 8 9]]\narr2d[:2]:  [[1 2 3]\n [4 5 6]]\narr2d[:2, 1:]:  [[2 3]\n [5 6]]\n[4 5]\n(2,)\n[3 6]\n[ 0  1  2  3  4 64 64 64  8  9]\n[[1]\n [4]\n [7]]\n\n\n\n\nBoolean Indexing\n\n# Boolean indexing\n\nnames = np.array([\"Bob\", \"Joe\", \"Will\", \"Bob\", \"Will\", \"Joe\", \"Joe\"])\ndata = np.array([[4, 7], [0, 2], [-5, 6], [0, 0], [1, 2], [-12, -4], [3, 4]])\nprint(names, names.shape)\nprint(data, data.shape)\n\n# Suppose each name corresponds to a row in the data array\n\n# row filtering using Boolean indexing, compare names with the string 'Bob' to filter the corresponding rows from data array\nprint(names == \"Bob\")  # output - array of comparison result for each element\n\nprint(data[names == \"Bob\"]) # filter the corresponding rows from data, where \"Bob\" was matched in names.\nprint(data[names == \"Bob\", :1]) # also slice the column(s) to select the first col.\nprint(data[names == \"Bob\", 1]) # reduced to 1-d\n\n# negating the conditon\nprint(\"\\n__Negating the condition__\")\nprint(\"names: \", names)\nprint(names == \"Bob\")\nprint(names != \"Bob\")\nprint(~(names == \"Bob\")) # same as !=\n\n# Filter rows in data with the same row index where Bob is not matched in names\nprint(data[~(names == \"Bob\")])\n\n# Use a reference variable to invert Boolean condition\ncond = names == \"Bob\"\nprint(cond)\nprint(data[~cond])  # same as data[~(names == \"Bob\")]\n\n# Multiple Boolean conditions - &, |\ncond = (names == \"Bob\") | (names == \"Will\")\nprint(cond)\nprint(data[cond])\n\n['Bob' 'Joe' 'Will' 'Bob' 'Will' 'Joe' 'Joe'] (7,)\n[[  4   7]\n [  0   2]\n [ -5   6]\n [  0   0]\n [  1   2]\n [-12  -4]\n [  3   4]] (7, 2)\n[ True False False  True False False False]\n[[4 7]\n [0 0]]\n[[4]\n [0]]\n[7 0]\n\n__Negating the condition__\nnames:  ['Bob' 'Joe' 'Will' 'Bob' 'Will' 'Joe' 'Joe']\n[ True False False  True False False False]\n[False  True  True False  True  True  True]\n[False  True  True False  True  True  True]\n[[  0   2]\n [ -5   6]\n [  1   2]\n [-12  -4]\n [  3   4]]\n[ True False False  True False False False]\n[[  0   2]\n [ -5   6]\n [  1   2]\n [-12  -4]\n [  3   4]]\n[ True False  True  True  True False False]\n[[ 4  7]\n [-5  6]\n [ 0  0]\n [ 1  2]]\n\n\n\nprint(\"\\n___Setting values with Boolean arrays___\\n\")\nprint(data)\nprint(data &lt; 0)\ndata[data &lt; 0] = 999  # value setting applies to element with True\nprint(data)\n\n\n___Setting values with Boolean arrays___\n\n[[  4   7]\n [  0   2]\n [ -5   6]\n [  0   0]\n [  1   2]\n [-12  -4]\n [  3   4]]\n[[False False]\n [False False]\n [ True False]\n [False False]\n [False False]\n [ True  True]\n [False False]]\n[[  4   7]\n [  0   2]\n [999   6]\n [  0   0]\n [  1   2]\n [999 999]\n [  3   4]]\n\n\n\n#\ncond = (names != \"Joe\") # Save the boolean array to a variable\nprint(cond) \ndata[cond] = 7 # set the all element in the filtered rows\nprint(data)\n\n[ True False  True  True  True False False]\n[[  7   7]\n [  0   2]\n [  7   7]\n [  7   7]\n [  7   7]\n [999 999]\n [  3   4]]\n\n\n\n\nFancy Indexing\nDescribe indexing using array of integers\n\n# Create an array of 5,4 filled initialized with zero\narr = np.zeros((8, 4))\n\nfor i in range(8):\n  arr[i] = i # fill each row with row index number\nprint(arr, \"\\n\")\n\n# select rows in custom order - pass an array of row index in customer order\nprint(arr[ [4, 3, 0, 6]], \"\\n\")\nprint(arr[ [-3, -5, -7] ], \"\\n\")\n\n# passing multiple index arrays - select using each tuple of indices\narr = np.arange(32).reshape((8, 4))\nprint(arr, \"\\n\")\nprint(arr[[1, 5, 7, 2], [0, 3, 1, 2]], \"\\n\") # indexing using (1,0) (5,3) (7,1) (2,2)\n\n# retangular shaping and subsetting\nprint(arr.shape)\nprint(arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]], \"\\n\") # rows subsetted in custom order, then cols subsetted in custom order\n\n# value assignment using fancy indexing\nprint(arr[[1, 5, 7, 2], [0, 3, 1, 2]])\narr[[1, 5, 7, 2], [0, 3, 1, 2]] = 777  # changes the indexed value\nprint(arr)\n\n[[0. 0. 0. 0.]\n [1. 1. 1. 1.]\n [2. 2. 2. 2.]\n [3. 3. 3. 3.]\n [4. 4. 4. 4.]\n [5. 5. 5. 5.]\n [6. 6. 6. 6.]\n [7. 7. 7. 7.]] \n\n[[4. 4. 4. 4.]\n [3. 3. 3. 3.]\n [0. 0. 0. 0.]\n [6. 6. 6. 6.]] \n\n[[5. 5. 5. 5.]\n [3. 3. 3. 3.]\n [1. 1. 1. 1.]] \n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]\n [24 25 26 27]\n [28 29 30 31]] \n\n[ 4 23 29 10] \n\n(8, 4)\n[[ 4  7  5  6]\n [20 23 21 22]\n [28 31 29 30]\n [ 8 11  9 10]] \n\n[ 4 23 29 10]\n[[  0   1   2   3]\n [777   5   6   7]\n [  8   9 777  11]\n [ 12  13  14  15]\n [ 16  17  18  19]\n [ 20  21  22 777]\n [ 24  25  26  27]\n [ 28 777  30  31]]\n\n\n\n\nTransposing Arrays and Swapping Axes\n\n# T attribute\nprint(\"\\nT attribute in array to transpose...\\n\")\narr = np.arange(15).reshape(3, 5)\nprint(arr)\nprint(arr.T)\n\n# inner matrix product using T\nprint(\"\\ninner matrix product using T...\\n\")\narr = np.array([[0, 1, 0], [1, 2, -2], [6, 3, 2], [-1, 0, -1], [1, 0, 1\n]])\nprint(arr)\nprint(arr.T)\nprint(np.dot(arr.T, arr)) # matrix dot product\nprint(arr.T @ arr) # same as above\n\nprint(\"\\ndarray.swapaxes, same effect of transposing...\\n\")\nprint(arr)\nprint(arr.swapaxes(0, 1))\nprint(arr.T == arr.swapaxes(0,1))\n\n\nT attribute in array to transpose...\n\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]]\n[[ 0  5 10]\n [ 1  6 11]\n [ 2  7 12]\n [ 3  8 13]\n [ 4  9 14]]\n\ninner matrix product using T...\n\n[[ 0  1  0]\n [ 1  2 -2]\n [ 6  3  2]\n [-1  0 -1]\n [ 1  0  1]]\n[[ 0  1  6 -1  1]\n [ 1  2  3  0  0]\n [ 0 -2  2 -1  1]]\n[[39 20 12]\n [20 14  2]\n [12  2 10]]\n[[39 20 12]\n [20 14  2]\n [12  2 10]]\n\ndarray.swapaxes, same effect of transposing...\n\n[[ 0  1  0]\n [ 1  2 -2]\n [ 6  3  2]\n [-1  0 -1]\n [ 1  0  1]]\n[[ 0  1  6 -1  1]\n [ 1  2  3  0  0]\n [ 0 -2  2 -1  1]]\n[[ True  True  True  True  True]\n [ True  True  True  True  True]\n [ True  True  True  True  True]]"
  },
  {
    "objectID": "posts/p4da3d-pandas.html",
    "href": "posts/p4da3d-pandas.html",
    "title": "[5] Pandas",
    "section": "",
    "text": "pandas is ….\nTwo workhorse data structures: Series and DataFrame, solid foundation of wide variety of data tasks.\nSeries - one-dimensional array-like object with a sequence of values and index: array + lables\n# Simple Series\nimport numpy as np\nimport pandas as pd\n\nobj = pd.Series([4, 7, -5, 3])\nprint(obj)\nprint(type(obj))\nprint(obj.array)\nprint(obj.index)\n\n# A Series with a custom index\nobj2 = pd.Series([4, 7, -5, 3], index=[\"d\", \"b\", \"a\", \"c\"])\nprint(obj2)\nprint(obj2.index)\n\n# Select, modify array elemtns using index\nobj2[\"a\"]\nobj2[\"d\"] = 6\nobj2[[\"c\", \"a\", \"d\"]]  # using a list of 'indices'-like\n\n# Preservation of index-value link\nprint(obj2[obj2 &gt; 0])\nprint(obj2 * 2)\nprint(np.exp(obj2))\n\n# Series and dictionary, the shared context: mapping of index value to data value\nprint(\"b\" in obj2)\nprint(\"B\" in obj2)\n\n# Convert Python dictionary to/from a Series\ndata_dict = {\n    \"name\": \"John Smith\",\n    \"age\": 30,\n    \"city\": \"New York\",\n    \"email\": \"john.smith@email.com\",\n    \"is_student\": False\n}\nprint(data_dict)\ns_data = pd.Series(data_dict)  # dictionary to Series\nprint(s_data, type(s_data))\nd_data = s_data.to_dict()  # Series to dictionary\nprint(d_data, type(d_data))\n\n# Preservation of index-value\nnew_index = [\"gender\", \"name\", \"age\", \"city\", \"email\"]\nobj4 = pd.Series(data_dict, index=new_index)\nprint(obj4)\n\n# Detecting missing data using pd and Series\nprint(pd.isna(obj4))\nprint(pd.notna(obj4))\nprint(obj4.isna())\nprint(obj4.notna())\n\n0    4\n1    7\n2   -5\n3    3\ndtype: int64\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;PandasArray&gt;\n[4, 7, -5, 3]\nLength: 4, dtype: int64\nRangeIndex(start=0, stop=4, step=1)\nd    4\nb    7\na   -5\nc    3\ndtype: int64\nIndex(['d', 'b', 'a', 'c'], dtype='object')\nd    6\nb    7\nc    3\ndtype: int64\nd    12\nb    14\na   -10\nc     6\ndtype: int64\nd     403.428793\nb    1096.633158\na       0.006738\nc      20.085537\ndtype: float64\nTrue\nFalse\n{'name': 'John Smith', 'age': 30, 'city': 'New York', 'email': 'john.smith@email.com', 'is_student': False}\nname                    John Smith\nage                             30\ncity                      New York\nemail         john.smith@email.com\nis_student                   False\ndtype: object &lt;class 'pandas.core.series.Series'&gt;\n{'name': 'John Smith', 'age': 30, 'city': 'New York', 'email': 'john.smith@email.com', 'is_student': False} &lt;class 'dict'&gt;\ngender                     NaN\nname                John Smith\nage                         30\ncity                  New York\nemail     john.smith@email.com\ndtype: object\ngender     True\nname      False\nage       False\ncity      False\nemail     False\ndtype: bool\ngender    False\nname       True\nage        True\ncity       True\nemail      True\ndtype: bool\ngender     True\nname      False\nage       False\ncity      False\nemail     False\ndtype: bool\ngender    False\nname       True\nage        True\ncity       True\nemail      True\ndtype: bool\nQ: How to assign a name to a pandas Series and customize its index name?\nsdata = {\"Ohio\": 35000, \"Texas\": 71000, \"Oregon\": 16000, \"Utah\": 5000}\nstates = [\"California\", \"Ohio\", \"Oregon\", \"Texas\"]\n\nobj4 = pd.Series(sdata, index=states)\nprint(obj4)\n\nobj4.name = \"population\"\nobj4.index.name = \"state\"\nprint(obj4)\n\nCalifornia        NaN\nOhio          35000.0\nOregon        16000.0\nTexas         71000.0\ndtype: float64\nstate\nCalifornia        NaN\nOhio          35000.0\nOregon        16000.0\nTexas         71000.0\nName: population, dtype: float64\nQ: Change the index of Series in place\n# change the index using index attribute\nobj.index = [\"One\", \"Two\", \"Three\", \"Four\"]\nprint(obj)\n\nOne      4\nTwo      7\nThree   -5\nFour     3\ndtype: int64"
  },
  {
    "objectID": "posts/p4da3d-pandas.html#index-objects",
    "href": "posts/p4da3d-pandas.html#index-objects",
    "title": "[5] Pandas",
    "section": "Index Objects",
    "text": "Index Objects\nIndex objects holds the information about axis labels (labels assigned to rows and columns in DataFrame and Series) and meta data. Index objects are immutable and it makes it safer to share index objects among data structures.\nQ: Construct a Series object and retrieve index information from it.\n\n# Create a Series\nobj = pd.Series(np.arange(3), index=[\"a\", \"b\", \"c\"])  # row labels\nprint(obj, type(obj))\nprint(obj.index)\nprint(obj.index[1:])  # slicing the array of index object\n\na    0\nb    1\nc    2\ndtype: int32 &lt;class 'pandas.core.series.Series'&gt;\nIndex(['a', 'b', 'c'], dtype='object')\nIndex(['b', 'c'], dtype='object')\n\n\n\nQ: Create an Index Object and construct a Series using the object.\n\n# Create an Index object\nlabels = pd.Index(np.arange(3))\nprint(labels, type(labels))\n\n# Create a Series using the index object\nobj2 = pd.Series([1.5, -2.5, 0], index=labels)\nprint(obj2, type(obj2))\n\n# Test equality of the index object\nprint(obj2.index is labels)\nprint(id(obj2.index))\nprint(id(labels))  # refering to the the same object\n\nInt64Index([0, 1, 2], dtype='int64') &lt;class 'pandas.core.indexes.numeric.Int64Index'&gt;\n0    1.5\n1   -2.5\n2    0.0\ndtype: float64 &lt;class 'pandas.core.series.Series'&gt;\nTrue\n1720139551120\n1720139551120\n\n\n\n\nQ: Create a DataFrame and use its index and columns to test Set operation for element existence.\n\n# Data source in a idctionary\npopulations = {\n    \"Ohio\": {2000: 1.5, 2001: 1.7, 2002: 3.6},\n    \"Nevada\": {2001: 2.4, 2002: 2.9}\n}\n\n# Create a DataFrame\nframe3 = pd.DataFrame(populations)\nprint(frame3)\nprint(\"Index: Outer keys become column labels, Inner keys become row labels.\")\nprint(\"-\" * 50)\n\n# Indexes for columns and rows\nprint(frame3.columns, type(frame3.columns))\nprint(frame3.index, type(frame3.index))\nprint(\"-\" * 50)\n\n# Check with set operator\nprint(\"Ohio is in index for columns: \", \"Ohio\" in frame3.columns)\nprint(\"2003 is in index for rows: \", \"2003\" in frame3.index)\n\n      Ohio  Nevada\n2000   1.5     NaN\n2001   1.7     2.4\n2002   3.6     2.9\nIndex: Outer keys become column labels, Inner keys become row labels.\n--------------------------------------------------\nIndex(['Ohio', 'Nevada'], dtype='object') &lt;class 'pandas.core.indexes.base.Index'&gt;\nInt64Index([2000, 2001, 2002], dtype='int64') &lt;class 'pandas.core.indexes.numeric.Int64Index'&gt;\n--------------------------------------------------\nOhio is in index for columns:  True\n2003 is in index for rows:  False\n\n\n\n\nQ: Concatenate with additional Index objects, producing a new index\n\n# Create an initial index\nindex1 = pd.Index([1, 2, 3])\n\n# Create a new index to append\nindex2 = pd.Index([4, 5, 6])\n\n# Use the append() method to concatenate index2 to index1import pandas as pd\n\n# Create two Index objects\nindex1 = pd.Index([1, 2, 3, 4, 5])\nindex2 = pd.Index([3, 4, 5, 6, 7])\n\n# Use the difference() method to find the set difference\nresult = index1.difference(index2)\n\n# Display the elements in index1 that are not in index2\nprint(result)\n\nnew_index = index1.append(index2)\n\n# Display the new concatenated index\nprint(new_index)\n\nInt64Index([1, 2], dtype='int64')\nInt64Index([1, 2, 3, 4, 5, 3, 4, 5, 6, 7], dtype='int64')\n\n\n\n\nQ: Compute set difference, intersection and union as an index between two index objects\n\n# Create two Index objects\nindex1 = pd.Index([1, 2, 3, 4, 5])\nindex2 = pd.Index([3, 4, 5, 6, 7])\n\n# Use the difference() method to find the set difference\nresult = index1.difference(index2)\nprint(result)\n\n# intersection\nresult = index1.intersection(index2)\nprint(result)\n\n# union\nresult = index1.union(index2)\nprint(result)\n\nInt64Index([1, 2], dtype='int64')\nInt64Index([3, 4, 5], dtype='int64')\nInt64Index([1, 2, 3, 4, 5, 6, 7], dtype='int64')\n\n\n\n\nQ: Compute Boolean array indicating whether each value is contained in the passed collection\n\n# Create an Index object\n\nindex = pd.Index([1, 2, 3, 4, 5])\n\n# Create a collection of values to check for containment\n\ncollection = [3, 5, 7]\n\n# Use the isin() method to check for containment\n\nis_in_collection = index.isin(collection)\n\n# Display the Boolean array for each element's containment\n\nprint(is_in_collection)\n\n[False False  True False  True]\n\n\n\n\nQuestion: Create a new Index object by deleting passed lables from the index.\n\n# Create an Index\nindex = pd.Index(['A', 'B', 'C', 'D', 'E'])\nprint(\"original Index obj: \", index)\n\n# Drop specific labels from the Index\nlabels_to_drop = ['B', 'C']\nnew_index = index.drop(labels_to_drop)\nprint(\"new Index obj: \", new_index)\n\noriginal Index obj:  Index(['A', 'B', 'C', 'D', 'E'], dtype='object')\nnew Index obj:  Index(['A', 'D', 'E'], dtype='object')\n\n\n\n\nQuestion: Create a new Index object by inserting new labels to an existing Index object, after the element matching with the first letter of the new label\n\n# Create a Index object\noriginal_labels = ['A', 'B', 'C', 'D', 'E']\nindex = pd.Index(original_labels)\nprint(\"Original index: \", index)\n\n# additional labels to be inserted\nadditional_labels = ['C-3', 'C-5', 'C-1', 'C-2', 'C-4']\nprint(\"Labels to insert: \", additional_labels)\n\n# insert the labels\nnew_index = index\nadditional_labels.sort(reverse=True)\n# print(\"reverse-sorted additional labels: \", additional_labels)\n\nfor label in additional_labels:\n    first_letter = label[:1]\n\n    if (first_letter in original_labels):\n        # find the index of the first letter\n        idx = original_labels.index(first_letter)\n        # print(first_letter, label, idx)\n        new_index = new_index.insert(idx+1, label)\n\nprint(\"New index: \", new_index)\n\nOriginal index:  Index(['A', 'B', 'C', 'D', 'E'], dtype='object')\nLabels to insert:  ['C-3', 'C-5', 'C-1', 'C-2', 'C-4']\nNew index:  Index(['A', 'B', 'C', 'C-1', 'C-2', 'C-3', 'C-4', 'C-5', 'D', 'E'], dtype='object')\n\n\n\n\nQuestion: You have a Pandas Index object, and you want to check if it is monotonically decreasing. Write a Python function that takes an Index object as input and returns True if the index is monotonically decreasing, and False otherwise.\n\n# Function to check if an Index is monotonically  decreasing\ndef is_monotonic_decreasing(index):\n    return index.is_monotonic_decreasing\n  \n# indexes to test\nindex1 = pd.Index([5, 4, 3, 2, 1])\nindex2 = pd.Index(['E', 'D', 'C', 'B', 'A'])\nindex3 = pd.Index([4, 3, 2, 'D', 'C', 'B'])\n\nlist_of_index = [index1, index2, index3]\nfor index in list_of_index:\n    print(index, is_monotonic_decreasing(index))\n\nInt64Index([5, 4, 3, 2, 1], dtype='int64') True\nIndex(['E', 'D', 'C', 'B', 'A'], dtype='object') True\nIndex([4, 3, 2, 'D', 'C', 'B'], dtype='object') False\n\n\n\n\nQuestion: You have a Pandas Index object, and you want to check if it contains unique elements. Write a Python function that takes an Index object as input and returns True if all the elements are unique, and False otherwise.\n\n# Create indexes for testing\nindex1 = pd.Index(['A', 'B', 'C', 'D', 'E'])\nindex2 = pd.Index(['A', 'B', 'C', 'A', 'D'])\n\n# Function to check if an Index contatins unique elements\ndef is_unique(index):\n    return index.is_unique\n\n# Test the indexes\nprint(index1, is_unique(index1))\nprint(index2, is_unique(index2))\n\nIndex(['A', 'B', 'C', 'D', 'E'], dtype='object') True\nIndex(['A', 'B', 'C', 'A', 'D'], dtype='object') False\n\n\n\n\nYou have a Pandas Index object, and you want to retrieve the unique elements from it. Write a Python function that takes an Index object as input and returns a list of unique elements from the index.\n\nindex = pd.Index(['A', 'B', 'C', 'A', 'D', 'B'])\n\ndef get_unique_elements(index):\n    return index.unique()\n\nunique_elements = get_unique_elements(index)\nprint(\"Unique elements: \", unique_elements)  # Index object\nprint(unique_elements.tolist())  # List object\n\nUnique elements:  Index(['A', 'B', 'C', 'D'], dtype='object')\n['A', 'B', 'C', 'D']"
  },
  {
    "objectID": "posts/p4da3d-pandas.html#the-mechanics-of-interacting-with-data-in-series-and-dataframe",
    "href": "posts/p4da3d-pandas.html#the-mechanics-of-interacting-with-data-in-series-and-dataframe",
    "title": "[5] Pandas",
    "section": "The mechanics of interacting with data in Series and DataFrame",
    "text": "The mechanics of interacting with data in Series and DataFrame\n\nQuestion: You’re working with a dataset of daily stock prices for a portfolio of companies. The dataset has missing values for certain days, and you want to reindex it to ensure you have a complete set of daily data for a specified date range. You need to create a Python function that takes several arguments to demonstrate the use of the reindex() method and its optional parameters. Your task is to implement the reindex_stock_data() function, which demonstrates the use of these parameters. The function should reindex the stock price data to the specified date range, filling missing data with the provided fill_value and considering the other optional parameters.\n\ndata: A DataFrame containing stock price data with a date index.\nstart_date: The start date of the desired date range.\nend_date: The end date of the desired date range.\nfill_value: The value to fill missing data points.\nmethod: A method to use for filling missing values (e.g., ‘ffill’, ‘bfill’).\nlimit: The maximum number of consecutive NaN values to fill.\ntolerance: The maximum allowed difference in the index value when reindexing.\nlevel: The level in case of MultiIndex.\ncopy: Whether to create a copy of the original data or reindex in-place.\n\n\n# Function to reindex data as the parameters instruct\ndef reindex_stock_data(\n    data,\n    start_date,\n    end_date,\n    fill_value,\n    method=None,\n    limit=None,\n    tolerance=None,\n    level=None,\n    copy=False\n):\n    # print(\"fill_value passed in: \" + str(fill_value))\n    # Fill NaN values with the specified fill_value\n    data_filled = data.fillna(fill_value)\n    # print(\"data_filled:\\n\", data_filled)\n\n    # Generate fixed-frequency Datetime index - index of dates and times\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n    # Reindex the filled data\n    result = data_filled.reindex(date_range, method=method, tolerance=tolerance,\n                                 limit=limit, fill_value=fill_value, level=level, copy=copy)\n    return result\n\n# Create an incomplete initial data for testing\ndata = pd.DataFrame({\n    'AAPL': [150, 151, np.nan, 153, np.nan],\n    'GOOGL': [2000, np.nan, 2020, 2030, 2040],\n    'MSFT': [300, np.nan, 302, np.nan, 304],\n    'AMZN': [3500, 3510, np.nan, np.nan, 3540],\n    'TSLA': [np.nan, 701, 702, 703, np.nan]\n}, index=pd.date_range(start='2022-01-01', periods=5, freq='D'))\nprint(\"Original data:\\n\", data)\n\n# Set parameters and values for the function - reindexing\nstart_date = '2022-01-01'\nend_date = '2022-01-05'\nfill_value = 0\nmethod = 'bfill'  # Backward fill\nlimit = 1\ntolerance = '1D'  # Tolerance of 1 day\nlevel = None\ncopy = True\n\n# print(\"fill_value is: \", str(fill_value))\n# Call the function and reindex the data\nresult = reindex_stock_data(\n    data, start_date, end_date, fill_value, method, limit, tolerance, level, copy)\nprint(\"Result re-indexed data:\\n\", result)\n\nOriginal data:\n              AAPL   GOOGL   MSFT    AMZN   TSLA\n2022-01-01  150.0  2000.0  300.0  3500.0    NaN\n2022-01-02  151.0     NaN    NaN  3510.0  701.0\n2022-01-03    NaN  2020.0  302.0     NaN  702.0\n2022-01-04  153.0  2030.0    NaN     NaN  703.0\n2022-01-05    NaN  2040.0  304.0  3540.0    NaN\nResult re-indexed data:\n              AAPL   GOOGL   MSFT    AMZN   TSLA\n2022-01-01  150.0  2000.0  300.0  3500.0    0.0\n2022-01-02  151.0     0.0    0.0  3510.0  701.0\n2022-01-03    0.0  2020.0  302.0     0.0  702.0\n2022-01-04  153.0  2030.0    0.0     0.0  703.0\n2022-01-05    0.0  2040.0  304.0  3540.0    0.0"
  },
  {
    "objectID": "posts/p4da3d-pandas.html#question-create-a-dataframe-called-sales_data-that-contains-sales-data-for-various-products-in-different-stores.-use-the-sales_data-and-create-dataframes-below.",
    "href": "posts/p4da3d-pandas.html#question-create-a-dataframe-called-sales_data-that-contains-sales-data-for-various-products-in-different-stores.-use-the-sales_data-and-create-dataframes-below.",
    "title": "[5] Pandas",
    "section": "Question: Create a DataFrame called ‘sales_data’ that contains sales data for various products in different stores. Use the ‘sales_data’ and create DataFrames below.",
    "text": "Question: Create a DataFrame called ‘sales_data’ that contains sales data for various products in different stores. Use the ‘sales_data’ and create DataFrames below.\n\nimport pandas as pd\n\n# Create a dictionary of store sales data and a DataFrmae\ndata = {\n    'Store': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\n    'Products': ['X', 'Y', 'Z', 'X', 'Y', 'Z', 'X', 'Y', 'Z'],\n    'Sales': [1000, 1500, 1200, 2000, 1800, 2200, 1200, 1400, 1100]\n}\n\nsales_data = pd.DataFrame(data)\nprint(sales_data)\n\n  Store Products  Sales\n0     A        X   1000\n1     B        Y   1500\n2     C        Z   1200\n3     A        X   2000\n4     B        Y   1800\n5     C        Z   2200\n6     A        X   1200\n7     B        Y   1400\n8     C        Z   1100\n\n\n\n# Create a DataFrame called 'store_A_sales', \n# by selecting all the products sold in Store A\n\ncond = sales_data['Store'] == 'A'\nstore_A_sales = sales_data[cond]\nprint(store_A_sales)\n\n  Store Products  Sales\n0     A        X   1000\n3     A        X   2000\n6     A        X   1200\n\n\nCreate a DataFrame called ‘high_sales_products’ for rows where ‘Sales’ is greater than or equal to 1500. Use the DataFrame ‘sales_data’. Display only ‘Products’ and ‘Sales’ columns.\n\n# Create a DataFrame indexing with Boolean values meeting the sales condition\nhigh_sales_products = sales_data.loc[sales_data['Sales'] &gt;= 1500, ['Products', 'Sales']]\nprint(\"High sales products from store 'A'\\n\", high_sales_products)\n\nHigh sales products from store 'A'\n   Products  Sales\n1        Y   1500\n3        X   2000\n4        Y   1800\n5        Z   2200\n\n\n\nDropping Entries from an Axis\nCreate a Series for the questions.\n\nimport pandas as pd\nimport numpy as np\n\nobj = pd.Series(np.arange(5.), index=list(\"abcde\"))\nprint(obj)\n\na    0.0\nb    1.0\nc    2.0\nd    3.0\ne    4.0\ndtype: float64\n\n\n\nQuestion: Drop single or multiple indexes - ‘c’ or ‘c’ and ‘d’ both from the Series.\n\nnew_obj = obj.drop('c')\nprint(new_obj)\nnew_obj = obj.drop(['c','d'])\nprint(new_obj)\n\na    0.0\nb    1.0\nd    3.0\ne    4.0\ndtype: float64\na    0.0\nb    1.0\ne    4.0\ndtype: float64\n\n\n\n\nIndexing and retrieving from DataFrame with a single value of sequence\n\nimport pandas as pd\nimport numpy as np\n\n# Create a DataFrame\ndata = pd.DataFrame(np.arange(16).reshape((4, 4)),\n                    index=[\"Ohio\", \"Colorado\", \"Utah\", \"New York\"],\n                    columns=[\"one\", \"two\", \"three\", \"four\"]\n)\nprint(data)\n\n# Drop 'Colorado' and 'Ohio', with a sequence of labels from the row labels (axis=0)\ndata2 = data.drop(index=['Colorado', 'Ohio'])\nprint(data2)\n\n# Drop the column 'two'\ndata2 = data.drop(columns = 'two')\nprint(data2)\n\n# Drop column(s) using axis parameter\ndata2 = data.drop(\"one\", axis=1)\nprint(data2)\ndata2 = data.drop([\"two\", \"four\"], axis=1)\nprint(data2)\n\n          one  two  three  four\nOhio        0    1      2     3\nColorado    4    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\n          one  two  three  four\nUtah        8    9     10    11\nNew York   12   13     14    15\n          one  three  four\nOhio        0      2     3\nColorado    4      6     7\nUtah        8     10    11\nNew York   12     14    15\n          two  three  four\nOhio        1      2     3\nColorado    5      6     7\nUtah        9     10    11\nNew York   13     14    15\n          one  three\nOhio        0      2\nColorado    4      6\nUtah        8     10\nNew York   12     14\n\n\n\n\nIndexing, Selection, and Filtering\n\nimport pandas as pd\nimport numpy as np\n\n# Create a Seriese\nobj = pd.Series(np.arange(4.), index=[\"a\",\"b\",\"c\",\"d\"])\nprint(obj)\n\n# Select using a label\nprint(obj[\"b\"])\n\n# Select using a integer index (like NumPy)\nprint(obj[1])\n\n# Select using a slice of integers (like NumPy)\nprint(obj[2:4])\n\n# Select using a sequence of labels\nprint(obj[[\"b\", \"a\" , \"d\"]])\n\n# Select using a list of integers\nprint(obj[[1, 3]])\n\n# Select using an array of Boolean values, condition\ncond = obj &lt; 2\nprint(obj[cond]) # equivalent to obj[[obj &lt; 2]]\n\n# Create Serieses\nobj1 = pd.Series([1, 2, 3], index=[2, 0, 1])\nobj2 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\nprint(\"obj1\\n:\", obj1)\nprint(\"obj2\\n:\", obj2)\n\n# Prefer to use loc when selecting using labels\nprint(obj2.loc[[\"b\", \"a\", \"c\"]])\n#print(obj2.loc[[0, 1, 2]]) # error, not all indexes have matching labels\nprint(obj1.loc[[0, 1, 2]])\n\n# Use iloc for indexing with integers\nprint(obj1.iloc[[0, 1, 2]])\nprint(obj2.iloc[[0, 1, 2]]) # iloc always work with integer indexing\n\n# Slicing inclusive of endpoint\nprint(obj2.loc[\"b\":\"c\"]) # \"c\" label included for selection\nobj2.loc[\"b\":\"c\"] = 99 # modify values using label slicing\nprint(obj2)\n\na    0.0\nb    1.0\nc    2.0\nd    3.0\ndtype: float64\n1.0\n1.0\nc    2.0\nd    3.0\ndtype: float64\nb    1.0\na    0.0\nd    3.0\ndtype: float64\nb    1.0\nd    3.0\ndtype: float64\na    0.0\nb    1.0\ndtype: float64\nobj1\n: 2    1\n0    2\n1    3\ndtype: int64\nobj2\n: a    1\nb    2\nc    3\ndtype: int64\nb    2\na    1\nc    3\ndtype: int64\n0    2\n1    3\n2    1\ndtype: int64\n2    1\n0    2\n1    3\ndtype: int64\na    1\nb    2\nc    3\ndtype: int64\nb    2\nc    3\ndtype: int64\na     1\nb    99\nc    99\ndtype: int64\n\n\n\n\nIndexing into a DataFrame retriving one or more columns with single value or sequence\n\nimport pandas as pd\nimport numpy as np\n\n# Create some state index and columns with random integer value, 4x4\ndata = pd.DataFrame(np.arange(16).reshape(4, 4), \n                    index=[\"Ohio\", \"Colorado\", \"Utah\", \"New York\"],\n                    columns=[\"one\", \"two\", \"three\", \"four\"]\n)\nprint(data)\nprint(data.index)\nprint(data.columns)\n\n# Select with a single or sequence\nprint(data[\"one\"]) # single column \"one\"\nprint(data[[\"three\", \"two\"]]) # multiple columns - three, \n\n# Select with slicing\nprint(data[:2])  # select row index at 0, 1\n\n# Select / assign using Boolean array\ncond = data[\"three\"] &gt; 5\nprint(\"cond:\\n\", cond, type(cond)) # return a Series\nprint(data[cond])\ncond = data &lt; 5\nprint(\"cond\\n\", cond, type(cond)) # return a DF\nprint(data[cond])\n\n          one  two  three  four\nOhio        0    1      2     3\nColorado    4    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\nIndex(['Ohio', 'Colorado', 'Utah', 'New York'], dtype='object')\nIndex(['one', 'two', 'three', 'four'], dtype='object')\nOhio         0\nColorado     4\nUtah         8\nNew York    12\nName: one, dtype: int32\n          three  two\nOhio          2    1\nColorado      6    5\nUtah         10    9\nNew York     14   13\n          one  two  three  four\nOhio        0    1      2     3\nColorado    4    5      6     7\ncond:\n Ohio        False\nColorado     True\nUtah         True\nNew York     True\nName: three, dtype: bool &lt;class 'pandas.core.series.Series'&gt;\n          one  two  three  four\nColorado    4    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\ncond\n             one    two  three   four\nOhio       True   True   True   True\nColorado   True  False  False  False\nUtah      False  False  False  False\nNew York  False  False  False  False &lt;class 'pandas.core.frame.DataFrame'&gt;\n          one  two  three  four\nOhio      0.0  1.0    2.0   3.0\nColorado  4.0  NaN    NaN   NaN\nUtah      NaN  NaN    NaN   NaN\nNew York  NaN  NaN    NaN   NaN\n\n\n\n\nSelection on DataFrame with loc and iloc\n\nprint(data)\n\n# loc with labels\nprint(data.loc[\"Colorado\"]) # Series\nprint(data.loc[[\"Colorado\", \"New York\"]]) # DF\nsel = data.loc[\"Colorado\", [\"two\", \"three\"]] # Series\nprint(sel)\nprint(type(sel))\n\n# iloc with integers\nprint(data.iloc[2]) # Series\nprint(data.iloc[[2, 1]]) # two rows, DF\nprint(data.iloc[2, [2, 1]]) # row + columns, Series\nprint(data.iloc[[1, 2], [3, 0, 1]]) # rows + columns, DF\n\n# Slicing\nprint(data.loc[:\"Utah\", \"two\"]) # slicing using row and colum labels at once\nprint(data.iloc[:, :3][data.three &gt;= 2])\n\n          one  two  three  four\nOhio        0    1      2     3\nColorado    4    5      6     7\nUtah        8    9     10    11\nNew York   12   13     14    15\none      4\ntwo      5\nthree    6\nfour     7\nName: Colorado, dtype: int32\n          one  two  three  four\nColorado    4    5      6     7\nNew York   12   13     14    15\ntwo      5\nthree    6\nName: Colorado, dtype: int32\n&lt;class 'pandas.core.series.Series'&gt;\none       8\ntwo       9\nthree    10\nfour     11\nName: Utah, dtype: int32\n          one  two  three  four\nUtah        8    9     10    11\nColorado    4    5      6     7\nthree    10\ntwo       9\nName: Utah, dtype: int32\n          four  one  two\nColorado     7    4    5\nUtah        11    8    9\nOhio        1\nColorado    5\nUtah        9\nName: two, dtype: int32\n          one  two  three\nOhio        0    1      2\nColorado    4    5      6\nUtah        8    9     10\nNew York   12   13     14\n\n\n\n\nArithmetic and Data Alignment with Series and DataFrame\n\nimport pandas as pd\nimport numpy as np\n\n# Create two Serieses with different number of data and labels\ns1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=[\"a\", \"c\", \"d\", \"e\"])\ns2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=[\"a\", \"c\", \"e\", \"f\", \"g\"])\nprint(\"s1: \", s1)\nprint(\"s2: \", s2)\n\n# s1 + s2, result with the union of the index pairs and data alignment\nprint(s1 + s2)\n\n# Create two DataFrames - 3x3 / 4x3\ndf1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=list(\"bcd\"), index=[\"Ohio\", \"Texas\", \"Colorado\"])\ndf2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list(\"bde\"), index=[\"Utah\", \"Ohio\", \"Texas\", \"Oregon\"])\nprint(df1)\nprint(df2)\n\n# df1 + df2 , Union of index pairs and data alignment\nprint(df1+df2)\n\n# Case of DataFrames with no rows or columns in common\ndf1 = pd.DataFrame({\"A\": [1, 2]})\ndf2 = pd.DataFrame({\"B\": [3, 4]})\nprint(df1)\nprint(df2)\nprint(df1+df2)\n\n# Practice to fill missing value(NaN, null) with sepcific value during arithmetic operations\n\n# Create DataFrames with different shapes, with data not in common and null\ndf1 = pd.DataFrame(np.arange(12.).reshape((3, 4)), columns=list(\"abcd\"))\ndf2 = pd.DataFrame(np.arange(20.).reshape((4, 5)), columns=list(\"abcde\"))\n\n# Set null for the value at 1, \"b\" in df2\ndf2.loc[1, \"b\"] = np.nan\n\nprint(\"df1:\\n\", df1)\nprint(\"df2:\\n\", df2)\n\n# addtion and missing values\nprint(\"df1+df2:\\n\", df1 + df2)\n\n# add() method and fill_value argument to substitute missing values during arithmetic operations.\nprint(df1.add(df2, fill_value=0))\n\ns1:  a    7.3\nc   -2.5\nd    3.4\ne    1.5\ndtype: float64\ns2:  a   -2.1\nc    3.6\ne   -1.5\nf    4.0\ng    3.1\ndtype: float64\na    5.2\nc    1.1\nd    NaN\ne    0.0\nf    NaN\ng    NaN\ndtype: float64\n            b    c    d\nOhio      0.0  1.0  2.0\nTexas     3.0  4.0  5.0\nColorado  6.0  7.0  8.0\n          b     d     e\nUtah    0.0   1.0   2.0\nOhio    3.0   4.0   5.0\nTexas   6.0   7.0   8.0\nOregon  9.0  10.0  11.0\n            b   c     d   e\nColorado  NaN NaN   NaN NaN\nOhio      3.0 NaN   6.0 NaN\nOregon    NaN NaN   NaN NaN\nTexas     9.0 NaN  12.0 NaN\nUtah      NaN NaN   NaN NaN\n   A\n0  1\n1  2\n   B\n0  3\n1  4\n    A   B\n0 NaN NaN\n1 NaN NaN\ndf1:\n      a    b     c     d\n0  0.0  1.0   2.0   3.0\n1  4.0  5.0   6.0   7.0\n2  8.0  9.0  10.0  11.0\ndf2:\n       a     b     c     d     e\n0   0.0   1.0   2.0   3.0   4.0\n1   5.0   NaN   7.0   8.0   9.0\n2  10.0  11.0  12.0  13.0  14.0\n3  15.0  16.0  17.0  18.0  19.0\ndf1+df2:\n       a     b     c     d   e\n0   0.0   2.0   4.0   6.0 NaN\n1   9.0   NaN  13.0  15.0 NaN\n2  18.0  20.0  22.0  24.0 NaN\n3   NaN   NaN   NaN   NaN NaN\n      a     b     c     d     e\n0   0.0   2.0   4.0   6.0   4.0\n1   9.0   5.0  13.0  15.0   9.0\n2  18.0  20.0  22.0  24.0  14.0\n3  15.0  16.0  17.0  18.0  19.0\n\n\n\n\nSeries and DataFrame method for arithmetic\n\nprint(df1)\nprint(1 / df1) # division\nprint(df1.rdiv(1)) # same as above\n\n     a    b     c     d\n0  0.0  1.0   2.0   3.0\n1  4.0  5.0   6.0   7.0\n2  8.0  9.0  10.0  11.0\n       a         b         c         d\n0    inf  1.000000  0.500000  0.333333\n1  0.250  0.200000  0.166667  0.142857\n2  0.125  0.111111  0.100000  0.090909\n       a         b         c         d\n0    inf  1.000000  0.500000  0.333333\n1  0.250  0.200000  0.166667  0.142857\n2  0.125  0.111111  0.100000  0.090909\n\n\n\n\nQuestion: Change column labels of df1 with df2, and fill null values with 0.\n\nprint(df1)\nprint(df2)\n# reindexing the columns with df2, filling 0 for not-in-commons\ndf1.reindex(columns=df2.columns, fill_value=0)\n\n     a    b     c     d\n0  0.0  1.0   2.0   3.0\n1  4.0  5.0   6.0   7.0\n2  8.0  9.0  10.0  11.0\n      a     b     c     d     e\n0   0.0   1.0   2.0   3.0   4.0\n1   5.0   NaN   7.0   8.0   9.0\n2  10.0  11.0  12.0  13.0  14.0\n3  15.0  16.0  17.0  18.0  19.0\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0.0\n1.0\n2.0\n3.0\n0\n\n\n1\n4.0\n5.0\n6.0\n7.0\n0\n\n\n2\n8.0\n9.0\n10.0\n11.0\n0\n\n\n\n\n\n\n\n\n\nOperations between DataFrame and Series - Broadcasting\n\nimport pandas as pd\nimport numpy as np\n\n# Operation between N-d array and 1-d array\n\n# Create an 3x4 array\narr = np.arange(12.).reshape((3,4))\nprint(arr, type(arr))\nprint(arr[0], type(arr[0])) # 1-d array\n\n# N-d operator 1-d = broadcasting to each row of N-d\nprint(arr - arr[0])\n\n# Operation between DataFrame and Series, similar\n\n# Create a 4x3 DataFrame\nframe = pd.DataFrame(np.arange(12.).reshape((4, 3)),\n                     columns=list(\"bde\"),\n                     index=[\"Utah\", \"Ohio\", \"Texas\", \"Oregon\"])\nprint(\"frame:\\n\", frame, type(frame))\nseries = frame.iloc[0] # first row\nprint(\"series:\\n\", series, type(series))\n# frame - series\nprint(\"frame-series:\\n\", frame-series) # both with matching indexes\n\n# Create another Series without matching indexes\nseries2 = pd.Series(np.arange(3), index=list(\"bef\"))\nprint(series2)\n# frame - series2\nprint(\"frame-serires2:\\n\", frame-series2) # result reindexed to form the union, and do operations for the matching indexes\n\n[[ 0.  1.  2.  3.]\n [ 4.  5.  6.  7.]\n [ 8.  9. 10. 11.]] &lt;class 'numpy.ndarray'&gt;\n[0. 1. 2. 3.] &lt;class 'numpy.ndarray'&gt;\n[[0. 0. 0. 0.]\n [4. 4. 4. 4.]\n [8. 8. 8. 8.]]\nframe:\n           b     d     e\nUtah    0.0   1.0   2.0\nOhio    3.0   4.0   5.0\nTexas   6.0   7.0   8.0\nOregon  9.0  10.0  11.0 &lt;class 'pandas.core.frame.DataFrame'&gt;\nseries:\n b    0.0\nd    1.0\ne    2.0\nName: Utah, dtype: float64 &lt;class 'pandas.core.series.Series'&gt;\nframe-series:\n           b    d    e\nUtah    0.0  0.0  0.0\nOhio    3.0  3.0  3.0\nTexas   6.0  6.0  6.0\nOregon  9.0  9.0  9.0\nb    0\ne    1\nf    2\ndtype: int32\nframe-serires2:\n           b   d     e   f\nUtah    0.0 NaN   1.0 NaN\nOhio    3.0 NaN   4.0 NaN\nTexas   6.0 NaN   7.0 NaN\nOregon  9.0 NaN  10.0 NaN\n\n\n\n# broadcast over columns\nseries3 = frame[\"d\"]\nprint(frame)\nprint(series3)\nframe.sub(series3, axis=\"index\") # over columns\n\n          b     d     e\nUtah    0.0   1.0   2.0\nOhio    3.0   4.0   5.0\nTexas   6.0   7.0   8.0\nOregon  9.0  10.0  11.0\nUtah       1.0\nOhio       4.0\nTexas      7.0\nOregon    10.0\nName: d, dtype: float64\n\n\n\n\n\n\n\n\n\nb\nd\ne\n\n\n\n\nUtah\n-1.0\n0.0\n1.0\n\n\nOhio\n-1.0\n0.0\n1.0\n\n\nTexas\n-1.0\n0.0\n1.0\n\n\nOregon\n-1.0\n0.0\n1.0\n\n\n\n\n\n\n\n\n\nFunction Application and Mapping\nNumpy ufuncs(element-wise array methods) also work with pandas objects\n\nimport pandas as pd\nimport numpy as np\n\n# Create a 4x3 random standard normal DataFrame\nframe = pd.DataFrame(np.random.standard_normal((4,3)), columns=list(\"bed\"), index=[\"Utah\", \"Ohio\", \"Texas\", \"Oregon\"])\nprint(frame,\"\\n\"+\"-\"*40)\nprint(np.abs(frame,),\"\\n\"+\"-\"*40)\n\n               b         e         d\nUtah    1.818187 -0.098990 -0.681573\nOhio   -0.702556 -1.159056 -0.533471\nTexas   0.148275 -1.556694  0.963959\nOregon  0.797847  1.006360 -1.860530 \n----------------------------------------\n               b         e         d\nUtah    1.818187  0.098990  0.681573\nOhio    0.702556  1.159056  0.533471\nTexas   0.148275  1.556694  0.963959\nOregon  0.797847  1.006360  1.860530 \n----------------------------------------\n\n\n\n# apply() to apply a function to each column / row\n\n# Function to max - min of Series\ndef f1(x):\n  return x.max() - x.min()\n\n\nframe.apply(f1) # apply f1 function to each column of DF\nprint(\"-\"*40)\nframe.apply(f1,  axis=\"columns\") # apply f1 function to each row - across the columns\n\n----------------------------------------\n\n\nUtah      2.499760\nOhio      0.625585\nTexas     2.520652\nOregon    2.866890\ndtype: float64\n\n\napply() can return a Series\n\n# apply() can return a Series\n\n# Create a function to return a Series\ndef f2(x):\n  return pd.Series([x.min(), x.max()], index=[\"min\", \"max\"])\nresult = frame.apply(f2)\nprint(result, type(result)) # return a df with multiple Series from f2\n\n            b         e         d\nmin -0.702556 -1.556694 -1.860530\nmax  1.818187  1.006360  0.963959 &lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nElement-wise Python functions with DataFrame\n\n# Create a function to return a formatted string\ndef my_format(x):\n  return f\"{x:.2f}\"\nframe.applymap(my_format)  # applymap() for DF\nprint(\"-\"*40)\nframe[\"e\"].map(my_format) # map() for Series\n\n----------------------------------------\n\n\nUtah      -0.10\nOhio      -1.16\nTexas     -1.56\nOregon     1.01\nName: e, dtype: object\n\n\n\nSorting and Ranking\nsort_index with a Series and DataFrame\n\n# Create a Series\nobj = pd.Series(np.arange(4), index=list(\"dabc\"))\nobj\nprint(\"-\"*40)\nobj.sort_index()\nprint(\"-\"*40)\nobj.sort_values()\n\n# Create a 2x4 DataFrame\nframe = pd.DataFrame(np.arange(8).reshape((2,4)),\n                     index=[\"three\", \"one\"],\n                     columns=list(\"dabc\"))\nframe\nprint(\"-\"*40)\nframe.sort_index()\nprint(\"-\"*40)\nframe.sort_index(axis=\"columns\", ascending=False)\n\n----------------------------------------\n----------------------------------------\n----------------------------------------\n----------------------------------------\n\n\n\n\n\n\n\n\n\nd\nc\nb\na\n\n\n\n\nthree\n0\n3\n2\n1\n\n\none\n4\n7\n6\n5"
  },
  {
    "objectID": "posts/analysis_movielens.html",
    "href": "posts/analysis_movielens.html",
    "title": "MovieLens 1M Dataset",
    "section": "",
    "text": "MovieLens 1M Dataset\nA number of collections of movie ratings data collected from users of MovieLens in the late 1990s and early 2000s.\nMovie metadata\nrecommendation systems\nMachine learning algorithms\nData slice and dice to processing need\nOne million ratings from 6000 users on 4000 movies\nThree tables: ratings, user information, movie information\n\n\nRead data files into pandas DataFrame\n\n# Helper function to print object, type and divider line together\ndef print_tl(x=None):\n  if x is None:\n    print(\"\\n -----------------------------\")\n  else:\n    print(x, \"\\n\", type(x), \"\\n -----------------------------\")\n\n\nimport pandas as pd\nimport numpy as np\n\n# Read 3 table data into Df, with custom column names\n\n# - users table data\nunames = [\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"] \nusers = pd.read_table(\"../data/movielens/users.dat\", sep=\"::\",\n                      header=None, names=unames, engine=\"python\")\n                    \n# - ratings table data\nrnames = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"] \nratings = pd.read_table(\"../data/movielens/ratings.dat\", sep=\"::\",\n                      header=None, names=rnames, engine=\"python\")\n                      \n# - movies table data\nmnames = [\"movie_id\", \"title\", \"genres\"] \nmovies = pd.read_table(\"../data/movielens/movies.dat\", sep=\"::\",\n                      header=None, names=mnames, engine=\"python\")\n\n\n# Check loaded data\nusers.head(5)\nratings.head(5)\nmovies.head(5)\nratings # 1M+ rows\n\n\n\n\n\n\n\n\nuser_id\nmovie_id\nrating\ntimestamp\n\n\n\n\n0\n1\n1193\n5\n978300760\n\n\n1\n1\n661\n3\n978302109\n\n\n2\n1\n914\n3\n978301968\n\n\n3\n1\n3408\n4\n978300275\n\n\n4\n1\n2355\n5\n978824291\n\n\n...\n...\n...\n...\n...\n\n\n1000204\n6040\n1091\n1\n956716541\n\n\n1000205\n6040\n1094\n5\n956704887\n\n\n1000206\n6040\n562\n5\n956704746\n\n\n1000207\n6040\n1096\n4\n956715648\n\n\n1000208\n6040\n1097\n4\n956715569\n\n\n\n\n1000209 rows × 4 columns\n\n\n\n\n\nMerge data files into one DataFrame\n\n# Merge three data files: (raings + users) + movies\ndata = pd.merge(pd.merge(ratings, users), movies)\ndata.info()\nprint_tl()\nprint_tl(data)\nprint_tl(data.iloc[0])\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1000209 entries, 0 to 1000208\nData columns (total 10 columns):\n #   Column      Non-Null Count    Dtype \n---  ------      --------------    ----- \n 0   user_id     1000209 non-null  int64 \n 1   movie_id    1000209 non-null  int64 \n 2   rating      1000209 non-null  int64 \n 3   timestamp   1000209 non-null  int64 \n 4   gender      1000209 non-null  object\n 5   age         1000209 non-null  int64 \n 6   occupation  1000209 non-null  int64 \n 7   zip         1000209 non-null  object\n 8   title       1000209 non-null  object\n 9   genres      1000209 non-null  object\ndtypes: int64(6), object(4)\nmemory usage: 83.9+ MB\n\n -----------------------------\n         user_id  movie_id  rating  timestamp gender  age  occupation    zip  \\\n0              1      1193       5  978300760      F    1          10  48067   \n1              2      1193       5  978298413      M   56          16  70072   \n2             12      1193       4  978220179      M   25          12  32793   \n3             15      1193       4  978199279      M   25           7  22903   \n4             17      1193       5  978158471      M   50           1  95350   \n...          ...       ...     ...        ...    ...  ...         ...    ...   \n1000204     5949      2198       5  958846401      M   18          17  47901   \n1000205     5675      2703       3  976029116      M   35          14  30030   \n1000206     5780      2845       1  958153068      M   18          17  92886   \n1000207     5851      3607       5  957756608      F   18          20  55410   \n1000208     5938      2909       4  957273353      M   25           1  35401   \n\n                                               title                genres  \n0             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n1             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n2             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n3             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n4             One Flew Over the Cuckoo's Nest (1975)                 Drama  \n...                                              ...                   ...  \n1000204                           Modulations (1998)           Documentary  \n1000205                        Broken Vessels (1998)                 Drama  \n1000206                            White Boys (1999)                 Drama  \n1000207                     One Little Indian (1973)  Comedy|Drama|Western  \n1000208  Five Wives, Three Secretaries and Me (1998)           Documentary  \n\n[1000209 rows x 10 columns] \n &lt;class 'pandas.core.frame.DataFrame'&gt; \n -----------------------------\nuser_id                                            1\nmovie_id                                        1193\nrating                                             5\ntimestamp                                  978300760\ngender                                             F\nage                                                1\noccupation                                        10\nzip                                            48067\ntitle         One Flew Over the Cuckoo's Nest (1975)\ngenres                                         Drama\nName: 0, dtype: object \n &lt;class 'pandas.core.series.Series'&gt; \n -----------------------------\n\n\n\n\nAnalysis: Get mean movie ratings for each film grouped by gender\n\n# Required fields for grouping\nprint_tl(data.info())\nprint_tl(data[\"rating\"].head(5))\nprint_tl(data[\"gender\"].head(5))\nprint_tl(data[\"movie_id\"].head(5))\n\n# df.pivot_table: gender to columns and movie title for index, mean on rating for aggregate function, spread-sheet style pivot table\nmean_ratings = data.pivot_table(\"rating\", index=\"title\", columns=\"gender\", aggfunc=\"mean\")\nprint_tl(mean_ratings.head(5))\n\n# More stats with rating field\nstats_ratings = data.pivot_table(\"rating\", index=\"title\", columns=\"gender\", aggfunc=[\"mean\", \"sum\", \"count\"])\nprint_tl(stats_ratings)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1000209 entries, 0 to 1000208\nData columns (total 10 columns):\n #   Column      Non-Null Count    Dtype \n---  ------      --------------    ----- \n 0   user_id     1000209 non-null  int64 \n 1   movie_id    1000209 non-null  int64 \n 2   rating      1000209 non-null  int64 \n 3   timestamp   1000209 non-null  int64 \n 4   gender      1000209 non-null  object\n 5   age         1000209 non-null  int64 \n 6   occupation  1000209 non-null  int64 \n 7   zip         1000209 non-null  object\n 8   title       1000209 non-null  object\n 9   genres      1000209 non-null  object\ndtypes: int64(6), object(4)\nmemory usage: 83.9+ MB\n\n -----------------------------\n0    5\n1    5\n2    4\n3    4\n4    5\nName: rating, dtype: int64 \n &lt;class 'pandas.core.series.Series'&gt; \n -----------------------------\n0    F\n1    M\n2    M\n3    M\n4    M\nName: gender, dtype: object \n &lt;class 'pandas.core.series.Series'&gt; \n -----------------------------\n0    1193\n1    1193\n2    1193\n3    1193\n4    1193\nName: movie_id, dtype: int64 \n &lt;class 'pandas.core.series.Series'&gt; \n -----------------------------\ngender                                F         M\ntitle                                            \n$1,000,000 Duck (1971)         3.375000  2.761905\n'Night Mother (1986)           3.388889  3.352941\n'Til There Was You (1997)      2.675676  2.733333\n'burbs, The (1989)             2.793478  2.962085\n...And Justice for All (1979)  3.828571  3.689024 \n &lt;class 'pandas.core.frame.DataFrame'&gt; \n -----------------------------\n                                                mean              sum          \\\ngender                                             F         M      F       M   \ntitle                                                                           \n$1,000,000 Duck (1971)                      3.375000  2.761905   54.0    58.0   \n'Night Mother (1986)                        3.388889  3.352941  122.0   114.0   \n'Til There Was You (1997)                   2.675676  2.733333   99.0    41.0   \n'burbs, The (1989)                          2.793478  2.962085  257.0   625.0   \n...And Justice for All (1979)               3.828571  3.689024  134.0   605.0   \n...                                              ...       ...    ...     ...   \nZed & Two Noughts, A (1985)                 3.500000  3.380952   28.0    71.0   \nZero Effect (1998)                          3.864407  3.723140  228.0   901.0   \nZero Kelvin (Kjærlighetens kjøtere) (1995)       NaN  3.500000    NaN     7.0   \nZeus and Roxanne (1997)                     2.777778  2.357143   25.0    33.0   \neXistenZ (1999)                             3.098592  3.289086  220.0  1115.0   \n\n                                           count         \ngender                                         F      M  \ntitle                                                    \n$1,000,000 Duck (1971)                      16.0   21.0  \n'Night Mother (1986)                        36.0   34.0  \n'Til There Was You (1997)                   37.0   15.0  \n'burbs, The (1989)                          92.0  211.0  \n...And Justice for All (1979)               35.0  164.0  \n...                                          ...    ...  \nZed & Two Noughts, A (1985)                  8.0   21.0  \nZero Effect (1998)                          59.0  242.0  \nZero Kelvin (Kjærlighetens kjøtere) (1995)   NaN    2.0  \nZeus and Roxanne (1997)                      9.0   14.0  \neXistenZ (1999)                             71.0  339.0  \n\n[3706 rows x 6 columns] \n &lt;class 'pandas.core.frame.DataFrame'&gt; \n -----------------------------\n\n\n\n# Save dataframe to file - excel, json, csv\ndef save_df_to(df, file_type=\"csv\"):\n  f_type = str.lower(file_type)\n  if f_type == \"excel\":\n    df.to_excel(\"save_df_to.xlsx\", index=True, sheet_name=\"Sheet1\") # save without index name\n  elif f_type == \"json\":\n    df.to_json(\"save_df_to.json\")\n  else: \n    df.to_csv(\"save_df_to.csv\", index=False) # save without index name\n  \n#save_df_to(mean_ratings, file_type=\"EXCEL\")  # Use lowercase \"excel\"\nsave_df_to(stats_ratings, file_type='EXCEL')\nsave_df_to(stats_ratings, file_type='JSON')\nsave_df_to(stats_ratings, file_type='CSV')"
  },
  {
    "objectID": "posts/analysis_us_baby_names.html#check-downloaded-data-source-files",
    "href": "posts/analysis_us_baby_names.html#check-downloaded-data-source-files",
    "title": "US Baby Names 1880-2022",
    "section": "1. Check downloaded data source files",
    "text": "1. Check downloaded data source files\n\n# Display directories and files for the downloaded data files\n\nimport os\n\n# Path to inspect\ndirectory_path = \"../data/babynames\"\n\n# List all items in the path, print\nitems = os.listdir(directory_path)\nfor item in items:\n  item_path = os.path.join(directory_path, item)\n  if os.path.isdir(item_path):\n    print(f\"[DIR] {item}\")\n  else:\n    print(item)\n\nnames.zip\nyob1880.txt\nyob1881.txt\nyob1882.txt\nyob1883.txt\nyob1884.txt\nyob1885.txt\nyob1886.txt\nyob1887.txt\nyob1888.txt\nyob1889.txt\nyob1890.txt\nyob1891.txt\nyob1892.txt\nyob1893.txt\nyob1894.txt\nyob1895.txt\nyob1896.txt\nyob1897.txt\nyob1898.txt\nyob1899.txt\nyob1900.txt\nyob1901.txt\nyob1902.txt\nyob1903.txt\nyob1904.txt\nyob1905.txt\nyob1906.txt\nyob1907.txt\nyob1908.txt\nyob1909.txt\nyob1910.txt\nyob1911.txt\nyob1912.txt\nyob1913.txt\nyob1914.txt\nyob1915.txt\nyob1916.txt\nyob1917.txt\nyob1918.txt\nyob1919.txt\nyob1920.txt\nyob1921.txt\nyob1922.txt\nyob1923.txt\nyob1924.txt\nyob1925.txt\nyob1926.txt\nyob1927.txt\nyob1928.txt\nyob1929.txt\nyob1930.txt\nyob1931.txt\nyob1932.txt\nyob1933.txt\nyob1934.txt\nyob1935.txt\nyob1936.txt\nyob1937.txt\nyob1938.txt\nyob1939.txt\nyob1940.txt\nyob1941.txt\nyob1942.txt\nyob1943.txt\nyob1944.txt\nyob1945.txt\nyob1946.txt\nyob1947.txt\nyob1948.txt\nyob1949.txt\nyob1950.txt\nyob1951.txt\nyob1952.txt\nyob1953.txt\nyob1954.txt\nyob1955.txt\nyob1956.txt\nyob1957.txt\nyob1958.txt\nyob1959.txt\nyob1960.txt\nyob1961.txt\nyob1962.txt\nyob1963.txt\nyob1964.txt\nyob1965.txt\nyob1966.txt\nyob1967.txt\nyob1968.txt\nyob1969.txt\nyob1970.txt\nyob1971.txt\nyob1972.txt\nyob1973.txt\nyob1974.txt\nyob1975.txt\nyob1976.txt\nyob1977.txt\nyob1978.txt\nyob1979.txt\nyob1980.txt\nyob1981.txt\nyob1982.txt\nyob1983.txt\nyob1984.txt\nyob1985.txt\nyob1986.txt\nyob1987.txt\nyob1988.txt\nyob1989.txt\nyob1990.txt\nyob1991.txt\nyob1992.txt\nyob1993.txt\nyob1994.txt\nyob1995.txt\nyob1996.txt\nyob1997.txt\nyob1998.txt\nyob1999.txt\nyob2000.txt\nyob2001.txt\nyob2002.txt\nyob2003.txt\nyob2004.txt\nyob2005.txt\nyob2006.txt\nyob2007.txt\nyob2008.txt\nyob2009.txt\nyob2010.txt\nyob2011.txt\nyob2012.txt\nyob2013.txt\nyob2014.txt\nyob2015.txt\nyob2016.txt\nyob2017.txt\nyob2018.txt\nyob2019.txt\nyob2020.txt\nyob2021.txt\nyob2022.txt"
  },
  {
    "objectID": "posts/analysis_us_baby_names.html#import-data-files",
    "href": "posts/analysis_us_baby_names.html#import-data-files",
    "title": "US Baby Names 1880-2022",
    "section": "2. Import data files",
    "text": "2. Import data files\n\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Straight import\ndirectory_path = \"../data/babynames\"\nitem = \"yob1880.txt\"\ndata_source = os.path.join(directory_path, item)\n\nnames1880 = pd.read_csv(data_source,\n                        names=[\"name\", \"sex\",\"births\"])\nnames1880.info()\nnames1880.describe()\nnames1880\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2000 entries, 0 to 1999\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   name    2000 non-null   object\n 1   sex     2000 non-null   object\n 2   births  2000 non-null   int64 \ndtypes: int64(1), object(2)\nmemory usage: 47.0+ KB\n\n\n\n\n\n\n\n\n\nname\nsex\nbirths\n\n\n\n\n0\nMary\nF\n7065\n\n\n1\nAnna\nF\n2604\n\n\n2\nEmma\nF\n2003\n\n\n3\nElizabeth\nF\n1939\n\n\n4\nMinnie\nF\n1746\n\n\n...\n...\n...\n...\n\n\n1995\nWoodie\nM\n5\n\n\n1996\nWorthy\nM\n5\n\n\n1997\nWright\nM\n5\n\n\n1998\nYork\nM\n5\n\n\n1999\nZachariah\nM\n5\n\n\n\n\n2000 rows × 3 columns"
  },
  {
    "objectID": "posts/analysis_us_baby_names.html#analysis",
    "href": "posts/analysis_us_baby_names.html#analysis",
    "title": "US Baby Names 1880-2022",
    "section": "3. Analysis",
    "text": "3. Analysis\n\n3.1 Total number of births from one data file\n\n# Get total number of births for the year\n# - Sum of the births column by sex\nnames1880.groupby(\"sex\")[\"births\"].sum()  # total births per sex\nnames1880[\"births\"].sum() # total births\n\n201484\n\n\n\n\n3.2 Consolidate all data files\n\nimport pandas as pd\nimport numpy as np\nimport os\n\n# yearly data file ending with year number, year 1880 - 2022\npieces = []\ndirectory_path = \"../data/babynames\"\nfor year in range(1880, 2023):\n  # Set up file path and name\n  path = f\"{directory_path}/yob{year}.txt\"\n  frame = pd.read_csv(path, names=[\"name\", \"sex\", \"births\"])\n  \n  # Add a column for the year\n  frame[\"year\"] = year\n  pieces.append(frame)\n\n# Concatenate each DataFrame into a single DataFrame\nnames = pd.concat(pieces, ignore_index=True)\nnames  # [2085158 rows x 4 columns]\n\n\n\n\n\n\n\n\nname\nsex\nbirths\nyear\n\n\n\n\n0\nMary\nF\n7065\n1880\n\n\n1\nAnna\nF\n2604\n1880\n\n\n2\nEmma\nF\n2003\n1880\n\n\n3\nElizabeth\nF\n1939\n1880\n\n\n4\nMinnie\nF\n1746\n1880\n\n\n...\n...\n...\n...\n...\n\n\n2085153\nZuberi\nM\n5\n2022\n\n\n2085154\nZydn\nM\n5\n2022\n\n\n2085155\nZylon\nM\n5\n2022\n\n\n2085156\nZymeer\nM\n5\n2022\n\n\n2085157\nZymeire\nM\n5\n2022\n\n\n\n\n2085158 rows × 4 columns\n\n\n\n\n\n3.3 Data aggregation: total births by sex and year\n\nimport matplotlib.pyplot as plt\n\n# Aggregation using groupby() or pivot_table()\n\n# total births\ntotal_births = names.pivot_table(\"births\", index=\"year\", columns=\"sex\", aggfunc=sum)\ntotal_births.tail() # total births per sex for the last 5 years\n\n# Plot Total Births\ntotal_births.plot(title=\"Total births by sex and year\")\nplt.show()\n\n\n\n\n\n\n3.4 Process data -\n\n# Insert a new column 'prop' with fraction of babies given each name relative to the total number of births\n# - Insert to each group of year and sex\n\n\ndef add_prop(group):\n  \"\"\"\n  print(\"Processing group......\")\n  print(group)\n  print(\"group[\\\"births\\\"] is.....\")\n  print(group[\"births\"])\n  print(\"Total births per group: group[\\\"births\\\"].sum() is......\")\n  print(group[\"births\"].sum())\n  \"\"\"\n  group[\"prop\"] = group[\"births\"] / group[\"births\"].sum()  # Seroes / scalar\n  return group\n\nnames\nprint(\"---------------------------------\")\nprint(\"Update names data frame with a calculated column of proportion...\")\nnames = names.groupby([\"year\", \"sex\"], group_keys=False).apply(add_prop)\nprint(\"---------------------------------\")\nnames\n\n---------------------------------\nUpdate names data frame with a calculated column of proportion...\n---------------------------------\n\n\n\n\n\n\n\n\n\nname\nsex\nbirths\nyear\nprop\n\n\n\n\n0\nMary\nF\n7065\n1880\n0.077642\n\n\n1\nAnna\nF\n2604\n1880\n0.028617\n\n\n2\nEmma\nF\n2003\n1880\n0.022012\n\n\n3\nElizabeth\nF\n1939\n1880\n0.021309\n\n\n4\nMinnie\nF\n1746\n1880\n0.019188\n\n\n...\n...\n...\n...\n...\n...\n\n\n2085153\nZuberi\nM\n5\n2022\n0.000003\n\n\n2085154\nZydn\nM\n5\n2022\n0.000003\n\n\n2085155\nZylon\nM\n5\n2022\n0.000003\n\n\n2085156\nZymeer\nM\n5\n2022\n0.000003\n\n\n2085157\nZymeire\nM\n5\n2022\n0.000003\n\n\n\n\n2085158 rows × 5 columns\n\n\n\n\n# Sanity check, Prop in each group must add up to 1.\n\nnames.groupby([\"year\", \"sex\"])[\"prop\"].sum()\n\nyear  sex\n1880  F      1.0\n      M      1.0\n1881  F      1.0\n      M      1.0\n1882  F      1.0\n            ... \n2020  M      1.0\n2021  F      1.0\n      M      1.0\n2022  F      1.0\n      M      1.0\nName: prop, Length: 286, dtype: float64\n\n\n\n\nAnalysis - Top 1000 names for each sex/year combination\n\n# Another group operation for top 1000 names\ndef get_top1000(group):\n  return group.sort_values(\"births\", ascending=False)[:1000]\n\ngrouped = names.groupby([\"year\",\"sex\"])\ntop1000 = grouped.apply(get_top1000)\ntop1000.head(12)\n\n\n\n\n\n\n\n\n\n\nname\nsex\nbirths\nyear\nprop\n\n\nyear\nsex\n\n\n\n\n\n\n\n\n\n\n1880\nF\n0\nMary\nF\n7065\n1880\n0.077642\n\n\n1\nAnna\nF\n2604\n1880\n0.028617\n\n\n2\nEmma\nF\n2003\n1880\n0.022012\n\n\n3\nElizabeth\nF\n1939\n1880\n0.021309\n\n\n4\nMinnie\nF\n1746\n1880\n0.019188\n\n\n5\nMargaret\nF\n1578\n1880\n0.017342\n\n\n6\nIda\nF\n1472\n1880\n0.016177\n\n\n7\nAlice\nF\n1414\n1880\n0.015539\n\n\n8\nBertha\nF\n1320\n1880\n0.014506\n\n\n9\nSarah\nF\n1288\n1880\n0.014155\n\n\n10\nAnnie\nF\n1258\n1880\n0.013825\n\n\n11\nClara\nF\n1226\n1880\n0.013473\n\n\n\n\n\n\n\n\n# top1000 dataset does not need the group index\ntop1000 = top1000.reset_index(drop=True)\ntop1000.head(12)\n\n\n\n\n\n\n\n\nname\nsex\nbirths\nyear\nprop\n\n\n\n\n0\nMary\nF\n7065\n1880\n0.077642\n\n\n1\nAnna\nF\n2604\n1880\n0.028617\n\n\n2\nEmma\nF\n2003\n1880\n0.022012\n\n\n3\nElizabeth\nF\n1939\n1880\n0.021309\n\n\n4\nMinnie\nF\n1746\n1880\n0.019188\n\n\n5\nMargaret\nF\n1578\n1880\n0.017342\n\n\n6\nIda\nF\n1472\n1880\n0.016177\n\n\n7\nAlice\nF\n1414\n1880\n0.015539\n\n\n8\nBertha\nF\n1320\n1880\n0.014506\n\n\n9\nSarah\nF\n1288\n1880\n0.014155\n\n\n10\nAnnie\nF\n1258\n1880\n0.013825\n\n\n11\nClara\nF\n1226\n1880\n0.013473\n\n\n\n\n\n\n\n\n\nAnalysis - Naming Trends\n\n# Split top1000 into boy and girl portion\n\nboys = top1000[top1000[\"sex\"] == \"M\"]\ngirls = top1000[top1000[\"sex\"] == \"F\"]\nboys\ngirls\n\n\n\n\n\n\n\n\nname\nsex\nbirths\nyear\nprop\n\n\n\n\n0\nMary\nF\n7065\n1880\n0.077642\n\n\n1\nAnna\nF\n2604\n1880\n0.028617\n\n\n2\nEmma\nF\n2003\n1880\n0.022012\n\n\n3\nElizabeth\nF\n1939\n1880\n0.021309\n\n\n4\nMinnie\nF\n1746\n1880\n0.019188\n\n\n...\n...\n...\n...\n...\n...\n\n\n284871\nLuella\nF\n262\n2022\n0.000161\n\n\n284872\nNancy\nF\n262\n2022\n0.000161\n\n\n284873\nCielo\nF\n261\n2022\n0.000160\n\n\n284874\nMadalyn\nF\n261\n2022\n0.000160\n\n\n284875\nKahlani\nF\n260\n2022\n0.000160\n\n\n\n\n142880 rows × 5 columns\n\n\n\n\n# Simple time serires for names\n\n# Pivot table\ntotal_births = top1000.pivot_table(\"births\", index=\"year\", columns=\"name\", aggfunc=sum)\ntotal_births\n\n\n\n\n\n\n\nname\nAaden\nAadhya\nAaliyah\nAanya\nAarav\nAarna\nAaron\nAarush\nAarya\nAayan\n...\nZola\nZollie\nZona\nZora\nZoya\nZula\nZuri\nZyair\nZyaire\nZyon\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1880\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n102.0\nNaN\nNaN\nNaN\n...\n7.0\nNaN\n8.0\n28.0\nNaN\n27.0\nNaN\nNaN\nNaN\nNaN\n\n\n1881\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n94.0\nNaN\nNaN\nNaN\n...\n10.0\nNaN\n9.0\n21.0\nNaN\n27.0\nNaN\nNaN\nNaN\nNaN\n\n\n1882\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n85.0\nNaN\nNaN\nNaN\n...\n9.0\nNaN\n17.0\n32.0\nNaN\n21.0\nNaN\nNaN\nNaN\nNaN\n\n\n1883\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n105.0\nNaN\nNaN\nNaN\n...\n10.0\nNaN\n11.0\n35.0\nNaN\n25.0\nNaN\nNaN\nNaN\nNaN\n\n\n1884\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n97.0\nNaN\nNaN\nNaN\n...\n14.0\n6.0\n8.0\n58.0\nNaN\n27.0\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2018\nNaN\nNaN\n3836.0\nNaN\n488.0\nNaN\n5986.0\nNaN\nNaN\nNaN\n...\n277.0\nNaN\nNaN\n269.0\nNaN\nNaN\n1131.0\nNaN\n323.0\nNaN\n\n\n2019\nNaN\nNaN\n3661.0\nNaN\n419.0\nNaN\n5534.0\nNaN\nNaN\n209.0\n...\n266.0\nNaN\nNaN\n353.0\nNaN\nNaN\n1181.0\nNaN\n540.0\n213.0\n\n\n2020\nNaN\nNaN\n3572.0\nNaN\n418.0\nNaN\n5087.0\nNaN\n256.0\nNaN\n...\n274.0\nNaN\nNaN\n308.0\n282.0\nNaN\n1264.0\n225.0\n668.0\nNaN\n\n\n2021\nNaN\nNaN\n3603.0\nNaN\n431.0\n268.0\n4843.0\nNaN\n255.0\nNaN\n...\n262.0\nNaN\nNaN\n337.0\n267.0\nNaN\n1435.0\n324.0\n830.0\n242.0\n\n\n2022\nNaN\n268.0\n3270.0\nNaN\n406.0\nNaN\n4496.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n330.0\n358.0\nNaN\n1513.0\n352.0\n867.0\n275.0\n\n\n\n\n143 rows × 7322 columns\n\n\n\n\n# plot the pivot table for a few names\ntotal_births.info()\nsubset = total_births[[\"John\", \"Harry\", \"Mary\", \"Marilyn\"]]\nsubset.plot(subplots=True, figsize=(12, 10), title=\"Number of births per year\")\nplt.show()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 143 entries, 1880 to 2022\nColumns: 7322 entries, Aaden to Zyon\ndtypes: float64(7322)\nmemory usage: 8.0 MB\n\n\n\n\n\n\n\nAnalysis - Measure the increase in naming diversity\nThe decrease in plots above is that fewer parents are choosing the common names for their children. Explore and confirm in the data for this hypothesis. - Proportion of births represented by the top 1000 most popular names by year and sex\n\ntable = top1000.pivot_table(\"prop\", index=\"year\", columns=\"sex\", aggfunc=sum)\ntable\n\nfig, ax = plt.subplots()\nax = table.plot(title=\"Sum of table1000.prop by year and sex\", yticks=np.linspace(0, 1.2, 13))\ndescription = \"Proportion of births represented in top one thousand names by sex\"\nfig.text(0.1, 0.01, description, fontsize=10, ha=\"left\")\nplt.show()\n\n\n\n\n\n\n\n\nIndeed, there is a noticeable trend of rising diversity in names, accompanied by a declining overall proprtion within the top one thousand.\n\n\n\nAnalysis - Number of distinct names, by popularity in the top 50% of births\n\n# With boys data from year 2010\ndf = boys[boys[\"year\"] == 2010]\nprop_cumsum = df[\"prop\"].sort_values(ascending=False).cumsum() # To find the position where prop == 0.5\nprint(\"Top 10 prop sorted:\")\nprint(prop_cumsum[:10], type(prop_cumsum))\n\nprint(\"Indices where prop == 0.5:\")\nprop_cumsum.searchsorted(0.5) + 1 # Indices for prop == 0.5, sort maintained, 1 added due to zero-based index\n\n# With the data from year 1900\ndf = boys[boys.year == 1900]\nin1900 = df.sort_values(\"prop\", ascending=False).prop.cumsum()\nin1900.searchsorted(0.5) + 1\n\nTop 10 prop sorted:\n260876    0.011547\n260877    0.020938\n260878    0.029998\n260879    0.038962\n260880    0.047860\n260881    0.056599\n260882    0.065186\n260883    0.073454\n260884    0.081559\n260885    0.089643\nName: prop, dtype: float64 &lt;class 'pandas.core.series.Series'&gt;\nIndices where prop == 0.5:\n\n\n25\n\n\n\n# Apply the operation to each group of sex/year combination from top1000\n\ntop1000\ndef get_quantile_count(group, q=0.5):\n  group = group.sort_values(\"prop\", ascending=False)\n  return group.prop.cumsum().searchsorted(q) + 1\n\ndiversity = top1000.groupby([\"year\", \"sex\"])\n\n# Check some data from the grouped df.\ntype(diversity)\nselected_group = diversity.get_group((2000, 'F'))\nprint(\"Selected group - 2000 F: \")\nprint(selected_group)\n\n# Get quantile count fpr prop==0.5 for each group\ndiversity = top1000.groupby([\"year\", \"sex\"]).apply(get_quantile_count)\ndiversity\ntype(diversity)  # Series\ndiversity = diversity.unstack()  # unstack, Series --&gt; DF\ntype(diversity)\n\nSelected group - 2000 F: \n           name sex  births  year      prop\n239876    Emily   F   25957  2000  0.014295\n239877   Hannah   F   23085  2000  0.012713\n239878  Madison   F   19968  2000  0.010997\n239879   Ashley   F   17998  2000  0.009912\n239880    Sarah   F   17712  2000  0.009754\n...         ...  ..     ...   ...       ...\n240871    Kenia   F     213  2000  0.000117\n240872    Maiya   F     213  2000  0.000117\n240873   Melisa   F     213  2000  0.000117\n240874   Adrian   F     212  2000  0.000117\n240875   Marlen   F     212  2000  0.000117\n\n[1000 rows x 5 columns]\n\n\npandas.core.frame.DataFrame\n\n\n\n# Plot the Number of popular names in top 50%\nprint(diversity)\ndiversity.plot(title=\"Number of popular names in top 50%\")\nplt.show()\n\nsex     F    M\nyear          \n1880   38   14\n1881   38   14\n1882   38   15\n1883   39   15\n1884   39   16\n...   ...  ...\n2018  259  149\n2019  265  155\n2020  272  163\n2021  276  167\n2022  283  173\n\n[143 rows x 2 columns]\n\n\n\n\n\nThe plot displays that girl names, and they have only become more so over time. Further analysis of what exactly is driving the diversity, .ike the increase of alternative spellings, is lfet to the reader.\n\n\nAnalysis - The “last letter” revolution\nIn 2007, baby name researcher Laura Wattenberg pointed out that the distribution of boy names by final letter has changed significantly over the last 100 years\n\n# Verify the analysis of Lauren Wattenberg\n# boy names, last 100 years, final letter\n\n# Aggregate dataset by year, sex and final letter\nprint(names)\nprint(names[\"name\"])\n\ndef get_last_letter(x):\n  return x[-1]\n\nlast_letters = names[\"name\"].map(get_last_letter)\nlast_letters  # New Series :name\" with last letter of names \nlast_letters.name = \"last_letter\" # Change Series name tp \"last_letter\"\nlast_letters\n\ntable = names.pivot_table(\"births\", index=last_letters, columns=[\"sex\", \"year\"], aggfunc=sum)  # Turn names df into a pivot table by last letter with sex/year combo, total births for each last letter\ntable\n\n# Pick three years to represent 100 years over time, and review some rows. table df has multiple column index of \"sex\" and \"year\"\nsubtable = table.reindex(columns=[1910, 1960, 2010], level=\"year\")\nsubtable.head()\n\n# Normalize the table by total births. To compute a new table containing the proprtion of toral births for each sex ending in each letter\nsubtable.sum()\ntype(subtable)  # DF \ntype(subtable.sum())  # Series\n\n# Compute last_letter_proportion for each letter grouped by sex and year fpr the three time points\nletter_prop = subtable / subtable.sum()  # DF / Series\nletter_prop\n\n              name sex  births  year      prop\n0             Mary   F    7065  1880  0.077642\n1             Anna   F    2604  1880  0.028617\n2             Emma   F    2003  1880  0.022012\n3        Elizabeth   F    1939  1880  0.021309\n4           Minnie   F    1746  1880  0.019188\n...            ...  ..     ...   ...       ...\n2085153     Zuberi   M       5  2022  0.000003\n2085154       Zydn   M       5  2022  0.000003\n2085155      Zylon   M       5  2022  0.000003\n2085156     Zymeer   M       5  2022  0.000003\n2085157    Zymeire   M       5  2022  0.000003\n\n[2085158 rows x 5 columns]\n0               Mary\n1               Anna\n2               Emma\n3          Elizabeth\n4             Minnie\n             ...    \n2085153       Zuberi\n2085154         Zydn\n2085155        Zylon\n2085156       Zymeer\n2085157      Zymeire\nName: name, Length: 2085158, dtype: object\n\n\n\n\n\n\n\n\nsex\nF\nM\n\n\nyear\n1910\n1960\n2010\n1910\n1960\n2010\n\n\nlast_letter\n\n\n\n\n\n\n\n\n\n\na\n0.273388\n0.341890\n0.381239\n0.005031\n0.002447\n0.015067\n\n\nb\nNaN\n0.000343\n0.000256\n0.002116\n0.001834\n0.020494\n\n\nc\n0.000013\n0.000024\n0.000539\n0.002482\n0.007250\n0.012183\n\n\nd\n0.017026\n0.001845\n0.001489\n0.113857\n0.122937\n0.023394\n\n\ne\n0.336943\n0.215131\n0.178430\n0.147599\n0.083838\n0.067968\n\n\nf\nNaN\n0.000010\n0.000055\n0.000783\n0.004329\n0.001206\n\n\ng\n0.000144\n0.000156\n0.000377\n0.002260\n0.009488\n0.001406\n\n\nh\n0.051531\n0.036220\n0.076011\n0.045564\n0.037914\n0.051828\n\n\ni\n0.001526\n0.039965\n0.031700\n0.000844\n0.000610\n0.022672\n\n\nj\nNaN\nNaN\n0.000093\nNaN\nNaN\n0.000772\n\n\nk\n0.000121\n0.000155\n0.000356\n0.036579\n0.049374\n0.018531\n\n\nl\n0.043188\n0.033871\n0.026391\n0.065023\n0.104888\n0.070455\n\n\nm\n0.001200\n0.008611\n0.002599\n0.058046\n0.033832\n0.024707\n\n\nn\n0.079238\n0.130680\n0.140044\n0.143387\n0.152510\n0.362404\n\n\no\n0.001672\n0.002440\n0.001247\n0.017064\n0.012840\n0.042558\n\n\np\n0.000018\n0.000023\n0.000020\n0.003172\n0.005674\n0.001268\n\n\nq\nNaN\nNaN\n0.000030\nNaN\nNaN\n0.000180\n\n\nr\n0.013395\n0.006765\n0.018045\n0.064472\n0.031050\n0.087390\n\n\ns\n0.039036\n0.012762\n0.013336\n0.130797\n0.102699\n0.065160\n\n\nt\n0.027432\n0.015197\n0.007838\n0.072885\n0.065643\n0.022861\n\n\nu\n0.000683\n0.000574\n0.000416\n0.000124\n0.000057\n0.001227\n\n\nv\nNaN\n0.000060\n0.000117\n0.000113\n0.000036\n0.001451\n\n\nw\n0.000020\n0.000031\n0.001190\n0.006323\n0.007706\n0.016183\n\n\nx\n0.000015\n0.000037\n0.000730\n0.003965\n0.001851\n0.008599\n\n\ny\n0.110975\n0.152551\n0.116753\n0.077345\n0.161011\n0.058207\n\n\nz\n0.002436\n0.000658\n0.000700\n0.000170\n0.000184\n0.001829\n\n\n\n\n\n\n\n\n# Bar plots for each sex, broken down by year\nimport matplotlib.pyplot as plt\n\nletter_prop.head()  # review the data to plot\n\nfig, axes = plt.subplots(2, 1, figsize=(10,8))\nplt.subplots_adjust(hspace=0.7)  # Adjust he spacing between subplots\n\n# Male data on the first plot, Female data on the second plot\nletter_prop[\"M\"].plot(kind=\"bar\", rot=0, ax=axes[0], title=\"Male\")\nletter_prop[\"F\"].plot(kind=\"bar\", rot=0, ax=axes[1], title=\"Female\",legend=False)\nplt.show()\n\n\n\n\nBoy names ending in n have experienced significatn growth since the 1960s.\n\ntable   # original table\ntype(table)\ntable.index\ns = table.sum()  # Normalized Series, sum of births for all letters per sex/year combo\ns.index\ns\n\nletter_group = table / table.sum()  # DF / Serieis \nletter_group\n\ndny_ts = letter_group.loc[[\"d\",\"n\",\"y\"], \"M\"].T  # Transpose\ntype(dny_ts)\ndny_ts\n\n\n\n\n\n\n\nlast_letter\nd\nn\ny\n\n\nyear\n\n\n\n\n\n\n\n1880\n0.083057\n0.153217\n0.075763\n\n\n1881\n0.083246\n0.153221\n0.077459\n\n\n1882\n0.085332\n0.149561\n0.077538\n\n\n1883\n0.084053\n0.151656\n0.079149\n\n\n1884\n0.086122\n0.149927\n0.080408\n\n\n...\n...\n...\n...\n\n\n2018\n0.023095\n0.333490\n0.048823\n\n\n2019\n0.022800\n0.323724\n0.048088\n\n\n2020\n0.022618\n0.314561\n0.047809\n\n\n2021\n0.022628\n0.306184\n0.047655\n\n\n2022\n0.022563\n0.296986\n0.047495\n\n\n\n\n143 rows × 3 columns\n\n\n\n\n# Plot with three ending letters over time - d, n, y\nplt.clf()\ndny_ts.plot()\nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\nAnalysis - Boy names that became girl names (and vice versa)\nExamine the trend of popular names with one gender eariler but have become preferred as a name for the other gender over time. - i.e Lesley / Leslie\n\n# Work with top1000 DF, name starting with \"Lesl\"\n\n# Review top1000\ntop1000\n\n# Compute a list of names occurring in the dataset starting with \"Lesl\"\nall_names = pd.Series(top1000[\"name\"].unique())  # Series of unique names\nall_names\nlesley_like = all_names[all_names.str.contains(\"Lesl\")]\nlesley_like\n\n# Filter the top1000 to the list of narrowed names, total count for each name\nfiltered = top1000[top1000[\"name\"].isin(lesley_like)]\nfiltered\nfiltered.groupby(\"name\")[\"births\"].sum()\n\n# Aggregate by sex and year\ntable = filtered.pivot_table(\"births\", index=\"year\", columns=\"sex\", aggfunc=\"sum\")\ntable\n\n# Then normalize the aggregated data (row-wise normalization)\ntable.sum(axis=\"columns\") # row sum\ntable = table.div(table.sum(axis=\"columns\"), axis=\"index\") # divide each element in a row by its corresponding row sum\ntable\n\n\n\n\n\n\n\nsex\nF\nM\n\n\nyear\n\n\n\n\n\n\n1880\n0.091954\n0.908046\n\n\n1881\n0.106796\n0.893204\n\n\n1882\n0.065693\n0.934307\n\n\n1883\n0.053030\n0.946970\n\n\n1884\n0.107143\n0.892857\n\n\n...\n...\n...\n\n\n2018\n1.000000\nNaN\n\n\n2019\n1.000000\nNaN\n\n\n2020\n1.000000\nNaN\n\n\n2021\n1.000000\nNaN\n\n\n2022\n1.000000\nNaN\n\n\n\n\n143 rows × 2 columns\n\n\n\n\n\nData Normalization\nNormalization in the context of data typically refers to the process of scaling and transforming data into a standard formt, making it more consistent and comparable. This is crucial in data analysis and machine learning for several reasons:\n\nScale Consistency:\nNormalization ensures that all variables have the ame scale. It is important when working with algorithms that are sensitive to the scale of the input features, such as gradient-based optimization algorithms in machine leraning\nComparability:\nNormalizing data allows for meaninful comparisons between different variables.\nConvergene Speed:\nSome optimization algorithms converge faster when the input features are on a similar scale. Normalzation ca speed up the convergence of iterative optimization algorithms.\nInterpretability:\nNormalized data is often easeir to interpret. The coefficients in linear models, for example, represetn the change in the dependent variable for a one-unit change in the independent variable. Normalization ensures that this change is consistent across variables.\nHandling Outliers:\nNormalization can mitigate the ipact of outliers. When data has extreme values, normalization techiques can make the analysis less sensitive to these outliers.\n\n\nMin-Max Scaling: Scake the data to a specific range(e.g. between 0 and 1)\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Sample data\ndata = {'feature1': [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Min-Max Scaling\nscaler = MinMaxScaler()\nnormalized_data = scaler.fit_transform(df)\ndf_normalized = pd.DataFrame(normalized_data, columns=df.columns)\n\nprint(\"Original Data:\")\nprint(df)\nprint(\"\\nMin-Max Scaled Data:\")\nprint(df_normalized)\n\nOriginal Data:\n   feature1\n0        10\n1        20\n2        30\n3        40\n4        50\n\nMin-Max Scaled Data:\n   feature1\n0      0.00\n1      0.25\n2      0.50\n3      0.75\n4      1.00\n\n\n\nZ-score Standardization: Scales the data to have a mean of 0 and a standard deviation of\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Sample data\ndata = {'feature1': [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Z-score Standardization\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(df)\ndf_standardized = pd.DataFrame(standardized_data, columns=df.columns)\n\nprint(\"Original Data:\")\nprint(df)\nprint(\"\\nZ-score Standardized Data:\")\nprint(df_standardized)\n\nOriginal Data:\n   feature1\n0        10\n1        20\n2        30\n3        40\n4        50\n\nZ-score Standardized Data:\n   feature1\n0 -1.414214\n1 -0.707107\n2  0.000000\n3  0.707107\n4  1.414214\n\n\n\nDecimal Scaling: Shift the decimal point of values to achieve normalization.\n\n\nimport pandas as pd\n\n# Sample data\ndata = {'feature1': [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Decimal Scaling\ndf['feature1'].max()\n-len(str(df['feature1'].max()))\n10**(-len(str(df['feature1'].max())))\ndecimal_scaling_factor = 10**(-len(str(df['feature1'].max())))\ndf_decimal_scaled = df * decimal_scaling_factor\n\nprint(\"Original Data:\")\nprint(df)\nprint(\"\\nDecimal Scaled Data:\")\nprint(df_decimal_scaled)\n\nOriginal Data:\n   feature1\n0        10\n1        20\n2        30\n3        40\n4        50\n\nDecimal Scaled Data:\n   feature1\n0       0.1\n1       0.2\n2       0.3\n3       0.4\n4       0.5\n\n\n\nLog Transformation: Applies the logarithmic function to reduce the impact of large values.\n\n\nimport pandas as pd\nimport numpy as np\n\n# Sample data\ndata = {'feature1': [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Log Transformation (applies natural logarithm (base e) to each elementin the df after adding 1 to each element, technique avoiding issues with talking the logarithm of zero.)\ndf_log_transformed = np.log1p(df)\n\nprint(\"Original Data:\")\nprint(df)\nprint(\"\\nLog Transformed Data:\")\nprint(df_log_transformed)\n\nOriginal Data:\n   feature1\n0        10\n1        20\n2        30\n3        40\n4        50\n\nLog Transformed Data:\n   feature1\n0  2.397895\n1  3.044522\n2  3.433987\n3  3.713572\n4  3.931826"
  },
  {
    "objectID": "posts/pc_numerical_programming.html",
    "href": "posts/pc_numerical_programming.html",
    "title": "Numerical Programming",
    "section": "",
    "text": "Numerical Programming in Python\n\n# Hinton diagram using NumPy and Matplotlib\n\nfrom sys import getsizeof as size\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to create a Hinton diagram\n\n\ndef hinton(matrix, max_weight=None, ax=None):\n    if ax is None:\n        ax = plt.gca()\n\n    if not max_weight:\n        max_weight = 2**np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n        print(\"max_weight: \", max_weight)\n\n    ax.patch.set_facecolor('darkgray')\n    ax.set_aspect('equal', 'box')\n    ax.xaxis.set_major_locator(plt.NullLocator())\n    ax.yaxis.set_major_locator(plt.NullLocator())\n\n    for (x, y), w in np.ndenumerate(matrix):\n        color = 'green' if w &gt; 0 else 'red'\n        size = np.sqrt(np.abs(w) / max_weight)\n        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n                             facecolor=color, edgecolor=color)\n        ax.add_patch(rect)\n\n    ax.autoscale_view()\n    ax.invert_yaxis()\n\n\n# Create a random 14x20 matrix\nrandom_matrix = np.random.randn(20, 14)\n\n# Create a Hinton diagram for the random matrix\nplt.figure(figsize=(6, 8))\nhinton(random_matrix, max_weight=2)\nplt.show()\n\n\n\n\n\n# Utility functions\n\n\ndef print_tl(x=None, title=\"\"):\n    if x is not None:\n        print(title)\n        print(x, \"\\n\", type(x), \"\\n\", \"-\" * 42)\n    elif title == \"\":\n        print(x, \"\\n\", type(x), \"\\n\", \"-\" * 42)\n    else:\n        print(\"-\" * 42)\n\n\n# Convert temps in Celsius to Fahrenheit using scalar multiplication with a numpy array\n\n\n# Set random see if reproducibility required\nnp.random.seed(42)\n\n# Generate some random real numbers within a range\ntemps_c = np.random.uniform(low=-20, high=45, size=7).round(1)\nprint_tl(temps_c, \"Random temperatures in C: \")\n\n# Convert to Farenheit\ntemps_f = temps_c * 9 / 5 + 32\nprint_tl(temps_f, \"Temps converted to F: \")\n\nRandom temperatures in C: \n[  4.3  41.8  27.6  18.9  -9.9  -9.9 -16.2] \n &lt;class 'numpy.ndarray'&gt; \n ------------------------------------------\nTemps converted to F: \n[ 39.74 107.24  81.68  66.02  14.18  14.18   2.84] \n &lt;class 'numpy.ndarray'&gt; \n ------------------------------------------\n\n\n\n# Simple plot using a numpy array\n\n\nplt.clf()\nplt.plot(temps_c)\nplt.show()\n\n\n\n\n\n# Memory Consumption: ndarray vs list\n\n# Calculate memory consumption\n\n\nlst = [24, 12, 57]\n\nsize_of_list_object = size(lst)  # size of list object itself\nsize_of_elements = size(lst[0]) * len(lst)\ntotal_list_size = size_of_list_object + size_of_elements\n\nprint(\"List: \", lst)\nprint(\"Size without the size of the elements: \", size_of_list_object)\nprint(\"Size of all the elements: \", size_of_elements)\nprint(\"Total size of list, including elements: \", total_list_size)\nprint_tl()\n\nprint(\"List with four elements...\")\nlst = [24, 12, 57, 42]\nprint(lst)\nsize(lst)\nsize_of_elements = len(lst) * size(lst[0])\nsize_of_elements\ntotal_list_size = size_of_list_object + size_of_elements\nprint(\"Size without the size of the elements: \", size_of_list_object)\nprint(\"Size of all the elements: \", size_of_elements)\nprint(\"Total size of list, including elements: \", total_list_size)\n\n# numpy arrary\na = np.array([24, 12, 57])\nprint(size(a))\n\nList:  [24, 12, 57]\nSize without the size of the elements:  88\nSize of all the elements:  84\nTotal size of list, including elements:  172\nNone \n &lt;class 'NoneType'&gt; \n ------------------------------------------\nList with four elements...\n[24, 12, 57, 42]\nSize without the size of the elements:  88\nSize of all the elements:  112\nTotal size of list, including elements:  200\n124\n\n\n\n# Calculate memory usage of numpy array\n\nfrom sys import getsizeof as size\nimport numpy as np\n\na = np.array([24, 12, 57])\nprint_tl(a, \"Numpy array:\")\nprint_tl(size(a), \"Array size: \")\nprint_tl(size(np.array([])), \"Empty array size: \")\n\nNumpy array:\n[24 12 57] \n &lt;class 'numpy.ndarray'&gt; \n ------------------------------------------\nArray size: \n124 \n &lt;class 'int'&gt; \n ------------------------------------------\nEmpty array size: \n112 \n &lt;class 'int'&gt; \n ------------------------------------------"
  },
  {
    "objectID": "posts/p4da3d-data-cleaning_prep.html#handling-missing-data",
    "href": "posts/p4da3d-data-cleaning_prep.html#handling-missing-data",
    "title": "Data Cleaning and Preparation",
    "section": "1. Handling Missing Data",
    "text": "1. Handling Missing Data\nNaN as sentinel value for missing data\nisna to generate a Series of Boolean for null\nNone from Python built-in for missing data\nWhen data cleaning, consider running analysis on missing data itself for data collection issues or potential biaes in the data caused by missing data\n\nimport pandas as pd\nimport numpy as np\n\n# NaN in float64\nfloat_data = pd.Series([1.2, -3.5, np.nan, 0])\nfloat_data\nprint(\"\\n\")\nfloat_data.isna()\n\n\n\n\n\n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# None, Python built-in null\nstring_data = pd.Series([\"aardvark\", np.nan, None, \"avocado\"])\nstring_data\nstring_data.isna()\nprint(\"\\n\")\n\nfloat_data = pd.Series([1, 2, None], dtype=\"float64\")\nfloat_data\nfloat_data.isna()\n\n\n\n\n\n0    False\n1    False\n2     True\ndtype: bool\n\n\n\n“You have a dataset containing information about users and their scores in an online game. The dataset has missing values, and you need to clean it by addressing the missing values using pandas functions.”\n\nRemove rows with missing values.\nFill missing values in the ‘score’ column with the mean score.\nIdentify rows with missing values in the ‘username’ column.\nIdentify rows without missing values in the ‘level’ column.\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Dataset\ndata = {\n  'username': ['user1', 'user2', np.nan, 'user4', 'user5'],\n  'score': [100, np.nan, 150, 200, np.nan],\n  'level': [1, 2, 3, np.nan, 5]\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original data:\")\nprint(df, \"\\n\")\n\n# 1. Remove rows with missing values.\ndf_cleaned1 = df.dropna()\nprint(\"Remove rows with missing values:\")\nprint(df_cleaned1, \"\\n\")\n\n# 2. Fill missing values in the 'score' column with the mean score.\nmean_score = df['score'].mean()\ndf_cleaned2 = df.fillna(value={'score': mean_score})\nprint(\"Fill missing scores with mean score:\")\nprint(df_cleaned2, \"\\n\")\n\n# 3. Identify rows with missing values in the 'username' column.\nis_missing_username = df.isna()[\"username\"]\ndf_rows_with_missing_username = df[is_missing_username]\nprint(\"Rows with missing username:\")\nprint(df_rows_with_missing_username, \"\\n\")\n\n# 4. Identify rows without missing values in the 'level' column.\nis_without_missing_level = df.notna()[\"level\"]\ndf_rows_without_missing_level = df[is_without_missing_level]\nprint(\"Rows without missing level:\")\nprint(df_rows_without_missing_level, \"\\n\")\n\nOriginal data:\n  username  score  level\n0    user1  100.0    1.0\n1    user2    NaN    2.0\n2      NaN  150.0    3.0\n3    user4  200.0    NaN\n4    user5    NaN    5.0 \n\nRemove rows with missing values:\n  username  score  level\n0    user1  100.0    1.0 \n\nFill missing scores with mean score:\n  username  score  level\n0    user1  100.0    1.0\n1    user2  150.0    2.0\n2      NaN  150.0    3.0\n3    user4  200.0    NaN\n4    user5  150.0    5.0 \n\nRows with missing username:\n  username  score  level\n2      NaN  150.0    3.0 \n\nRows without missing level:\n  username  score  level\n0    user1  100.0    1.0\n1    user2    NaN    2.0\n2      NaN  150.0    3.0\n4    user5    NaN    5.0 \n\n\n\n\n1.1 Filtering Out Missing Data\nFiltering out missing data in Series\n\ndata = pd.Series([1, np.nan, 3.5, np.nan, 7])\nprint(\"Data: \")\nprint(data, type(data), \"\\n\")\n\n# Equvalency of dropna() and notna()\nprint(\"Equivalency of dropna() and notna(): \")\nprint(data.dropna())\nprint(data[data.notna()], \"\\n\")\n\nData: \n0    1.0\n1    NaN\n2    3.5\n3    NaN\n4    7.0\ndtype: float64 &lt;class 'pandas.core.series.Series'&gt; \n\nEquivalency of dropna() and notna(): \n0    1.0\n2    3.5\n4    7.0\ndtype: float64\n0    1.0\n2    3.5\n4    7.0\ndtype: float64 \n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame([[1., 6.5, 3.],\n                    [1., np.nan, np.nan],\n                    [np.nan, np.nan, np.nan],\n                    [np.nan, 6.5, 3.]])\nprint(\"Original data: \")\nprint(data, \"\\n\")\n\n# Drop any ROWS with missing values - default\nprint(\"Drop any ROWS with missing value: \")\nprint(data.dropna(), \"\\n\")\n\n# Drop only ROWS that are all NA\nprint(\"Drop only ROWS that are all NA:\")\nprint(data.dropna(how=\"all\"), \"\\n\")\n\n# Drop columns\nprint(\"Adding a new column with NA and dropping it: \")\ndata[4] = np.nan\nprint(data)\nprint(data.dropna(axis=\"columns\", how=\"all\"), \"\\n\") # drop only columns that are all NA\n\nOriginal data: \n     0    1    2\n0  1.0  6.5  3.0\n1  1.0  NaN  NaN\n2  NaN  NaN  NaN\n3  NaN  6.5  3.0 \n\nDrop any ROWS with missing value: \n     0    1    2\n0  1.0  6.5  3.0 \n\nDrop only ROWS that are all NA:\n     0    1    2\n0  1.0  6.5  3.0\n1  1.0  NaN  NaN\n3  NaN  6.5  3.0 \n\nAdding a new column with NA and dropping it: \n     0    1    2   4\n0  1.0  6.5  3.0 NaN\n1  1.0  NaN  NaN NaN\n2  NaN  NaN  NaN NaN\n3  NaN  6.5  3.0 NaN\n     0    1    2\n0  1.0  6.5  3.0\n1  1.0  NaN  NaN\n2  NaN  NaN  NaN\n3  NaN  6.5  3.0 \n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Preserve some rows with missing values, do not drop all of them\n\n# Sample data and set some NA values\ndf = pd.DataFrame(np.random.standard_normal((7, 3)))\ndf.iloc[:4, 1] = np.nan\ndf.iloc[:2, 2] = np.nan\nprint(\"Original data: \")\nprint(df, \"\\n\")\n\n# Drop any rows with missing value\nprint(\"Drop any rows with missing value:\")\nprint(df.dropna(), \"\\n\")\n\n# Drop any rows with missing values, but keep the row with certain number of NAs.\nprint(\"Drop any rows with missing values, but preserve if NAs &lt; 2: \")\nprint(df.dropna(thresh=2), \"\\n\")\n\nOriginal data: \n          0         1         2\n0 -1.228094       NaN       NaN\n1  0.982186       NaN       NaN\n2 -0.026328       NaN -1.319205\n3  0.721260       NaN -2.274546\n4  0.664697 -0.627932 -1.301767\n5  1.000291  1.446766  0.796659\n6 -0.182660 -0.496278 -0.578758 \n\nDrop any rows with missing value:\n          0         1         2\n4  0.664697 -0.627932 -1.301767\n5  1.000291  1.446766  0.796659\n6 -0.182660 -0.496278 -0.578758 \n\nDrop any rows with missing values, but preserve if NAs &lt; 2: \n          0         1         2\n2 -0.026328       NaN -1.319205\n3  0.721260       NaN -2.274546\n4  0.664697 -0.627932 -1.301767\n5  1.000291  1.446766  0.796659\n6 -0.182660 -0.496278 -0.578758 \n\n\n\n\n\n1.2 Filling In Missing Data\nRather than dropping out missing data(potentially discarding other data along with it), you may want to fill in the “holes” in ways. - fillna()\n\n# Filling in missing data\n\nimport pandas as pd\nimport numpy as np\n\n# Sample data and set some NA values\ndf = pd.DataFrame(np.random.standard_normal((7, 3)))\ndf.iloc[:4, 1] = np.nan\ndf.iloc[:2, 2] = np.nan\nprint(\"Original data: \")\nprint(df, \"\\n\")\n\n# Fill NA with some value\nprint(\"Filling NA with number 0: \")\nprint(df.fillna(0), \"\\n\")  # filling NA with 0\n\nprint(\"Filling NA with different values for columns\")\n# Fill NA with different fill value columns\nprint(df.fillna({1: 0.5, 2:0}), \"\\n\") # 0.5 for col1, 0 for \n\n# Fill NA with more meaningful values for imputation\n# Sample data frame and setting some NA values\ndf = pd.DataFrame(np.random.standard_normal((8, 3)))\ndf.iloc[2:7, 1] = np.nan\ndf.iloc[4:, 2] = np.nan\nprint(\"Original data:\")\nprint(df, \"\\n\")\n\n# Fill NA using 'forward fill' method, do only for n consecutive NAs\nprint(\"Fill NA using 'forward fill' method, do only for 3 consecutive NAs\")\nprint(df.fillna(method='ffill', limit=3)) # using the previous non-NA value\nprint(df.fillna(method='bfill', limit=3)) # using the non-NA value behind\n\nOriginal data: \n          0         1         2\n0 -1.411913       NaN       NaN\n1  0.347497       NaN       NaN\n2 -0.948835       NaN -2.177123\n3  0.107214       NaN  1.495539\n4  0.260233 -0.685786 -1.097632\n5  0.316877 -2.474690 -0.323401\n6  0.393796  0.866074  1.070525 \n\nFilling NA with number 0: \n          0         1         2\n0 -1.411913  0.000000  0.000000\n1  0.347497  0.000000  0.000000\n2 -0.948835  0.000000 -2.177123\n3  0.107214  0.000000  1.495539\n4  0.260233 -0.685786 -1.097632\n5  0.316877 -2.474690 -0.323401\n6  0.393796  0.866074  1.070525 \n\nFilling NA with different values for columns\n          0         1         2\n0 -1.411913  0.500000  0.000000\n1  0.347497  0.500000  0.000000\n2 -0.948835  0.500000 -2.177123\n3  0.107214  0.500000  1.495539\n4  0.260233 -0.685786 -1.097632\n5  0.316877 -2.474690 -0.323401\n6  0.393796  0.866074  1.070525 \n\nOriginal data:\n          0         1         2\n0 -0.757152  0.023778  1.168869\n1  0.575411 -0.138309  0.970031\n2  0.663887       NaN  0.425958\n3  2.201875       NaN -0.046227\n4  0.205636       NaN       NaN\n5  0.441329       NaN       NaN\n6 -1.794051       NaN       NaN\n7 -0.078953  0.342802       NaN \n\nFill NA using 'forward fill' method, do only for 3 consecutive NAs\n          0         1         2\n0 -0.757152  0.023778  1.168869\n1  0.575411 -0.138309  0.970031\n2  0.663887 -0.138309  0.425958\n3  2.201875 -0.138309 -0.046227\n4  0.205636 -0.138309 -0.046227\n5  0.441329       NaN -0.046227\n6 -1.794051       NaN -0.046227\n7 -0.078953  0.342802       NaN\n          0         1         2\n0 -0.757152  0.023778  1.168869\n1  0.575411 -0.138309  0.970031\n2  0.663887       NaN  0.425958\n3  2.201875       NaN -0.046227\n4  0.205636  0.342802       NaN\n5  0.441329  0.342802       NaN\n6 -1.794051  0.342802       NaN\n7 -0.078953  0.342802       NaN\n\n\n\n# Data imputation using fillna()\n\nimport pandas as pd\nimport numpy as np\n\n# Sample data\ndata = pd.Series([1., np.nan, 3.5, np.nan, 7])\nprint(\"Original data: \")\nprint(data, \"\\n\")\n\n# Fill NAs with mean / median value of the data set\nprint(\"Fill NAs with mean value: \")\nprint(data.fillna(data.mean()), \"\\n\")\nprint(\"Fill Nas with median value: \")\nprint(data.fillna(data.median()), \"\\n\")\n\nOriginal data: \n0    1.0\n1    NaN\n2    3.5\n3    NaN\n4    7.0\ndtype: float64 \n\nFill NAs with mean value: \n0    1.000000\n1    3.833333\n2    3.500000\n3    3.833333\n4    7.000000\ndtype: float64 \n\nFill Nas with median value: \n0    1.0\n1    3.5\n2    3.5\n3    3.5\n4    7.0\ndtype: float64 \n\n\n\nfillna function arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nvalue\nScalar value or dictionary-like object to use to fill missing values\n\n\nmethod\nInterpolation method: one of \"bfill\" (backward fill) or \"ffill\" (forward fill); default is None\n\n\naxis\nAxis to fill on (\"index\" or \"columns\"); default is axis=\"index\"\n\n\nlimit\nFor forward and backward filling, maximum number of consecutive periods to fill"
  },
  {
    "objectID": "posts/p4da3d-data-cleaning_prep.html#data-transformation",
    "href": "posts/p4da3d-data-cleaning_prep.html#data-transformation",
    "title": "Data Cleaning and Preparation",
    "section": "2. Data Transformation",
    "text": "2. Data Transformation\nFiltering, cleaning and other transformations\n\n2.1 Removing Duplicates\n\nimport pandas as pd\nimport numpy as np\n\n# Sample data frame\ndata = pd.DataFrame({\"k1\": [\"one\", \"two\"] * 3 + [\"two\"], \n                     \"k2\": [1, 1, 2, 3, 3, 4, 4]})\nprint(\"Original data:\")\nprint(data, \"\\n\")\n\n# Boolean Series for duplicate check\nprint(\"Boolean flagas for duplicate rows:\")\nprint(data.duplicated())\nprint(\"Duplicated rows(s):\")\nprint(data[data.duplicated()])\n\n# Drop duplicated rows\nprint(\"Dropping duplicated rows:\")\nprint(data.drop_duplicates(), \"\\n\")\n\n# Drop duplicates only on specific column(s).\ndata[\"v1\"] = range(7)  # add a new column\nprint(data)\nprint(\"Drop duplicates only on specific column(s).\")\nprint(data.drop_duplicates(subset=[\"k1\"]))\n\nOriginal data:\n    k1  k2\n0  one   1\n1  two   1\n2  one   2\n3  two   3\n4  one   3\n5  two   4\n6  two   4 \n\nBoolean flagas for duplicate rows:\n0    False\n1    False\n2    False\n3    False\n4    False\n5    False\n6     True\ndtype: bool\nDuplicated rows(s):\n    k1  k2\n6  two   4\nDropping duplicated rows:\n    k1  k2\n0  one   1\n1  two   1\n2  one   2\n3  two   3\n4  one   3\n5  two   4 \n\n    k1  k2  v1\n0  one   1   0\n1  two   1   1\n2  one   2   2\n3  two   3   3\n4  one   3   4\n5  two   4   5\n6  two   4   6\nDrop duplicates only on specific column(s).\n    k1  k2  v1\n0  one   1   0\n1  two   1   1\n\n\nduplicated, drop_duplicated keeps the first observed value combo. keep=\"last\" returns the last one instead.\n\nimport pandas as pd\nimport numpy as np\n\n# Sample data frame\ndata = pd.DataFrame({\"k1\": [\"one\", \"two\"] * 3 + [\"two\"], \n                     \"k2\": [1, 1, 2, 3, 3, 4, 4],\n                     \"v1\": range(7)})\nprint(\"Preserve the last observed value of duplicates.\")\nprint(data)\nprint(data.drop_duplicates([\"k1\", \"k2\"], keep=\"last\")) # dropping row#5, preserving row#6\n\nPreserve the last observed value of duplicates.\n    k1  k2  v1\n0  one   1   0\n1  two   1   1\n2  one   2   2\n3  two   3   3\n4  one   3   4\n5  two   4   5\n6  two   4   6\n    k1  k2  v1\n0  one   1   0\n1  two   1   1\n2  one   2   2\n3  two   3   3\n4  one   3   4\n6  two   4   6\n\n\n\n\n2.2 Transforming Using a Function or Mapping\nThe map method on a Series accepts a function or dictionary-like object containing a mapping to do the transformation of values. Convenient way to element-wise transformations and other data cleaning-related operations.\n\nimport pandas as pd\nimport numpy as np\n\n# Sample dataset\ndata = pd.DataFrame({\n  \"food\": [\"bacon\", \"pulled pork\", \"bacon\",\n           \"pastrami\", \"corned beef\", \"bacon\",\n           \"pastrami\", \"honey ham\", \"nova lox\"],\n  \"ounces\": [4, 3, 12, 6, 7.5, 8, 3, 5, 6]\n})\ndata\n\n# Dictionary to define key-value with meat and animal\nmeat_to_animal = {\n  \"bacon\": \"pig\",\n  \"pulled pork\": \"pig\",\n  \"pastrami\": \"cow\",\n  \"corned beef\": \"cow\",\n  \"honey ham\": \"pig\",\n  \"nova lox\": \"salmon\"\n}\n# Add a animal category column using map method and dictionary-like object\ndata[\"animal\"] = data[\"food\"].map(meat_to_animal)\ndata\n\n# Add a animal category column using map method with a function objecdt\ndef get_animal(x):\n  return meat_to_animal[x]\n\ndata[\"animal_cat\"] = data[\"food\"].map(get_animal)\ndata\n\n\n\n\n\n\n\n\nfood\nounces\nanimal\nanimal_cat\n\n\n\n\n0\nbacon\n4.0\npig\npig\n\n\n1\npulled pork\n3.0\npig\npig\n\n\n2\nbacon\n12.0\npig\npig\n\n\n3\npastrami\n6.0\ncow\ncow\n\n\n4\ncorned beef\n7.5\ncow\ncow\n\n\n5\nbacon\n8.0\npig\npig\n\n\n6\npastrami\n3.0\ncow\ncow\n\n\n7\nhoney ham\n5.0\npig\npig\n\n\n8\nnova lox\n6.0\nsalmon\nsalmon\n\n\n\n\n\n\n\n\n\n2.3 Replacing Values\nfillna method is a special case of more general value replacement. map can be used to modify a subset of values in an object. replace is simpler and more flexible\n\nimport pandas as pd\nimport numpy as np\n\n# Replace specific value(s) with replacment(s)\ndata = pd.Series([1., -999., 2., -999., -1000., 3., \"Brown Fox\"])\ndata\ndata.replace({-999: np.nan, -1000:0, \"Brown Fox\": \"Red Fox\"})  # multiple replacements using dictionary\n\n0        1.0\n1        NaN\n2        2.0\n3        NaN\n4          0\n5        3.0\n6    Red Fox\ndtype: object\n\n\n\n\n2.4 Renaming Axies Indexes\nUsing map method in axis indexes to modify axis labels in place, like map method in Series object. rename method of data frame creates new object with transformation\n\nimport pandas as pd\nimport numpy as np\n\n# Sample data\ndata = pd.DataFrame(np.arange(12).reshape((3, 4)),\n                    index=[\"ohio\", \"colorado\", \"new york\"],\n                    columns=[\"one\", \"two\", \"three\", \"four\"])\nprint(\"Original some states data frame: \\n\", data)\n\n# Transforming axis index labels in place\ndef transform(x):\n  return x[:4].upper()\nprint(\"\\nTransforming index labels in place:  \\n\", data)\ndata.index = data.index.map(transform)  # transformation of index labels\n\n# Transforming axis index labels in a new data frame, passing functions / dictionary\nprint(\"Transforming axis index labels in a new data frame, passing functions, dictionary..\")\ndata.rename(index=str.title, columns=str.upper)  # passing string methods to titlecase / upper case the index and columns\ndata.rename(index={\"OHIO\": \"INDIANA\"},\n            columns={\"three\": \"peekaboo\"})\nprint(\"\\nCurrent state data: \\n\", data)\n\nOriginal some states data frame: \n           one  two  three  four\nohio        0    1      2     3\ncolorado    4    5      6     7\nnew york    8    9     10    11\n\nTransforming index labels in place:  \n           one  two  three  four\nohio        0    1      2     3\ncolorado    4    5      6     7\nnew york    8    9     10    11\nTransforming axis index labels in a new data frame, passing functions, dictionary..\n\nCurrent state data: \n       one  two  three  four\nOHIO    0    1      2     3\nCOLO    4    5      6     7\nNEW     8    9     10    11\n\n\n\n\n2.5 Discretization and Binning\nDiscretization and binning refer to the process of converting continuous numerical data into discrete internals and bins. In data analysis, discretization and binning are often required to simplify the data, make it more manageable, and faciliate analysis. pandas.cut:\n\nimport pandas as pd\nimport numpy as np\n\n# Sample age bucket data\nages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n\n# Binning the data - 18-25, 26-35, 36-60, 61+\nbins = [18, 25, 35, 60, 100]\nage_categories = pd.cut(ages, bins)\n\nage_categories.codes\nage_categories.categories\nage_categories.categories[0]\npd.value_counts(age_categories)\n\n(18, 25]     5\n(25, 35]     3\n(35, 60]     3\n(60, 100]    1\ndtype: int64\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = np.random.uniform(size=20)\ndata\npd.cut(data, 4, precision=2)  # 4 of equal-length bins\n\n[(0.29, 0.52], (0.29, 0.52], (0.75, 0.99], (0.053, 0.29], (0.75, 0.99], ..., (0.053, 0.29], (0.75, 0.99], (0.29, 0.52], (0.52, 0.75], (0.75, 0.99]]\nLength: 20\nCategories (4, interval[float64, right]): [(0.053, 0.29] &lt; (0.29, 0.52] &lt; (0.52, 0.75] &lt; (0.75, 0.99]]\n\n\nBinning with quartiles: pandas.qcut\n\nimport pandas as pd\nimport numpy as np\n\n# 1000 random float64 numbers in standard normal dist.\ndata = np.random.standard_normal(1000)\nquartiles = pd.qcut(data, 4, precision=2)  # 4 bins using quantile info of data\nprint(quartiles, type(quartiles))\npd.value_counts(quartiles)\n\nprint(\"Binning with custom quantiles between 0 - 1:\")\npd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]).value_counts()\n\n[(-3.98, -0.6], (-0.6, 0.058], (-3.98, -0.6], (-0.6, 0.058], (-0.6, 0.058], ..., (0.63, 2.98], (-0.6, 0.058], (0.058, 0.63], (-0.6, 0.058], (0.63, 2.98]]\nLength: 1000\nCategories (4, interval[float64, right]): [(-3.98, -0.6] &lt; (-0.6, 0.058] &lt; (0.058, 0.63] &lt; (0.63, 2.98]] &lt;class 'pandas.core.arrays.categorical.Categorical'&gt;\nBinning with custom quantiles between 0 - 1:\n\n\n(-3.969, -1.213]    100\n(-1.213, 0.0581]    400\n(0.0581, 1.311]     400\n(1.311, 2.977]      100\ndtype: int64\n\n\n\n\n2.6 Detecting and Filtering Outliers\nLargely a matter of applying array operations.\n\nimport pandas as pd\nimport numpy as np\n\n# Describe the sample data frame in 1000 x 4 shape\ndata = pd.DataFrame(np.random.standard_normal((1000, 4)))\nprint(\"--- Original sample data frame of 1000 x 4: \\n\", data)\nprint(\"--- Describe the sample data frame in 1000 x 4 shape: \\n\", data.describe())\n\n# Filtering elements in column '2' with absolute value &gt; 3:\ncol = data[2] # colum 2\nprint(\"--- Display heads of column '2': \\n\", col.head())\nprint(\"--- Filtering elements with absolute value &gt; 3: \\n\", col[col.abs() &gt; 3]) # filtering\n\n# Select ALL ROWS with any element containing absolute value &gt; 3\nprint(\"--- Select ALL ROWS with any element containing absolute value &gt; 3: \")\nprint(data[(data.abs() &gt; 3).any(axis=\"columns\")])  # Set axis parameter to columns to make row-based filtering\ndata.tail(30)\ndata\n\n--- Original sample data frame of 1000 x 4: \n             0         1         2         3\n0    1.407498  0.579955  0.318191  0.373293\n1   -0.926634 -0.611101 -0.443350 -1.739187\n2    0.639597 -0.068382  0.718472 -1.130073\n3   -0.089489 -1.197352  0.355320 -1.095180\n4    0.441899 -0.055579  1.906773  2.014264\n..        ...       ...       ...       ...\n995  0.409653 -0.744040  0.429761  2.266320\n996 -0.067480  1.677813 -0.167166  1.050424\n997 -1.023779 -0.319502  0.871737  0.409182\n998 -1.832325 -0.265668  0.017586  0.426614\n999 -0.009098  0.589416 -1.399545  2.030039\n\n[1000 rows x 4 columns]\n--- Describe the sample data frame in 1000 x 4 shape: \n                  0            1            2            3\ncount  1000.000000  1000.000000  1000.000000  1000.000000\nmean     -0.009183     0.029173     0.004915     0.062535\nstd       0.989200     0.967977     0.956451     1.020223\nmin      -3.275011    -3.035883    -3.492354    -3.354237\n25%      -0.660060    -0.624276    -0.613096    -0.624135\n50%      -0.040555     0.014348     0.030848     0.074224\n75%       0.652973     0.693571     0.589122     0.772864\nmax       2.881007     3.278446     3.201319     3.147918\n--- Display heads of column '2': \n 0    0.318191\n1   -0.443350\n2    0.718472\n3    0.355320\n4    1.906773\nName: 2, dtype: float64\n--- Filtering elements with absolute value &gt; 3: \n 98     3.201319\n183   -3.492354\nName: 2, dtype: float64\n--- Select ALL ROWS with any element containing absolute value &gt; 3: \n            0         1         2         3\n76   0.558298  1.635638  1.132034 -3.023185\n98   0.937477 -0.463511  3.201319 -0.180321\n165  0.679715  3.278446  1.066659  1.073756\n183 -0.667685  0.283453 -3.492354  1.104043\n201  1.619332 -0.252212 -0.129636  3.147918\n302 -0.418741 -3.035883  0.117048  0.455650\n534 -2.216378 -0.168824 -1.066255 -3.354237\n715 -3.275011 -0.775129 -1.613530  0.735559\n989  0.747609 -3.001891 -1.061821 -0.417744\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.407498\n0.579955\n0.318191\n0.373293\n\n\n1\n-0.926634\n-0.611101\n-0.443350\n-1.739187\n\n\n2\n0.639597\n-0.068382\n0.718472\n-1.130073\n\n\n3\n-0.089489\n-1.197352\n0.355320\n-1.095180\n\n\n4\n0.441899\n-0.055579\n1.906773\n2.014264\n\n\n...\n...\n...\n...\n...\n\n\n995\n0.409653\n-0.744040\n0.429761\n2.266320\n\n\n996\n-0.067480\n1.677813\n-0.167166\n1.050424\n\n\n997\n-1.023779\n-0.319502\n0.871737\n0.409182\n\n\n998\n-1.832325\n-0.265668\n0.017586\n0.426614\n\n\n999\n-0.009098\n0.589416\n-1.399545\n2.030039\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n# Data capping - Transfrom DataFrame by capping data range to certain values\n\ndata2 = pd.DataFrame(np.random.standard_normal((1000, 4)))\nDATA_CAP = 3\nprint(\"Creating sample DF with 1000 x 4 dimension, standard normal dist.: \\n\", data2)\n\nprint(\"--- Describe the original data : \\n\", data2.describe())\ndata2[data2.abs() &gt; DATA_CAP] = np.sign(data2) * DATA_CAP  # Change in place, set the off limit element to DATA_CAP\nprint(\"--- Data transformed with DATA_CAP applied: \\n\", data2)\nprint(\"--- Describe the transformed data : \\n\", data2.describe())\n\nCreating sample DF with 1000 x 4 dimension, standard normal dist.: \n             0         1         2         3\n0    0.514188  1.195679  0.757760  0.283861\n1    0.070017  1.230087  0.509414  0.852981\n2   -0.422204 -0.280746  0.433437  1.832664\n3    1.374825 -0.617282  0.374333 -1.372530\n4    0.145290 -1.422786  0.220729 -1.016823\n..        ...       ...       ...       ...\n995  0.935260 -0.118439  0.541348 -1.601415\n996 -0.110730 -0.396774  0.649097  1.357872\n997  1.385949  1.043180  0.921401 -1.132001\n998 -1.690198 -2.162153  0.591353  0.257888\n999 -0.329659  0.183763 -1.273946  1.489007\n\n[1000 rows x 4 columns]\n--- Describe the original data : \n                  0            1            2            3\ncount  1000.000000  1000.000000  1000.000000  1000.000000\nmean     -0.035450    -0.034492    -0.032529    -0.020031\nstd       1.006872     0.984551     1.003461     1.040398\nmin      -2.768586    -3.294157    -3.392420    -3.903471\n25%      -0.700575    -0.690578    -0.699771    -0.718212\n50%      -0.029194    -0.041755     0.004529     0.011564\n75%       0.662792     0.587990     0.676275     0.666776\nmax       3.209506     3.716015     3.028693     3.121175\n--- Data transformed with DATA_CAP applied: \n             0         1         2         3\n0    0.514188  1.195679  0.757760  0.283861\n1    0.070017  1.230087  0.509414  0.852981\n2   -0.422204 -0.280746  0.433437  1.832664\n3    1.374825 -0.617282  0.374333 -1.372530\n4    0.145290 -1.422786  0.220729 -1.016823\n..        ...       ...       ...       ...\n995  0.935260 -0.118439  0.541348 -1.601415\n996 -0.110730 -0.396774  0.649097  1.357872\n997  1.385949  1.043180  0.921401 -1.132001\n998 -1.690198 -2.162153  0.591353  0.257888\n999 -0.329659  0.183763 -1.273946  1.489007\n\n[1000 rows x 4 columns]\n--- Describe the transformed data : \n                  0            1            2            3\ncount  1000.000000  1000.000000  1000.000000  1000.000000\nmean     -0.035840    -0.034721    -0.032165    -0.018111\nstd       1.005658     0.980275     1.002135     1.033490\nmin      -2.768586    -3.000000    -3.000000    -3.000000\n25%      -0.700575    -0.690578    -0.699771    -0.718212\n50%      -0.029194    -0.041755     0.004529     0.011564\n75%       0.662792     0.587990     0.676275     0.666776\nmax       3.000000     3.000000     3.000000     3.000000\n\n\n\n\n2.7 Permutation and Random Sampling\nPermutation and random sampling are techniques used in data analysis for different purposes. Understanding these techniques is crucial for ensuring the reliability and generalizability of statistical analyses and making informed decisions based on data.\nPermutation: Permutation refers to the arrangement of elements in a specific order. In Data Analysis,permutation is often used in permutation tests or resampling methods. Permutation tests involve randomly reordering the observed data to create a null distribution under the assumption that there is no effect or difference.\nCommon Use Cases:\n\nComparing two groups to assess if the observed difference is statistically significant.\nTesting hypotheses when assumptions of traditional statistical tests are not met.\n\nRandom Sampling: Random sampling involves selecting a subset of data points from a larger dataset in a way that each data point has an equal chance of being chosen. In Data Analysis: Random sampling is fundamental in creating representative samples from a population, and it is commonly used in inferential statistics to make predictions or inferences about a population based on a sample.\nCommon Use Cases:\n\nConducting surveys to gather opinions from a diverse population.\nEstimating population parameters using a sample.\n\nImportance:\n\nStatistical Validity: Both permutation and random sampling contribute to the statistical validity of analyses. Permutation tests provide a non-parametric approach to hypothesis testing, while random sampling ensures that collected samples are unbiased representations of the population.\nRobustness: Permutation tests can be more robust in certain situations, especially when assumptions of parametric tests are violated.\nGeneralization: Random sampling allows for generalizing insights from a sample to a larger population, making statistical analyses more applicable in real-world scenarios.\n\n\nimport pandas as pd\nimport numpy as np\n\n# Create a sample DF with integers, dimension of 5 x 7\ndf = pd.DataFrame(np.arange(5 * 7).reshape((5, 7)))\nprint(\"Original DF: \\n\", df)\nsampler = np.random.permutation(5)\nprint(\"Array of new row index info. for permuting 5 rows: \", sampler)\n\n# Reorder of row index using take()\nprint(\"Reordering using take() : \\n\", df.take(sampler)) # row index reordered\n\n# Reorder of row index using iloc[]\nprint(\"Reordering using iloc: \", df.iloc[sampler])\n\ncolumn_sampler = np.random.permutation(7)\nprint(\"New orders: \", column_sampler, type(column_sampler))\n# Reorder of column index\nprint(\"Reordering columns with axis info: \\n\", df.take(column_sampler, axis=\"columns\"))\n\nOriginal DF: \n     0   1   2   3   4   5   6\n0   0   1   2   3   4   5   6\n1   7   8   9  10  11  12  13\n2  14  15  16  17  18  19  20\n3  21  22  23  24  25  26  27\n4  28  29  30  31  32  33  34\nArray of new row index info. for permuting 5 rows:  [3 0 2 1 4]\nReordering using take() : \n     0   1   2   3   4   5   6\n3  21  22  23  24  25  26  27\n0   0   1   2   3   4   5   6\n2  14  15  16  17  18  19  20\n1   7   8   9  10  11  12  13\n4  28  29  30  31  32  33  34\nReordering using iloc:      0   1   2   3   4   5   6\n3  21  22  23  24  25  26  27\n0   0   1   2   3   4   5   6\n2  14  15  16  17  18  19  20\n1   7   8   9  10  11  12  13\n4  28  29  30  31  32  33  34\nNew orders:  [5 3 6 1 0 2 4] &lt;class 'numpy.ndarray'&gt;\nReordering columns with axis info: \n     5   3   6   1   0   2   4\n0   5   3   6   1   0   2   4\n1  12  10  13   8   7   9  11\n2  19  17  20  15  14  16  18\n3  26  24  27  22  21  23  25\n4  33  31  34  29  28  30  32\n\n\n\n# random subset without replacement - same row cannot appear twice\ndf = pd.DataFrame(np.arange(5 * 7).reshape((5, 7)))\nprint(\"--- Original sample DF: \\n\", df)\nprint(\"--- Random sampling of N rows, Must N &lt; total rows: \")\ndf.sample(n=3)  # random selection of n rows in random order, n must be less than the number of rows\n\n# random subset with replacement - allow repeat choices\nchoices = pd.Series([5, 7, -1, 6, 4])\nprint(\"--- Original Series data. \\n\", choices)  # allow repeated selections\nprint(\"--- Random sampling of N, repeating allowed.\")\nchoices.sample(n=10, replace=True)\n\n--- Original sample DF: \n     0   1   2   3   4   5   6\n0   0   1   2   3   4   5   6\n1   7   8   9  10  11  12  13\n2  14  15  16  17  18  19  20\n3  21  22  23  24  25  26  27\n4  28  29  30  31  32  33  34\n--- Random sampling of N rows, Must N &lt; total rows: \n--- Original Series data. \n 0    5\n1    7\n2   -1\n3    6\n4    4\ndtype: int64\n--- Random sampling of N, repeating allowed.\n\n\n4    4\n3    6\n0    5\n1    7\n3    6\n4    4\n0    5\n2   -1\n2   -1\n0    5\ndtype: int64\n\n\n\n\n2.8 Computing Indicator/Dummy Variables\nCommon type of transformation for machine learning applications / statistical modeling, converting a categorical variable into a dummy/indicator matrix. Convert a columsn with number of distinct values: k into a data frame with k columns with 1s and 0s. pandas.get_dummies\n\nimport pandas as pd\nimport numpy as np\n\n# Sample data frame\ndf = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\n                   \"data1\": range(6)})\nprint(\"-- Sample data frame: \\n\", df)\n\n# Get a matrix of dummy variables for the distinct values in column \"key\"\nprint(\"-- A matrix of dummy variables for column \\\"key\\\": \")\ndummies = pd.get_dummies(df[\"key\"], prefix=\"key\")  # adding prefix to the columns in matrix\nprint(dummies)\n\n# Merge with another / original data frame\ndf_with_dummy = df[[\"data1\"]].join(dummies)  # df[[\"data1\"]].shape()\nprint(\"-- Data frame merge: \")\ndf_with_dummy\n\n-- Sample data frame: \n   key  data1\n0   b      0\n1   b      1\n2   a      2\n3   c      3\n4   a      4\n5   b      5\n-- A matrix of dummy variables for column \"key\": \n   key_a  key_b  key_c\n0      0      1      0\n1      0      1      0\n2      1      0      0\n3      0      0      1\n4      1      0      0\n5      0      1      0\n-- Data frame merge: \n\n\n\n\n\n\n\n\n\ndata1\nkey_a\nkey_b\nkey_c\n\n\n\n\n0\n0\n0\n1\n0\n\n\n1\n1\n0\n1\n0\n\n\n2\n2\n1\n0\n0\n\n\n3\n3\n0\n0\n1\n\n\n4\n4\n1\n0\n0\n\n\n5\n5\n0\n1\n0\n\n\n\n\n\n\n\nHandling rows belong to multiple categpries\n\nimport pandas as pd\nimport numpy as np\n\n# MovieLens 1M dataset\n\n# Read general delimited files into DF\nmnames = [\"movie_id\", \"title\", \"genres\"]\nmovies = pd.read_table(\"../data/movielens/movies.dat\",\n                       sep=\"::\",\n                       header=None,\n                       names=mnames,\n                       engine=\"python\")  \nprint(\"-- Some movie data.. \")\nmovies[:10]\n\n# Get dummy variables on a column with multiple categorical values\nprint(\"-- Some dummy variables on Series 'genre', using str.get_dummies:\")\ndummies = movies[\"genres\"].str.get_dummies(\"|\")\ndummies.iloc[:10, :6]\n\n# Merge dummy variables with original data \nmovies_windic = movies.join(dummies.add_prefix(\"Genre_\"))\nprint(\"-- Movie data merged with dummy variables:\")\nmovies_windic.head()\nprint(\"-- Single movie data row:\")\nmovies_windic.iloc[0]\n\n-- Some movie data.. \n-- Some dummy variables on Series 'genre', using str.get_dummies:\n-- Movie data merged with dummy variables:\n-- Single movie data row:\n\n\nmovie_id                                       1\ntitle                           Toy Story (1995)\ngenres               Animation|Children's|Comedy\nGenre_Action                                   0\nGenre_Adventure                                0\nGenre_Animation                                1\nGenre_Children's                               1\nGenre_Comedy                                   1\nGenre_Crime                                    0\nGenre_Documentary                              0\nGenre_Drama                                    0\nGenre_Fantasy                                  0\nGenre_Film-Noir                                0\nGenre_Horror                                   0\nGenre_Musical                                  0\nGenre_Mystery                                  0\nGenre_Romance                                  0\nGenre_Sci-Fi                                   0\nGenre_Thriller                                 0\nGenre_War                                      0\nGenre_Western                                  0\nName: 0, dtype: object\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Create uniform sample data (repeatable)\nnp.random.seed(12345)\nvalues = np.random.uniform(size=10)  # All values are equally likely to be generated.\nprint(\"-- Sample random values in uniform distribution: \\n\", values, type(values))\n\n# Binning the values\nbins = [0, 0.2, 0.4, 0.6, 0.8, 1]  # bin edges to cut the values into\nvalues_bins = pd.cut(values, bins)  # categorical value mapping \nprint(\"-- Value - bin: \\n\", values_bins, type(values_bins))\n\n# Get dummy variabels for the categorical bins\nprint(\"-- Dummy variables of the categorical bins with sample values: \\n\", pd.get_dummies(values_bins))\n\n-- Sample random values in uniform distribution: \n [0.92961609 0.31637555 0.18391881 0.20456028 0.56772503 0.5955447\n 0.96451452 0.6531771  0.74890664 0.65356987] &lt;class 'numpy.ndarray'&gt;\n-- Value - bin: \n [(0.8, 1.0], (0.2, 0.4], (0.0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.4, 0.6], (0.8, 1.0], (0.6, 0.8], (0.6, 0.8], (0.6, 0.8]]\nCategories (5, interval[float64, right]): [(0.0, 0.2] &lt; (0.2, 0.4] &lt; (0.4, 0.6] &lt; (0.6, 0.8] &lt; (0.8, 1.0]] &lt;class 'pandas.core.arrays.categorical.Categorical'&gt;\n-- Dummy variables of the categorical bins with sample values: \n    (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]\n0           0           0           0           0           1\n1           0           1           0           0           0\n2           1           0           0           0           0\n3           0           1           0           0           0\n4           0           0           1           0           0\n5           0           0           1           0           0\n6           0           0           0           0           1\n7           0           0           0           1           0\n8           0           0           0           1           0\n9           0           0           0           1           0\n\n\nPandas Extended Data Types\n\nimport pandas as pd\n\n# category Data Type\n\n# Create a DataFrame with a categorical column\ndf = pd.DataFrame({'Category': ['A', 'B', 'A', 'C']})\nprint(\"-- Origianl sample data: \\n\", df)\ndf['Category']\nprint(\"-- Convert to type - category on Category column: \\n\")\ndf['Category'] = df['Category'].astype('category')\ndf['Category']\n\n# Nullable Integer Data Types, to allow and handle missing values NaN\n\n# Create a DataFrame with a nullable integer column\ndf = pd.DataFrame({'IntegerColumn': [1, 2, None, 4],\n                   'AnotherColumn': ['A', 'B', 'C', 'D']})\nprint(\"-- Original sample DF: \\n\", df)\nprint(\"df['IntegerColumn]: \\n\", df['IntegerColumn'])  # With None, converts to float64 and NaN\nprint(\"-- IntegerColumn astype Int64: \")\ndf['IntegerColumn'] = df['IntegerColumn'].astype('Int64')  # Keep integers and change None to &lt;NA&gt;\ndf['IntegerColumn']\n\n# StringDtype, allowing string data and missing data within string columns.\n\n# DF with a string colum\ndf = pd.DataFrame({'StringColumn': ['apple', 'banana', None, 'oragne']})\nprint(\"-- Original DF: \\n\", df)\nprint(\"StringColumn: \\n\", df['StringColumn'], type(df['StringColumn']))\ndf['StringColumn'] = df['StringColumn'].astype('string')\nprint(\"StringColumn with astype(string): \\n\", df)\n\n-- Origianl sample data: \n   Category\n0        A\n1        B\n2        A\n3        C\n-- Convert to type - category on Category column: \n\n-- Original sample DF: \n    IntegerColumn AnotherColumn\n0            1.0             A\n1            2.0             B\n2            NaN             C\n3            4.0             D\ndf['IntegerColumn]: \n 0    1.0\n1    2.0\n2    NaN\n3    4.0\nName: IntegerColumn, dtype: float64\n-- IntegerColumn astype Int64: \n-- Original DF: \n   StringColumn\n0        apple\n1       banana\n2         None\n3       oragne\nStringColumn: \n 0     apple\n1    banana\n2      None\n3    oragne\nName: StringColumn, dtype: object &lt;class 'pandas.core.series.Series'&gt;\nStringColumn with astype(string): \n   StringColumn\n0        apple\n1       banana\n2         &lt;NA&gt;\n3       oragne\n\n\nHelper functions.\n\n# Helper functions\n\nimport pandas as pd\nimport numpy as np\n\n# Helper function to transform data frame with capped value\n\ndef data_cap(data, condition, cap_value):\n  data[condition] = np.sign(data) * cap_value\n  \ndata = pd.DataFrame(np.random.standard_normal((100, 4)))\nprint(\"--- Creating sample DF with 100 x 4 dimension, standard normal dist.: \\n\", data)\nprint(\"--- Describing the original data : \\n\", data.describe())\nDATA_CAP_VALUE = 1.0\ndata_cap(data, data.abs() &gt; DATA_CAP_VALUE, DATA_CAP_VALUE)\nprint(\"--- Display and describing the transformed data with capped value: \\n\", data, \"\\n\", data.describe())\n\n--- Creating sample DF with 100 x 4 dimension, standard normal dist.: \n            0         1         2         3\n0   1.007189 -1.296221  0.274992  0.228913\n1   1.352917  0.886429 -2.001637 -0.371843\n2   1.669025 -0.438570 -0.539741  0.476985\n3   3.248944 -1.021228 -0.577087  0.124121\n4   0.302614  0.523772  0.000940  1.343810\n..       ...       ...       ...       ...\n95  0.106061  3.927528 -0.255126  0.854137\n96 -0.364807  0.131102 -0.697614  1.335649\n97 -0.151039  0.442938  0.941571  0.533364\n98  0.356266 -0.010115  1.415753  0.566106\n99  0.456487  0.194788 -0.655054 -0.565230\n\n[100 rows x 4 columns]\n--- Describing the original data : \n                 0           1           2           3\ncount  100.000000  100.000000  100.000000  100.000000\nmean     0.075414   -0.072037   -0.151045    0.073887\nstd      1.056971    1.099638    0.988262    0.905969\nmin     -2.644409   -2.420294   -2.557934   -1.860761\n25%     -0.699515   -0.773203   -0.788723   -0.571684\n50%      0.073201   -0.189255   -0.261119    0.090167\n75%      0.730487    0.624688    0.637665    0.649135\nmax      3.248944    3.927528    2.212303    2.613999\n--- Display and describing the transformed data with capped value: \n            0         1         2         3\n0   1.000000 -1.000000  0.274992  0.228913\n1   1.000000  0.886429 -1.000000 -0.371843\n2   1.000000 -0.438570 -0.539741  0.476985\n3   1.000000 -1.000000 -0.577087  0.124121\n4   0.302614  0.523772  0.000940  1.000000\n..       ...       ...       ...       ...\n95  0.106061  1.000000 -0.255126  0.854137\n96 -0.364807  0.131102 -0.697614  1.000000\n97 -0.151039  0.442938  0.941571  0.533364\n98  0.356266 -0.010115  1.000000  0.566106\n99  0.456487  0.194788 -0.655054 -0.565230\n\n[100 rows x 4 columns] \n                 0           1           2           3\ncount  100.000000  100.000000  100.000000  100.000000\nmean     0.050169   -0.073364   -0.113546    0.037149\nstd      0.731434    0.750175    0.728088    0.713627\nmin     -1.000000   -1.000000   -1.000000   -1.000000\n25%     -0.699515   -0.773203   -0.788723   -0.571684\n50%      0.073201   -0.189255   -0.261119    0.090167\n75%      0.730487    0.624688    0.637665    0.649135\nmax      1.000000    1.000000    1.000000    1.000000"
  }
]